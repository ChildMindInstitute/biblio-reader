Structured Bayesian Learning Through Mixture
Models
by

Francesca Petralia
Department of Statistical Science
Duke University
Date:
Approved:

David B. Dunson, Supervisor

David L. Banks

Surya T. Tokdar

Joseph Lucas

Dissertation submitted in partial fulfillment of the requirements for the degree of
Doctor of Philosophy in the Department of Statistical Science
in the Graduate School of Duke University
2013

Abstract
Structured Bayesian Learning Through Mixture Models
by

Francesca Petralia
Department of Statistical Science
Duke University
Date:
Approved:

David B. Dunson, Supervisor

David L. Banks

Surya T. Tokdar

Joseph Lucas

An abstract of a dissertation submitted in partial fulfillment of the requirements for
the degree of Doctor of Philosophy in the Department of Statistical Science
in the Graduate School of Duke University
2013

Copyright c 2013 by Francesca Petralia
All rights reserved except the rights granted by the
Creative Commons Attribution-Noncommercial Licence

Abstract
In this thesis, we develop some Bayesian mixture density estimation for univariate and multivariate data. We start proposing a repulsive process favoring mixture components further apart. While conducting inferences on the cluster-specific
parameters, current frequentist and Bayesian methods often encounter problems
when clusters are placed too close together to be scientifically meaningful. Current Bayesian practice generates component-specific parameters independently from
a common prior, which tends to favor similar components and often leads to substantial probability assigned to redundant components that are not needed to fit
the data. As an alternative, we propose to generate components from a repulsive
process, which leads to fewer, better separated and more interpretable clusters.
In the second part of the thesis, we face the problem of modeling the conditional
distribution of a response variable given a high dimensional vector of predictors
potentially concentrated near a lower dimensional subspace or manifold. In many
settings it is important to allow not only the mean but also the variance and shape of
the response density to change flexibly with features, which are massive-dimensional.
We propose a multiresolution model that scales efficiently to massive numbers of
features, and can be implemented efficiently with slice sampling.
In the third part of the thesis, we deal with the problem of characterizing the
conditional density of a multivariate vector of response given a potentially high dimensional vector of predictors. The proposed model flexibly characterizes the density

iv

of the response variable by hierarchically coupling a collection of factor models, each
one defined on a different scale of resolution. As it is illustrated in Chapter 4, our
proposed method achieves good predictive performance compared to competitive
models while efficiently scaling to high dimensional predictors.

v

to My Parents

vi

Contents
Abstract

iv

List of Tables

x

List of Figures

xiii

List of Abbreviations and Symbols

xvi

Acknowledgements

xviii

1 Introduction

1

1.1

Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1

1.2

Literature Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5

1.2.1

Mixture Models . . . . . . . . . . . . . . . . . . . . . . . . . .

5

1.2.2

Divide and Conquer Algorithms and Tree Based Models . . .

8

1.2.3

Factor Model and Mixture of Factor Analyzers . . . . . . . . .

9

Dissertation Outline . . . . . . . . . . . . . . . . . . . . . . . . . . .

11

1.3

2 Repulsive Mixtures
2.1

2.2

13

Bayesian Repulsive Mixture Models . . . . . . . . . . . . . . . . . .

14

2.1.1

Repulsive Densities . . . . . . . . . . . . . . . . . . . . . . . .

14

2.1.2

Theoretical Properties . . . . . . . . . . . . . . . . . . . . . .

16

Posterior Computation and Parameter Calibration . . . . . . . . . . .

19

2.2.1

Posterior Computation . . . . . . . . . . . . . . . . . . . . . .

19

2.2.2

Calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . .

21

vii

2.3

Synthetic Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . .

22

2.4

Real Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

24

3 Dictionary Learning for Conditional Distributions
3.1

3.2

3.3

3.4

Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

31

3.1.1

Model Overview . . . . . . . . . . . . . . . . . . . . . . . . . .

31

3.1.2

Model Specification . . . . . . . . . . . . . . . . . . . . . . . .

32

Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

34

3.2.1

Full Conditionals . . . . . . . . . . . . . . . . . . . . . . . . .

36

3.2.2

Predictions . . . . . . . . . . . . . . . . . . . . . . . . . . . .

37

Simulation Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . .

38

3.3.1

Illustrative Example . . . . . . . . . . . . . . . . . . . . . . .

38

3.3.2

Linear Lower Dimensional Space

. . . . . . . . . . . . . . . .

39

3.3.3

Non-Linear Lower Dimensional Space . . . . . . . . . . . . . .

41

3.3.4

Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

42

Real Application . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

44

4 Bayesian factor trees
4.1

4.2

4.3

30

52

Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

53

4.1.1

Model Structure . . . . . . . . . . . . . . . . . . . . . . . . . .

53

Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

55

4.2.1

Prior Specification . . . . . . . . . . . . . . . . . . . . . . . .

55

4.2.2

Selection of the Number of Factors . . . . . . . . . . . . . . .

56

4.2.3

Full Conditionals and Gibbs Sampler Steps . . . . . . . . . . .

56

Synthetic Example . . . . . . . . . . . . . . . . . . . . . . . . . . . .

59

4.3.1

Two Dimensional Predictors . . . . . . . . . . . . . . . . . . .

60

4.3.2

Higher Dimensional Predictors . . . . . . . . . . . . . . . . . .

63

viii

4.4

Real Application . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

64

5 Concluding Remarks and Future Direction

68

A Chapter 2: Theory

71

B

A.1 Cited Theorems and Assumptions . . . . . . . . . . . . . . . . . . . .

71

A.2 Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

72

Chapter 2: Additional Results

78

B.1 Synthetic examples . . . . . . . . . . . . . . . . . . . . . . . . . . . .

78

B.2 Additional results . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

80

Bibliography

83

Biography

91

ix

List of Tables
2.1
2.2

3.1

3.2

Posterior mean and standard deviation of weights, location and scale
parameters under dataset drawn from densities pIa, Ibq . . . . . . . .

25

Mean and standard deviation of K-L divergence, misclassification error
and sum of extra weights resulting from non-repulsive mixture and
repulsive mixture with a maximum number of clusters equal to six
under different synthetic data scenarios. . . . . . . . . . . . . . . . .

26

Linear manifold example 1: Mean and standard deviations of squared
errors under multiscale stick-breaking (MSB), CART and Lasso for
sample size 50 and 100 for different simulation scenarios. Variable
time indicates the mean of CPU usage to predict a single point, p is
the dimensionality of the predictor space, n is the sample size and
k indicates 1, 000, i.e. 300k=300,0000. Bold indicates best MSE, 
indicates best CPU time. As shown, MSB outperforms both CART
and Lasso regardless of ambient dimension and sample size. . . . . . .

47

Linear manifold example 2: Mean and standard deviations of squared
errors under multiscale stick-breaking (MSB), CART and Lasso for
sample size 50 and 100. Variable time indicates the mean of CPU
usage to predict a single point, p is the dimensionality of the predictor
space, n is the sample size and k indicates 1, 000, i.e. 300k=300,0000.
In this case, given the non-linear relationship between response and
predictors, CART outperforms Lasso. However, our model results in
the lowest mean squared errors. . . . . . . . . . . . . . . . . . . . . .

48

x

3.3

3.4

Non-linear manifold - MFA: Mean and standard deviations of squared
errors under multiscale stick-breaking (MSB), CART and Lasso for
sample size 50 and 100 for different simulations sampled from a mixture of factor analyzers. Variable time indicates the mean of CPU
usage to predict a single point, p is the dimensionality of the predictor
space, n is the sample size and k indicates 1, 000, i.e. 300k=300,0000.
Bold indicates best MSE,  indicates best CPU time. As shown, MSB
outperforms both CART and Lasso regardless of ambient dimension
and sample size. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

49

Non-linear manifold - Swissroll and S-Manifold: Mean and standard
deviations of squared errors under multiscale stick-breaking (MSB),
CART and Lasso for sample size 50 and 100 for different simulation
scenarios. Variable time indicates the mean of CPU usage to predict
a single point, p is the dimensionality of the predictor space, n is the
sample size and k indicates 1, 000, i.e. 300k=300,0000. Bold indicates
best MSE,  indicates best CPU time. As shown, MSB outperforms
both CART and Lasso regardless of ambient dimension and sample
size. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

50

3.5

Neuroscience application quantitative performance comparisons. Squared
error predictive accuracy per subject (using leave-one-out) was computed. We report the mean and standard deviation (s.d.) across subjects of squared error, and CPU time (in seconds). We compare multiscale stick-breaking (MSB), CART, Lasso and random forest (RF).
MSB outperforms all the competitors in terms of predictive accuracy
and scalability. Only MSB and Lasso even ran for the  106 dimensional application. Bold indicates best MSE,  indicates best CPU
time. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51

4.1

Two dimensional predictors: Mean and standard deviations of squared
errors under our bayesian factor tree (BFT), a mixture of factor analyzers (MFA) and covariate dependent mixture of factor analyzers
(dMFA) under the first (1) and second (2) simulation scenario. Bold
indicates best MSE. As shown, in almost all data scenarios, BFT leads
to the lowest MSE. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.2

62

Higher dimensional predictors: Mean and standard deviations of squared
errors under our bayesian factor tree (BFT), a mixture of factor analyzers (MFA) and covariate dependent mixture of factor analyzers
(dMFA). Bold indicates best MSE. . . . . . . . . . . . . . . . . . . . 65

xi

4.3

Real dataset: Percentiles (2.5%, 50% and 97.5%) of squared errors
under our Bayesian Factor Tree (BFT), a mixture of factor analyzers
(MFA) and covariate dependent mixture of factor analyzers (dMFA).
For the second data example, given the ultra-high dimensionality of
the predictor space, we compared our approach only to MFA. . . . .

67

B.1 Percentiles 2.5th and 97.5th of sum of extra weights (sew q and location parameters involved in the two components with highest weights
(Âµ1 , Âµ2 ) under repulsive and non-repulsive atoms for different values
of Î±Ìƒ considering 1, 000 draws from density IIb . . . . . . . . . . . . .

81

B.2 Mean and standard deviations of the total probability weight placed
on extra components (more than used in generating data) and KL divergence under non-repulsive and repulsive mixtures in different
synthetic data cases. . . . . . . . . . . . . . . . . . . . . . . . . . . .

81

xii

List of Figures
2.1

2.2

2.3

2.4

3.1

3.2

3.3

Contour plots of the repulsive prior Ï€ pÎ³1 , Î³2 q satisfying definition 1(ii)
under p2.1q either p2.2q or p2.3q and p2.4q with hyperparameters pÏ„, Î½ q
equal to pI qp1, 2q, pII qp1, 4q, pIII qp5, 2q and pIV qp5, 4q . . . . . . . . .

16

pI q Standard normal density (solid), two-component mixture of nor-

mals sharing the same location parameter (dash) and Studentâ€™s t density (dash-dot), referred as pIa, Ib, Icq, pII q two-components mixture
of poorly (solid) and well separated (dot-dash) Gaussian densities, referred as pIIa, IIbq, pIII q mixture of poorly (solid) and well separated
(dot-dash) Gaussian and Pearson densities, referred as pIIIa, IIIbq,
pIV q two-components mixture of two-dimensional non-spherical Gaussians . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

23

Histogram of galaxy data (I) and acidity data (IV) overlaid with a
nonparametric density estimate using Gaussian kernel density estimation. Estimated clusters under galaxy data for non-repulsive (II) and
repulsive (III) priors and under acidity data for non-repulsive (V) and
repulsive (VI) priors . . . . . . . . . . . . . . . . . . . . . . . . . . .

28

Density of sum of extra weights under k=6 for non-repulsive (solid)
and repulsive (dash) and k=10 components for non-repulsive (dashdot) and repulsive (dot) . . . . . . . . . . . . . . . . . . . . . . . . .

29

Partition tree schematic: (i) Multiscale partition of the data. (ii)
Estimate dictionary density and weight associated to each set. (iii)
Nodes along the tree containing xi P <p . (iv) Conditional density of
yi given xi defined as a convex combination of densities associated to
the nodes containing xi . . . . . . . . . . . . . . . . . . . . . . . . . .

33

Illustrative example: Plot of true density (red line) and estimated density (50th percentile: solid line, 2.5th and 97.5th percentiles: dashed
lines) for four data points pI, II, III, IV q considering different training set size (a:100, b:200, c:300). . . . . . . . . . . . . . . . . . . . .

40

Swissroll-Manifold and S-Manifold embedded in R3 . . . . . . . . . .

41

xiii

3.4

3.5

4.1

4.2

Numerical results for various simulation scenarios. Top plot depicts
the relative mean-squared error of MSB (our approach), versus CART
(red) and Lasso (black) as a function of ambient dimension of x. Bottom plot depicts the ratio of CPU time as a function of ambient dimension of x. The simulation scenario considered is the linear subspace.
MSB outperforms both CART and Lasso regardless of ambient dimenW
sion (rmse
1 for all p). MSB compute time is relatively constant as
p increases, whereas Lassoâ€™s compute time increases, thus, as n or p
increase, MSB CPU time becomes less than Lassoâ€™s. MSB was always
significantly faster than CART and PC regression, regardless of n or
p. For all panels, n  100 when p varies, and p  300k when n varies,
where k indicates 1000, e.g., 300k 3  105 . . . . . . . . . . . . . . .

43

Numerical results for various simulation scenarios. Top plots depict
the relative mean-squared error of MSB (our approach), versus CART
(red) and Lasso (black) as a function of ambient dimension of x. Bottom plots depict the ratio of CPU time as a function of sample size.
The two simulation scenarios are: MFA (left) and Swissroll (right).
MSB outperforms both CART and Lasso in all two scenarios regardW
less of ambient dimension (rmse
1 for all p). MSB compute time
is relatively constant as n or p increase, whereas Lassoâ€™s compute
time increases, thus, as n or p increase, MSB CPU time becomes less
than Lassoâ€™s. MSB was always significantly faster than CART and
PC regression, regardless of n or p. For all panels, n  100 when
p varies, and p  300k when n varies, where k indicates 1000, e.g.,
300k 3  105 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

44

True value and estimate under MFA, dMFA and BFT of five variables
of yi given xi  pxi1 , xi2 qT for i  t1, . . . , 100u. Each row correspond
to a different element of the response vector y, while each column
correspond to a different method utilized to predict y. As shown,
BFT performs similarly to dMFA in estimating the five elements of
y, while a simple MFA (not depending on covariates) is not able to
capture most of the spatial structure. . . . . . . . . . . . . . . . . . .

63

Plots depicts the relative CPU time of BFT (our approach), versus
dMFA as a function of ambient dimension of x, under the normal and
the swissroll simulation scenario with q  500 and n  100. The x-axis
is the number of predictors involved in the experiment, where k equals
1 thousand, so that 2k=2,000. BFT outperforms dMFA regardless of
ambient dimension (rcpu 1 for all p). . . . . . . . . . . . . . . . . .

66

xiv

B.1 Plot of K-L divergence under six and ten components p6 : I, 10 : II q for
different choice of separation level c under density pIIaq for different
sample sizes (100:solid ; 1000:dash) and density pIIbq for different
sample sizes (100:dash-dot; 1000:dot) . . . . . . . . . . . . . . . . . .

xv

82

List of Abbreviations and Symbols
Symbols
X

Space of predictors

Y

Space of response variable

x

Vector of predictors

y

Response variable

<

Real numbers

Dirichlet

Dirichlet density.

Un

Uniform density.

IG

Inverse Gamma density

N pÂµ, Ïƒ q
Np pÂµ, Î£q
M ult

univariate Normal density with mean Âµ and variance Ïƒ
p-variate Normal density with mean Âµ and covariance matrix Î£
Multinomial density

Abbreviations
MSE
N-R
R

Mean Squared Error.
Non-Repulsive.
Repulsive

MSB

Multiresolution Stick Breaking

BFT

Bayesian Factor Tree

MFA

Mixture of factor analyzers

xvi

dMFA

Covariates dependent Mixture of factor analyzers

K-L

Kullback-Leibler

AIC

Akaike Information Criterion

BIC

Bayesian Information Criterion

i.i.d.

Independent and identically distributed

CART
RF
CPU

Classification and regression trees
Random forest
Central processing unit

xvii

Acknowledgements
The completion of this work would have not been possible without the guidance and
advice of my advisor David Dunson. He helped me to grow as a researcher and to keep
things in perspective. At the same time, he motivated me a lot with his enthusiasm
towards statistics and his curiosity about other disciplines and new problems. I am
very grateful to my thesis committee members: Surya Tokdar, David Banks and
Joseph Lucas. A very special thanks to Mike West and David Banks. They helped
me start my experience at Duke and encouraged me during the difficult moments. I
really enjoyed collaborating with scientists from other fields. In particular I want to
mention Joshua Vogelstein and Vinayak Rao.

xviii

1
Introduction

1.1 Motivation
For decades mixture models have been extensively used for classification, discrimination and density estimation. In analyses of finite mixture models, a common concern
is over-fitting in which redundant mixture components having similar locations and
scales are introduced. Over-fitting can have an adverse impact on density estimation, since this leads to an unnecessarily complex model. Another common goal of
finite mixture modeling is clustering (Fraley and Raftery, 2002), and having components with similar locations, leads to overlapping kernels and lack of interpretability.
Introducing kernels with similar locations but different scales may be necessary to
fit heavy-tailed and skewed densities, and hence low separation in clustering and
over-fitting are distinct problems.
Recently, Rousseau and Mengersen (2011) studied the asymptotic behavior of
the posterior distribution in over-fitted Bayesian mixture models having more components than needed. They showed that a carefully chosen prior will lead to asymptotic emptying of the redundant components. However, several challenging practical
issues arise. For small to moderate sample sizes, the weight assigned to redundant
1

components is often substantial. This can be attributed to identifiability problems
that arise from a difficulty in distinguishing between models that partition each of
a small number of well separated components into a number of essentially identical
components. This issue leads to substantial uncertainty in clustering and estimation of the number of components, and is not specific to over-fitted mixture models;
similar behavior occurs in placing a prior on the number of components or using a
nonparametric Bayes approach such as the Dirichlet process.
The problem of separating components has been studied for Gaussian mixture
models (Dasgupta, 1999; Dasgupta and Schulman, 2007). Two Gaussians can be
separated by placing an arbitrarily chosen lower bound on the distance between their
means. Separated Gaussians have been mainly utilized to speed up convergence of
the Expectation-Maximization (EM) algorithm. In choosing a minimal separation
level, it is not clear how to obtain a good compromise between values that are
too low to solve the problem and ones that are so large that one obtains a poor fit.
Alternatively, we propose a repulsive prior discouraging closeness among componentspecific parameters without placing an hard constraint. This repulsive process leads
to better separated and more interpretable clusters while accurately estimating the
true density of the data.
Mixture models are also utilized to describe the conditional distribution of a
response variables given a set of predictors. In this framework an important issue is
the scalability of mixture models to massive numbers of predictors. Massive datasets
present statistical and computational challenges for machine learning because many
previously developed approaches do not scale-up sufficiently. Specifically, challenges
arise because of the ultrahigh-dimensionality, and relatively low sample size (the
â€œlarge p, small nâ€ problem, Bernardo et al. (2003)). Parsimonious models for such big
data assume that the density in the ambient dimension concentrates around a lowerdimensional and possibly nonlinear subspace. Indeed, a plethora of methodologies
2

are emerging to estimate such lower-dimensional manifolds from high-dimensional
data (Rahman et al., 2005; Allard et al., 2012).
There is a rich machine learning and statistical literature on conditional density
estimation of a response y

P Y given a set of features (predictors) x  px1, x2, . . . , xpq P

X . Common approaches include hierarchical mixtures of experts (Jacobs et al.,
1991; Jiang and Tanner, 1999), kernel methods (Fan et al., 1996; Fan and Yim, 2004;
Holmes et al., 2010; Fu et al., 2011), Bayesian finite mixture models (Nott et al., 2012;
Tran et al., 2012; Norets and Pelenis, 2012) and Bayesian nonparametrics (Griffin
and Steel, 2006; Dunson et al., 2007; Chung and Dunson, 2009; Tokdar et al., 2010).
In all these works, there has been limited consideration of scaling to large p settings,
with the variational Bayes approach of Tran et al. (2012) being a notable exception.
For dimensionality reduction, they follow a greedy variable selection algorithm. Their
approach does not scale to the sized applications we are interested in. For example,
in a problem with p  1, 000 and n  500, they reported a CPU time of 51.7 minutes
for a single analysis. We are interested in problems many orders of magnitude or
more larger than this, and require a faster computing time while also accurately estimating the conditional density of a response variable. To our knowledge, there are
no nonparametric density regression competitors to our approach, which maintain
a characterization of uncertainty in estimating the conditional densities; rather, all
sufficiently scalable algorithms provide point predictions and/or rely on restrictive
assumptions such as linearity.
A widely used method to estimate a covariates-dependent density is to partition
observations into a nested sequence of subsets based on feature similarity, with simple models fit within each subset. This is the basis for CART (Breiman et al., 1984),
modifications such as random forests (Breiman, 2001), boosting (Shapire et al., 1998)
and bagging (Breiman, 1996). Though these algorithms can substantially improve
mean square error performance, computation can be expensive and performance de3

grades as the dimensionality of the predictor space increases. In fact, a significant
downside of divide-and-conquer algorithms is their poor scalability to high dimensional predictors. As the number of features increases, the problem of finding the best
splitting attribute becomes intractable so that tree based models cannot be efficiently
applied. As the number of features increases, also mixture of experts models become
computationally demanding, since both mixture weights and dictionary densities
are feature-dependent. In an attempt to make mixtures of experts more efficient,
sparse extensions relying on different variable selection algorithms have been proposed (Mossavat and Amft, 2011). However, performing variable selection in high
dimensions is effectively intractable: algorithms need to efficiently search for the
best subsets of predictors to include in weight and mean functions within a mixture model, an NP-hard problem. In chapter 3 we propose an algorithm based on
a novel stick breaking process which can scale substantially better than competitors
to high dimensional predictors while efficiently estimating the conditional density of
a response variable.
In this thesis we also focus on the challenging problem of learning a multivariate
density of a vector y

P

Y

Â„

<p indexed by features x

P

X

Â„

<q . This is an

important problem in many domains. For example, one may want to learn the joint
density of brain activity across sensors from MEG, EEG or fMRI data as a function
of patient tasks and characteristics. In modeling such data, it is most common to
assume either independence across sensors or that the data are multivariate Gaussian,
with the emphasis then on estimating a covariates dependent mean vector Âµpxq and

covariance matrix Î£pxq (Fyshe et al., 2012). Though flexible approaches have been

introduced to model the p  p feature-dependent covariance matrix (Pourahmadi,
1999; Chiu et al., 1996; Hoff and Niu, 2012; Gelfand et al., 2004; Williams, 1996),
the predictive performance of such models depends strictly on the validity of the
normality assumption.
4

There has been relatively limited attention on multivariate conditional density estimation. Notable exceptions include Krauthausen and Hanebeck (2010) and Davis
and Hwang (1998). Krauthausen and Hanebeck (2010) models y through a mixture of spherical Gaussians with feature-dependent weights, while Davis and Hwang
(1998) estimates f py |xq by placing kernels with varying bandwidths at each of the
training data points. These algorithms have been tested only on low dimensional
datasets, and may become computationally intractable in bigger problems. Tree
based models, typically applied to settings involving a univariate response but can
be easily implemented in multivariate settings (Death, 2002; Larsen and Speckman,
2004; Hothorn et al., 2006; Lutz and Buhlmann, 2006). Although performance is
often excellent in small to moderate dimensions, scaling to large numbers of features
is a general problem for usual tree-based models.

1.2 Literature Review
1.2.1

Mixture Models

Finite mixture models characterize the density of y
f py |p, Î³ q 

k
Â¸



P Y Â„ <m as

ph Ï†py; Î³h q,

(1.1)

h 1

where p

 pp1, . . . , pk qT

is a vector of probabilities summing to one, and Ï†p; Î³ q is

a kernel depending on parameters Î³

P Î“, which may consist of location and scale

parameters (McLachlan and Peel, 2000). There is a very rich literature on inference
for finite mixture models from both a frequentist (Figueiredo and Jain, 2002; Muthen
and Shedden, 1999) and Bayesian (Richardson and Green, 1997) perspective. In
practice, most of the frequentist literature focuses on maximum likelihood estimation,
with the Akaike information criterion (AIC) and other criteria used to estimate the
number of mixture components (Raftery and Fraley, 1998). Bayesian approaches
5

instead rely on placing a prior on the number of components and the componentspecific parameters, and hence may have some advantages in terms of accounting
for uncertainty in estimating the number of components while also regularizing the
component-specific parameters (Escobar and West, 1995; Richardson and Green,
1997). However, due to ease in computation, it has become popular to use over-fitted
mixture models in which a conservative upper bound on the number of components
is chosen.
Considering the finite mixture model in expression (1.1), a Bayesian specification is completed by choosing priors for the number of components k, the proba-

 pÎ³1, . . . , Î³k qT . TypDirichletpÎ±q prior with

bility weights p, and the component-specific parameters Î³
ically, k is assigned a Poisson or multinomial prior, p a
Î±  pÎ±1 , . . . , Î±k qT , and Î³h

 P0 independently, with P0 often chosen to be conjugate

to the kernel Ï†. As an example, when Ï† is the normal kernel and Î³ is a vector
containing mean and standard deviation, i.e. Î³

 pÂµ, ÏƒqT , a normal inverse-Gamma

prior is assigned to Î³.
Posterior computation can proceed via a reversible jump Markov chain Monte
Carlo (Richardson and Green, 1997) algorithm involving moves for adding or deleting
mixture components. Unfortunately, in making a k

Ã‘k

1 change in model dimen-

sion, efficient moves critically depend on the choice of proposal density. Stephens
(2000a) proposed an alternate Markov chain Monte Carlo method, which treats the
parameters as a marked point process, but does not have clear computational advantages relative to reversible jump. For these reasons It has become popular to use
over-fitted mixture models in which k is chosen as a conservative upper bound on
the number of components. From a practical perspective, the success of over-fitted
mixture models has been largely due to ease in computation.
As motivated in Ishwaran and Zarepour (2002), simply letting Î±h

 c{k

for

h P t1, . . . , k u and a constant c Â¡ 0 leads to an approximation to a Dirichlet process
6

mixture model for the density of y, which is obtained in the limit as k approaches infinity. An alternative finite approximation to a Dirichlet process mixture is obtained
by truncating the stick-breaking representation of Sethuraman (1994a), leading to
a similarly simple Gibbs sampling algorithm (Ishwaran and James, 2001). These
approaches are now used routinely in practice.
When working with mixture models, an important issue is the identifiability of
mixture parameters pÎ³, pq. In general, parameters are identifiable if distinct param-

eter values lead to different densities. Let f py |Î¸q be a mixture density defined as
in 1.1 with Î¸ being the vector of mixture parameters, i.e. Î¸

 pÎ³ T , pT qT .

It can

be easily shown that two different vectors Î¸ and Î¸1 can lead to the same mixture
density, i.e. f py |Î¸q

 f py|Î¸1q.

The lack of identifiability is mainly caused by the in-

variance of the likelihood to relabeling of the components and over-fitting. The first
identifiability issue does not create problems when the model is estimated through
maximum likelihood; however it causes major problems when dealing with Bayesian
methods based on Markov chains Monte Carlo. In the Bayesian literature the lack
of identifiability due to relabeling of the components is a challenging problem better
known as label switching problem (Stephens, 2000b; Lavine and West, 1992; Jasra
et al., 2005). An effective technique used to overcome this identifiability problem is
relabeling the clusters at each MCMC iteration using a post-processing algorithm.
Examples of such approach include Yao and Lindsay (2009), Stephens (2000b) and
Cron and West (2011). A more serious identifiability issue is due to the introduction
of equal components. As an example, consider two mixture models f py |ppkq , Î³ pkq q

and f py |ppk 1q , Î³ pk 1q q involving k and k

1 components respectively. It can be

easily shown that parameters pppkq , Î³ pkq q and pppk 1q , Î³ pk 1q q satisfying the following

constraints

pk 1q  ppkq ,

ph

h

pk 1q  Î³ pkq ,
h

Î³h

7

@h

k

pkq  ppk 1q

pk

k 1

pk 1q ,

pk

pk 1q  Î³ pk 1q

Î³k

1

k

lead to the same mixture density, i.e. f py |ppkq , Î³ pkq q  f py |ppk 1q , Î³ pk 1q q. This identifiability issue may result in over-fitted mixtures where identical and consequently
unnecessary components are introduced. To our knowledge no methods have been
proposed to solve this identifiability issue.
1.2.2

Divide and Conquer Algorithms and Tree Based Models

Divide and conquer algorithms fit surfaces to data by explicitly dividing the input
space into a nested sequence of regions, and by fitting simple surfaces within these
regions. A well known example of such algorithms is CART (classification and regression trees) (Breiman et al., 1984). Starting from a set including all observations
(root), tree based methods recursively splits each subset into subsets containing more
homogenous observations. Generally observations are allocated to different subsets
through greedy algorithms and the number of subsets is determined by pruning the
tree according to a model choice criterion such as AIC and BIC. Recently, tree based
methods relying on full Bayesian specifications have been introduced. Bayesian tree
models estimate the tree by placing a prior on the space of all trees and implementing
stochastic search algorithms to explore the entire space (Chipman et al., 1993; Wu
et al., 2007; Mallick, 1998).
Though CART models are appealing in providing a simple, flexible and interpretable mechanism of dimensionality reduction, it is well known that single tree
estimates commonly have high variance and poor performance. There is a rich machine learning literature proposing improvements based on bagging (Breiman, 1996),
boosting (Shapire et al., 1998) and random forests (Breiman, 2001). All these methods overcome the limit associated to single tree models by combining results generated from multiple trees. The multiple trees setup can certainly leads to better
mean square errors by reducing the variability associated to the estimates. However,
8

these approaches may become computationally intensive when dealing with massive
number of features.
Another divide-and-conquer algorithm particularly useful to reduce the variance
associated to single tree estimates is mixture of experts (Jacobs et al., 1991). As
opposed to other divide-and-conquer algorithms, mixture of experts relies on soft
partitioning algorithms that allows observations to lie simultaneously in different
subsets. A mixture of experts model is a mixture model in which the model parameters, including mixture weights, are functions of covariates. In practice, observations
are assigned to different experts by a gating network through a probabilistic model.
Then, within each expert, observations are considered identically distributed. A
variety of mixture of experts models have been proposed in the last twenty years.
Some of them deal with infinitely many experts (Rasmussen and Ghahramani, 2002;
Meeds and Osindero, 2006), others propose a hierarchical structure where the density within each expert is a mixture model (Jordan and Jacobs, 1994; Bishop and
Svensen, 2003).
1.2.3

Factor Model and Mixture of Factor Analyzers

Factor analysis has been one of the most flexible tools utilized to model the dependence structure of a p-dimensional vector of random variables through a sparse decomposition of a p  p covariance matrix, Î£  Î˜Î˜T

Î£0 with Î£0

 diagpÏƒ1, . . . , Ïƒpq.

This covariance decomposition is obtained by considering the following model for
yi

P <p
yi

 Âµ0

with Î˜ being a p

k

Î˜Î·i

i

Î·i

 Nk p0, I q

loading matrix and k

i

 Npp0, Î£0q

(1.2)

p. Model 1.2 implies that the

elements of yi are conditionally independent given the latent factors and the marginal
dependence among them is induced by the shared dependence on the latent factors.
The covariance matrix Î£ can be derived by marginalizing out Î·i . Tipping and Bishop
9

(2012) with their probabilistic principal component analysis showed that, under an
isotropic error model, i.e. Î£0



ÏƒI, the maximum likelihood estimate of the k

columns of the loading matrix converges to the first k principal components of the
data as Ïƒ approaches zero. Therefore, considering an isotropic error, it is possible to
combine the advantages of a probabilistic model with those of principal component
analysis.
Often there is interest in estimating the latent factors, interpreted as underlying
processes characterizing the data. However, these factors are not identifiable without
imposing further constraints on the loading matrix (Bernardo et al., 2003; Lopes and
West, 2004). In fact, for any k  k orthogonal matrix Î“, Î˜ and Î˜1

 Î˜Î“, lead to

the same covariance decomposition. To solve this identifiability issue and uniquely
estimate the latent factors one could constrain the loading matrix to be lower triangular (Geweke and Zhou, 1996; Aguilar and West, 2000) or orthogonal (Seber,
2004). Though inference on latent factors remains an interesting and open problem,
in many applications the main focus is the estimation of the covariance matrix Î£.
Latent factor models provide a low rank approximation of a large scale covariance
matrix and it is related to a set of articles, including Zou et al. (2006), Shen and
Huang (2008), Witten et al. (2009) and Johnstone and Lu (2009), which mainly
focused on sparse principal component analysis. In the analysis of factor models
another crucial point is the determination of the number of factors. The number of
latent factors can be determined through variable selection criteria (Onatski, 2005;
Minka, 2001), reversible jump algorithms (Lopes et al., 2011; Hastie and Green, 2012)
and adaptive Markov chains (Bhattacharya and Dunson, 2011).
Though factor model offers a flexible tool to describe the dependence structure
of a set of variables its applicability is limited by linearity. This limitation can
be overcome by combining local models in the form of finite mixture (Tipping and
Bishop, 1997). Mixtures of factor analyzers (MFA) model a p-dimensional vector of
10

observations as follows

yi

N
Â¸





ph Np pÂµh , Î˜h Î˜1h

Î£h0 q

(1.3)

h 1

P <p, Î˜h P <pk being the loading matrix, pp1, . . . , pN qT being positive
weights summing up to one, Î£h0  diag pÏƒh1 , . . . , Ïƒhp q. According to model 4.1, obwith Âµh

servations are assumed to belong to the hth cluster with probability ph and, within
each cluster, observations are modeled through a linear factor model (equation 1.2)
with parameters pÂµh , Î›h , Î£h0 q. Mixture of factor analyzers offers the potential to adequately model the density of high-dimensional observations while also allowing for
both clustering and local dimensionality reduction. In order to estimate the parameters of the MFA many approaches have been introduced. Some of them (Ghahramani
and Hinton, 1997; Zhou and Liu, 2008) estimate the model using the Expectation
Maximization algorithm (Dempster et al., 1977), others rely on variational inference (Ghahramani and Beal, 2000), others on full bayesian inference (Utsugi and
Kumagai, 2011).

1.3 Dissertation Outline
In this thesis we deal with different problems in mixture modeling such us identifiability, over-fitting and scalability to massive number of features.
The second chapter offers a possible solution to the identifiability and over-fitting
problem characterizing finite mixture models. In contrast to the majority of the
Bayesian literature on discrete mixture models, instead of drawing the componentspecific parameters tÎ³h u in 1.1 independently from a common prior, we propose a
joint prior for Î³

 pÎ³1, . . . , Î³k qT

that is chosen to assign low density to Î³h â€™s located

close together. We consider two types of repulsive priors, (i) priors guarding against
over-fitting by penalizing redundant kernels having close to identical locations and
11

scales and case (ii) priors discouraging closeness in only the locations to favor well
separated clusters.
The third chapter focuses on learning the conditional density of a response variable given an high dimensional vector of predictors. We present a multiresolution
approach which learns a multiscale dictionary of densities, constructed as Gaussian
within each set of a multiscale partition tree for the features. This tree is efficiently learned in a first stage using a fast and scalable graph partitioning algorithm
(Karypis and Kumar, 1999). Then, the conditional density f py |xq for each x P X is
expressed as a convex combination of coarse to fine scale dictionary densities. This
is accomplished in a Bayesian manner using a novel multiresolution stick-breaking
process, which allows the data to inform about the optimal bias-variance tradeoff.
The proposed model allows borrowing information across different resolution levels
and reaches a good compromise in terms of the bias-variance tradeoff. We show that
the algorithm scales efficiently to massive numbers of features.
Finally, the fourth chapter focuses on learning the conditional density of a multivariate vector of response given an high dimensional vector of predictors. In many
applications, there is interest in assessing how the density of a multivariate response
changes as function of features, with both the response and the predictor being
highly dimensional. To address this challenging problem, we propose a multiscale
predictor-dependent mixture of factor analyzers in which specific-component parameters depend on the path of the predictor vector through a multiscale partition tree.
By borrowing information across resolution levels, we allow local adaptivity in which
a single factor model may suffice in terms of the bias-variance tradeoff in certain
regions of the predictor space, while in other regions additional layers are required.

12

2
Repulsive Mixtures

Mixture models have been extensively utilized for density estimation, clustering and
as a component in flexible hierarchical models. In using mixture models for clustering, identifiability problems arise if mixture components are not sufficiently well
separated and the data for the different sub-populations contain substantial overlap.
Insufficiently separated components also create problems in using mixture models for
density estimation and robust modeling, as redundant components that are located
close together can be introduced leading to an unnecessarily complex model as well
as to various computational problems. Current practice in Bayesian mixture modeling generates the component-specific parameters from a common prior, which tends
to favor components that are close together. As an alternative, in this chapter, we
propose to generate mixture components from a repulsive process that favors placing
components further apart.

13

2.1

Bayesian Repulsive Mixture Models

2.1.1

Repulsive Densities

We seek a prior on the component parameters in (1.1) that automatically favors
spread out components near the support of the data. Instead of generating the
atoms Î³h independently from P0 , one could generate them from a repulsive process
that automatically pushes the atoms apart. This idea is conceptually related to the
literature on repulsive point processes (Huber and Wolpert, 2009). In the spatial
statistics literature, a variety of repulsive processes have been proposed. One such
model assumes that points are clustered spatially, with the vector of cluster centers Î³
having a Strauss density (Lawson and Clark, 2002), that is ppk, Î³ q9Î² k ÏrpÎ³ q where k is
the number of clusters, Î²

Â¡ 0, 0

Ï Â¤ 1 and rpÎ³ q is the number of pairwise centers

that lie within a pre-specified distance r of each other. A possibly unappealing
feature is that repulsion is not directly dependent on the pairwise distances between
the clusters. We propose an alternative class of priors, which smoothly push apart
components based on their pairwise distances.
Def 1. A density hpÎ³ q is repulsive if for any Î´

Â¡ 0 there is a corresponding  Â¡ 0
such that hpÎ³ q Î´ for all Î³ P Î“zG , where G  tÎ³ : dpÎ³s , Î³j q Â¡ ; s  1, . . . , k; j su
and d is a distance.
We consider two special cases (i) dpÎ³s , Î³j q is the distance between the sth and jth

kernel, (ii) dpÎ³s , Î³j q is the distance between sub-vectors of Î³s and Î³j corresponding to
only locations. Priors following definition 1(i) limit over-fitting in density estimation,
while priors following definition 1(ii) favor well-separated clusters.
As a convenient class of repulsive priors which smoothly push components apart,
we propose
Ï€ pÎ³ q  c1



k
Â¹



g0 pÎ³j q hpÎ³ q,

j 1

14

(2.1)

with c1 being a normalizing constant that can be intractable to calculate. The
dependence of c1 on k leads to complications in estimating k that motivate the use
of an over-specified mixture that treats k as an upper bound on the number of
components. The proposed prior is closely related to a class of point processes from
the statistical physics and spatial statistics literature called Gibbs processes (Daley
and Vere-Jones, 2008). We assume g0 : Î“ Ã‘ < and h : Î“k

Ã‘ r0, 8q are continuous

with respect to Lesbesgue measure, and h is bounded above by a positive constant c2
and is repulsive according to definition 1 with d differing across cases. It follows that
density Ï€ defined in (2.1) is also repulsive. For location-scale kernels, let Î³j

 pÂµj , Î£j q

and g0 pÂµj , Î£j q  Î¾ pÂµj qÏˆ pÎ£j q with Âµj and Î£j being respectively the location and the
scale parameters. A special hardcore repulsion is produced if the repulsion function
is zero when at least one pairwise distance is smaller than a pre-specified threshold.
Such a density implies choosing a minimal separation level between the atoms.
We avoid hard separation thresholds by considering repulsive priors that smoothly
push components apart. In particular, we propose two repulsion functions defined as
hpÎ³ q 

Â¹

tps,j qPAu

g tdpÎ³s , Î³j qu

(2.2)

hpÎ³ q  min g tdpÎ³s , Î³j qu

tps,j qPAu

(2.3)

 tps, j q : s  1, . . . , k; j su and g : < Ã‘ r0, M s a strictly monotone differentiable function with g p0q  0, g pxq Â¡ 0 for all x Â¡ 0 and M
8. It is
with A

straightforward to show that h in (2.2) and (2.3) is integrable and satisfies definition
1. The two alternative repulsion functions differ in their dependence on the relative distances between components, with all the pairwise distances playing a role in
(2.2), while (2.3) only depends on the minimal separation. A flexible choice of g
corresponds to
g tdpÎ³s , Î³j qu  exp
where Ï„



 Ï„ tdpÎ³s, Î³j quÎ½



,

(2.4)

Â¡ 0 is a scale parameter and Î½ is a positive integer controlling the rate at
15

which g approaches zero as dpÎ³s , Î³j q decreases. Figure 2.1 shows contour plots of the

prior Ï€ pÎ³1 , Î³2 q defined as (2.1) and satisfying definition 1(ii) with Î³1 , Î³2

P R, d the

Euclidean distance, g0 the standard normal density, the repulsive function defined as
(2.2) or (2.3) and g defined as (2.4) for different values of pÏ„, Î½ q. As Ï„ and Î½ increase,
the prior increasingly favors well separated components.
(I)

(II)

5

5

0

0

âˆ’5

âˆ’5
âˆ’5

0

5

âˆ’5

(III)

0

5

(IV)

5

5

0

0

âˆ’5

âˆ’5
âˆ’5

0

5

âˆ’5

0

5

Figure 2.1: Contour plots of the repulsive prior Ï€ pÎ³1 , Î³2 q satisfying definition
1(ii) under p2.1q either p2.2q or p2.3q and p2.4q with hyperparameters pÏ„, Î½ q equal
to pI qp1, 2q, pII qp1, 4q, pIII qp5, 2q and pIV qp5, 4q

2.1.2

Theoretical Properties

Theoretical properties of the proposed prior are considered under definition 1(ii),
though all results can be modified to accommodate definition 1(i). For some results,
the kernel will be assumed to depend only on location parameters, while for others
on both location and scale parameters. Let Î  be the prior induced on
where Fk is the space of all distributions defined as (1.1). Let
Â³

Â”8

 Fk ,

j 1

}  }1 denote the L1

norm and KLpf0 , f q  f0 logpf0 {f q refer to the Kullback-Leibler (K-L) divergence
between f0 and f . Density f0 belongs to the K-L support of the prior Î  if Î tf :
16

KLpf0 , f q

u

Â¡ 0 for all  Â¡ 0. Let the true density f0 : <m Ã‘ < be defined
Â°
as f0  kh1 p0h Ï†pÎ³0h q with Î³0h P Î“ and Î³0j s such that there exists an 1 Â¡ 0 such
that mintps,j q:s j u dpÎ³0s , Î³0j q Â¥ 1 , d being the Euclidean distance of sub-vectors of
Â°
Î³0j and Î³0s corresponding to only locations. Let f  kh1 ph Ï†pÎ³h q with Î³h P Î“. Let
Î³  Ï€ and Ï€ satisfy definition 1(ii). Let p  Î» with Î»  DirichletpÎ±q and k  Ï‘
with Ï‘pk  k0 q Â¡ 0. Let Î¸  pp, Î³ q. These assumptions on f0 and f will be referred
0

to as condition B0. The next lemma provides sufficient conditions under which the
true density is in the K-L support of the prior for location kernels.
Lemma 2. Assume condition B0 is satisfied with m



1. Let D0 be a compact

set containing location parameters pÎ³01 , . . . , Î³0k0 q. Let Ï† and Ï€ satisfy the following
conditions:

P Y, the map Î³ Ã‘ Ï†py; Î³ q is uniformly continuous
A2. for any y P Y, Ï†py; Î³ q is bounded above by a constant

(
Â³ 
A3. f0 log supÎ³ PD Ï†pÎ³ q  log tinf Î³ PD Ï†pÎ³ qu 8

A1. for any y

0

0

P Î“k
with mintps,j q:s j u dpxs , xj q Â¥ Ï… for Ï… Â¡ 0 there is a Î´ Â¡ 0 such that Ï€ pÎ³ q Â¡ 0
for all Î³
satisfying ||Î³  x||1 Î´
A4. Ï€ is continuous with respect to Lebesgue measure and for any vector x

Then f0 is in the K-L support of the prior Î .
Lemma 3. The repulsive density in (2.1) with h defined as either (2.2) or (2.3)
satisfies condition A4 in lemma 2.
The next lemma formalizes the posterior rate of concentration for univariate
location mixtures of Gaussians.
Lemma 4. Let condition B0 be satisfied, let m



1 and Ï† be the normal kernel

depending on a location parameter Âµ and a scale parameter Ïƒ. Assume that condition

piq, piiq and piiiq of theorem 3.1 in Scricciolo (2011) and assumption A4 in lemma 2
are satisfied. Furthermore, assume that
17

C1) the joint density Ï€ leads to exchangeable random variables and for all k the
density of Âµ1 satisfies Ï€m p|Âµ1 | Â¥ tq Ã€ exp pq1 t2 q for a given q1

marginal

C2) there are constants u1 , u2 , u3
 Â¤ u3

Ï€ p||Âµ  Âµ0 ||1

Â¡0

Â¡ 0, possibly depending on f0, such that for any

Â¤ q Â¥ u1 exppu2k0 logp1{qq

Then the posterior rate of convergence relative to the L1 metric is n

 n1{2 log n.

Lemma 4 is basically a modification of theorem 3.1 in Scricciolo (2011) to our
proposed repulsive mixture model. Lemma 5 gives sufficient conditions for Ï€ to
satisfy condition C1 and C2 in lemma 4.
Lemma 5. Let Ï€ be defined as (2.1) and h be defined as either (2.2) or (2.3), then
Ï€ satisfies condition C2 in lemma 4. Furthermore, if for a positive constant n1 the
function Î¾ satisfies Î¾ p|x| Â¥ tq Ã€ exppn1 t2 q, Ï€ satisfies condition C1 in lemma 4.
As motivated above, when the number of mixture components is chosen to be
conservatively large, it is appealing for the posterior distribution of the weights of
the extra components to be concentrated near zero. Theorem 6 formalizes the rate of
concentration with increasing sample size n. One of the main assumptions required
in theorem 6 is that the posterior rate of convergence relative to the L1 metric is
Î´n

 n1{2plog nqq with q Â¥ 0. We provided the contraction rate, under the proposed

prior specification and univariate Gaussian kernel, in lemma 4. However, theorem
6 is a more general statement and it applies to multivariate mixture density of any
kernel.
Theorem 6. Let assumptions B0  B5 be satisfied. Let Ï€ be defined as (2.1) and h

 maxpÎ±1, . . . , Î±k q m{2 and for positive
constants r1 , r2 , r3 the function g satisfies g pxq Â¤ r1 xr for 0 Â¤ x r3 then

be defined as either (2.2) or (2.3). If Î±Ì„

2



lim lim sup En0 P

M

Ã‘8

Ã‘8

n

#



min

tÎ¹PSk u

k
Â¸



pÎ¹piq

i k0 1

18

Â¡ M n1{2plog nqqp1

p

q{ q

s k0 ,Î± sr2

+

0

with spk0 , Î±q

 k0  1

mk0

Î±Ì„pk  k0 q, sr2

possible permutations of t1, . . . , k u.

 r2

m{2  Î±Ì„ and Sk the set of all

Theorem 6 is a modification of theorem 1 in Rousseau and Mengersen (2011) to
our proposed repulsive mixture model. Theorem 6 implies that the posterior expectation of weights of the extra components is of order Opn1{2 plog nqqp1 spk0 ,Î±q{sr2 q q.
When g is defined as (2.4), parameters r1 and r2 can be chosen such that r1
r2

 Î½.

 Ï„ and

When the number of components is unknown, with only an upper bound known,
the posterior rate of convergence is equivalent to the parametric rate n1{2 (Ishwaran
et al., 2001). In this case, the rate in theorem 6 is n1{2 under usual priors or our
repulsive prior. However, in our experience using usual priors, the sum of the extra
components can be substantial in small to moderate sample sizes, and often has
high variability. As we show in Section 2.3, for repulsive priors the sum of the extra
component weights is close to zero and has small variance for small as well as large
sample sizes. When an upper bound on the number of components is unknown, the
posterior rate of concentration is n1{2 plog nqq with q

Â¡ 0.

In this case, according

to theorem 6, using our prior specification the logarithmic factor in theorem 1 of
Rousseau and Mengersen (2011) can be improved.

2.2 Posterior Computation and Parameter Calibration
2.2.1

Posterior Computation

For posterior computation, we use a slice sampling algorithm (Neal, 2003), a class of
Markov chain Monte Carlo algorithms widely used for posterior inference in infinite
mixture models (Kalli et al., 2011). Letting g0 be a conjugate prior, introduce a

19

latent variable u which is jointly modeled with Î³ through
Ï€ pÎ³1 , . . . , Î³k , uq9



k
Â¹



g0 pÎ³h q 1 thpÎ³1 , . . . , Î³k q Â¡ uu .

h 1

Here 1pB q is the indicator function, equalling 1 if the event B occurs and 0 otherwise.

Marginalizing out u, we recover the original density Ï€ pÎ³1 , . . . , Î³k q. For a repulsion
function defined as (2.3), let Bj

 Â“ts:sju rÎ³j : gtdpÎ³s, Î³j qu Â¡ us. When the repulsion

function is defined as (2.2), one can introduce a latent variable for each product
term. Under repulsive priors satisfying definition 1(i), the set Bj might not be easy
to compute. However, when covariance matrices are constrained to be diagonal,
vectors Î³j s can be easily sampled element-wise. For multivariate observations, the
location parameter vector can be sampled element-wise from truncated distributions.
For simplicity, assume that h is defined as 2.3, Ïˆ is the Inverse-Gamma density
with parameters paÏƒ , bÏƒ q, g0 is the m-variate standard normal density and Ï† is the
m-variate spherical normal kernel. Let Si

P t1, . . . , ku be the variable indicating

which cluster the ith observation belongs to. Let nj be the number of data points
in the jth cluster and let yÌ„j be the average of observations in the jth cluster. Let
u

 g1puq, Î±p  pÎ±1

n1 , . . . , Î±k

nk q and Î³j

 pÂµj , Ïƒj q.

Then the sampling

algorithm can be summarized by the following steps:
Step 1. Update Si , for i P t1, . . . , nu, by multinomial sampling

pSi|q  M ultpl1, . . . , lk q,

lj

 Â°k pj Ï†pyi; Âµj , Ïƒj I q ;
h1 ph Ï†pyi ; Âµh , Ïƒh I q

Step 2. For repulsive priors satisfying definition 1(ii), sample pÂµj , Ïƒj q from

pÂµj |q  fÂµ 9N p1
j

nj {Ïƒj q1 yÌ„j nj {Ïƒj , I p1

20

(

nj {Ïƒj q1 1tÂµj

P ApÂµj qu

$
&

pÏƒj |q  fÏƒ  IG %aÏƒ
j

Â¸

,
.

1
pyi  Âµj qT pyi  Âµj q2 ti:S j u

nj m
, bÏƒ
2

i

For repulsive priors satisfying definition 1(i) sample pÂµj , Ïƒj q from
Âµj

 fÂµ

j

;

1{Ïƒj

 fÏƒ1 9fÏƒ 1tÏƒj P ApÏƒj qu
j

j

The set A is defined as Apq  t : dpÎ³j , Î³s q Â¡ u , @s  j u with dp, q being defined as
the symmetric K-L divergence for repulsive priors satisfying definition 1(i) and the
Euclidean distance for repulsive priors satisfying definition 1(ii).

Step 3. Sample u and p from

pu|q  U n t0, hpÎ³ qu ,
2.2.2

p  Dirichlet pÎ±p q

Calibration

An important issue in implementing repulsive mixture models is elicitation of the
repulsion hyper-parameters pÏ„, Î½ q. Although a variety of strategies can be considered,
we propose a simple approach that can be used to obtain a default hyper-parameter
choice in general applications. In case (i) we choose dp, q as the symmetric KullbackLeibler divergence defined for Gaussian kernels as
s12

 dpÎ³1, Î³2q  trpÎ£1Î£2 1q

1
trpÎ£
1 Î£2 q  2m

pÂµ1  Âµ2qT pÎ£1 1

1
Î£
2 qpÂµ1  Âµ2 q,

while in case (ii) we use the Euclidean distance between the location parameters.
For both case (i) and case (ii), define dÂ¯ as the mean of pairwise distances between
Â°
atoms, dÂ¯  np1Aq ps,j qPA dpÎ³s , Î³j q with A

 tps, j q : s  1, . . . , k; j

su and npAq

the cardinality of set A. Let f1 and f2 denote the densities of dÂ¯ under repulsive and
non-repulsive priors respectively, with p%j , Ï‚j q the mean and standard deviation of fj
for j

 1, 2. We choose pÏ„, Î½ q so that f1 and f2 are well-separated using the following

definition of separation (Dasgupta, 1999).
21

Def 7. Given a positive constant c, f1 and f2 are c-separated if %1 %2
We have found that Î½

Â¥ c maxpÏ‚1, Ï‚2q.

 2 and Î½  1 provide good default values in case (i) and

(ii) respectively and we fix Î½ at these values in all our applications below. For a
given value of Î½, Ï„ is found by starting with small values, estimating the mean and
variance of dÂ¯ through Monte Carlo draws, and incrementing Ï„ until definition 7 is
satisfied for a pre-specified c. We use c

 4 in our implementations.

A sensitivity

analysis for different values of c can be found in appendix B.

2.3 Synthetic Examples
Simulation examples were considered to assess the performance of the repulsive prior
in density estimation, clustering and emptying of extra components. Figure 2.2 plots
the true densities in the various cases that we considered. For each synthetic dataset,
repulsive and non-repulsive mixture models were compared considering a fixed upper
bound on the number of components; extra components should be assigned small
probabilities and hence effectively excluded. The slice sampler was run for 10, 000
iterations with a burn-in of 5, 000. The chain was thinned by keeping every 10th
draw. To overcome the label switching problem, the samples were post-processed
following the algorithm of Stephens (2000b). Details on parameters involved in the
true densities, choice of prior distributions and methods used to compute quantities
presented in this section can be found in Appendix B.
Repulsive mixtures satisfying definition 1(i) and non-repulsive mixtures were compared. For this experiment 1, 000 draws from a standard normal density and a two
component mixture of overlapping normals was considered. Both repulsive and nonrepulsive mixtures were run considering six as the upper bound of the number of
components. Table 2.1 shows posterior summaries of parameters involved in the
components with highest weights. Clearly, repulsive mixtures lead to a more parsi22

(I)

(II)

0.6

0.6

0.4

0.4

0.2

0.2

0
âˆ’5

0

0

5

âˆ’2

0

(III)

(IV)

1

3

0.8

2

0.6

1

0.4

0

0.2

âˆ’1

0
âˆ’3

âˆ’2

âˆ’1

0

2

1

2

âˆ’2
âˆ’2

3

âˆ’1

0

1

2

3

Figure 2.2: pI q Standard normal density (solid), two-component mixture of normals sharing the same location parameter (dash) and Studentâ€™s t density (dash-dot),
referred as pIa, Ib, Icq, pII q two-components mixture of poorly (solid) and well separated (dot-dash) Gaussian densities, referred as pIIa, IIbq, pIII q mixture of poorly
(solid) and well separated (dot-dash) Gaussian and Pearson densities, referred as
pIIIa, IIIbq, pIV q two-components mixture of two-dimensional non-spherical Gaussians
monious representation of the true densities and more accurate parameter estimates.
The mean and standard deviation of the K-L divergence under the first data example
were p0.003, 0.002q and p0.004, 0.002q for non-repulsive and repulsive mixtures respec-

tively; while under the second data example were p0.006, 0.003q and p0.009, 0.003q
for non-repulsive and repulsive mixtures respectively. Therefore, repulsive mixtures
were able to concentrate more on the reduced model while performing similarly to
non-repulsive mixtures in estimating the true density.
Repulsive mixtures satisfying definition 1 (ii) and non-repulsive mixtures were
compared to assess clustering performance. Table 2.2 shows summary statistics of
the K-L divergence, the misclassification error and the sum of extra weights under repulsive and non-repulsive mixtures with six mixture components as the upper bound.

23

Table 2.2 shows also the misclassification error resulting from hierarchical clustering
(Locarek-Junge and Weihs, 2009). In practice, observations drawn from the same
mixture component were considered as belonging to the same category and for each
dataset a similarity matrix was constructed. The misclassification error was established in terms of divergence between the true similarity matrix and the posterior
similarity matrix. As shown in table 2.2, the K-L divergences under repulsive and
non-repulsive mixtures become more similar as the sample size increases. For smaller
sample sizes, the results are more similar when components are very well separated.
Since a repulsive prior tends to discourage overlapping mixture components, a repulsive model might not estimate the density quite as accurately when a mixture of
closely overlapping components is needed. However, as the sample size increases, the
fitted density approaches the true density regardless of the degree of closeness among
clusters. Again, though repulsive and non-repulsive mixtures perform similarly in
estimating the true density, repulsive mixtures place considerably less probability on
extra components leading to more interpretable clusters. In terms of misclassification error, the repulsive model outperforms the other two approaches while, in most
cases, the worst performance was obtained by the non-repulsive model.
Potentially, one may favor fewer clusters, and hence possibly better separated
clusters, by penalizing the introduction of new clusters more through modifying the
precision in the Dirichlet prior for the weights; in appendix B, we demonstrate that
this cannot solve the problem.

2.4 Real Data
We tested the performance of our proposed prior specification on three real datasets.
The first involves 82 measurements of the velocities in km/s of galaxies diverging
24

Table 2.1: Posterior mean and standard deviation of weights, location and scale
parameters under dataset drawn from densities pIa, Ibq

True
N-R
R

Density Ia
Comp 1
pÌ‚1
ÂµÌ‚1
ÏƒÌ‚1
1
0
1
0.53 0.01 0.85

Density Ib
pÌ‚1
0.7
0.44

0.00

0.67

Comp 1
ÂµÌ‚1
ÏƒÌ‚1
0
0.2
0.08
1.21

pÌ‚2
0.3
0.34

Comp 2
ÂµÌ‚2
ÏƒÌ‚2
0
2
0.12 1.33

p0.16q p0.04q p0.25q p0.06q p0.10q p1.05q p0.06q p0.16q p1.11q
0.87

0.84

0.02

0.28

0.27

0.09

2.36

p0.07q p0.01q p0.04q p0.05q p0.03q p0.02q p0.09q p0.23q p0.75q

from our own (Escobar and West (1995), Richardson and Green (1997)), the second
consists of the acidity index measured in a sample of 155 lakes in north central
Wisconsin (Richardson and Green (1997)), and the third consists of 150 observations
from three different species of iris each with four measurements (Wang, 2010).
For the first two datasets, a repulsive mixture satisfying definition 1(i) was considered and a five-component mixture model was fit while for the third dataset a
repulsive mixture satisfying definition 1(ii) was considered and both six components
and ten components were considered as the upper bound. The same prior specification, Markov chain Monte Carlo sampler, and relabeling technique as in section 2.3
were utilized.
For the galaxy data, figure 2.3 reveals that there are three non-overlapping clusters with the one close to the origin relatively large compared to the others. Although
this large cluster might be interpreted as two highly overlapping clusters, it appears
to be well approximated by a single normal density. Richardson and Green (1997)
and Escobar and West (1995) estimated the number of components, obtaining a posterior distribution on k concentrating on values ranging from 5 to 7. This may be
due to the non-repulsive prior allowing closely overlapping components, favoring relatively large values of k. Figure 2.3 reveals that the non-repulsive prior specification
leads to two overlapping and essentially indistinguishable clusters. Under repulsive

25

Table 2.2: Mean and standard deviation of K-L divergence, misclassification error
and sum of extra weights resulting from non-repulsive mixture and repulsive mixture with a maximum number of clusters equal to six under different synthetic data
scenarios.
n=100

n=1,000

Ic
IIa
K-L divergence
N-R
0.05 0.03
p0.03q p0.01q p0.02q
R
0.06 0.05
p0.03q p0.02q
Misclassification
HCT
0.12 0.11
N-R
0.69 0.26
p0.10q p0.10q
R
0.53 0.18
p0.10q p0.09q
Sum of extra weights
N-R
0.30 0.21
p0.10q p0.11q
R
0.08 0.08
p0.05q p0.07q
K-L divergence
N-R
0.01 0.01
p0.00q p0.00q
R
0.01 0.01
p0.00q p0.00q
Misclassification
HCT
0.05 0.42
N-R
0.65 0.24
p0.11q p0.08q
R
0.46 0.13
p0.16q p0.04q
Sum of extra weights
N-R
0.30 0.21
p0.11q p0.11q
R
0.10 0.09
p0.04q p0.06q

IIb

IIIa

IIIb

IV

0.07

0.05

0.08

0.22

0.08

0.07

0.09

0.28

0.04
0.06

0.12
0.17

0.08
0.05

0.21
0.13

0.01

0.10

0.01

0.05

0.09

0.16

0.07

0.13

0.02

0.04

0.02

0.06

0.01

0.01

0.01

0.02

0.01

0.01

0.01

0.03

0.01
0.03

0.42
0.14

0.01
0.03

0.20
0.19

0.00

0.03

0.00

0.17

0.03

0.16

0.03

0.29

0.00

0.01

0.00

0.25

p0.02q p0.03q p0.05q

p0.03q p0.03q p0.03q p0.04q
p0.04q p0.09q p0.06q p0.05q
p0.02q p0.05q p0.01q p0.02q
p0.07q p0.09q p0.07q p0.08q
p0.02q p0.05q p0.02q p0.03q
p0.00q p0.00q p0.00q p0.01q
p0.00q p0.00q p0.00q p0.01q
p0.04q p0.09q p0.03q p0.02q
p0.01q p0.02q p0.01q p0.01q
p0.04q p0.10q p0.03q p0.03q
p0.00q p0.01q p0.00q p0.03q

26

priors, no clusters overlap significantly and unnecessary components receive a weight
close to zero.
For the acidity data, figure 2.3 suggests that two clusters are involved. Since
one of them appears to be highly skewed, we expect that three clusters might be
needed to approximate this density well. Richardson and Green (1997) obtained a
posterior for k almost equally concentrated on values of k ranging from 3 to 5. Figure
2.3 shows the estimated clusters for both repulsive and non-repulsive priors. With
non-repulsive priors, four clusters receive significant weight and two of them overlap
significantly. With repulsive priors, only three clusters receive significant weight and
all of them appear fairly separated.
The iris data were previously analyzed by Sugar and James (2003) and Wang
(2010) using new methods to estimate the number of clusters based on minimizing
loss functions. They concluded the optimal number of clusters was two. This result
did not agree with the number of species due to low separation in the data between
two of the species. Such point estimates of the number of clusters do not provide
a characterization of uncertainty in clustering in contrast to Bayesian approaches.
Repulsive and non-repulsive mixtures were fitted under different choices of upper
bound on the number of components. Since the data contains three true biological
clusters, with two of these having similar distributions of the available features, we
would expect the posterior to concentrate on two or three components. Posterior
means and standard deviations of the three highest weights were p0.30, 0.23, 0.13q

and p0.05, 0.04, 0.04q for non-repulsive and p0.56, 0.29, 0.08q and p0.05, 0.04, 0.03q for
repulsive. Clearly, repulsive priors lead to a posterior more concentrated on two
components, and assign low probability to more than three components. Figure
2.4 shows the density of the total probability assigned to the extra components.
This quantity was computed considering the number of species as the true number
of clusters. According to figure 2.4, our repulsive prior specification leads to extra
27

component weights very close to zero regardless of the upper bound on the number of
components. The posterior uncertainty is also small. Non-repulsive mixtures assign
large weight to extra components, with posterior uncertainty increasing considerably
as the number of components increases.
(I)

(II)

(III)

1

1

1

0.8

0.8

0.8

0.6

0.6

0.6

0.4

0.4

0.4

0.2

0.2

0.2

0

âˆ’2

0

0

2

âˆ’2

(IV)

0

0

2

(V)
1

1

0.8

0.8

0.8

0.6

0.6

0.6

0.4

0.4

0.4

0.2

0.2

0.2

âˆ’2

0

2

0

âˆ’2

0

0

2

(VI)

1

0

âˆ’2

2

0

âˆ’2

0

2

Figure 2.3: Histogram of galaxy data (I) and acidity data (IV) overlaid with a
nonparametric density estimate using Gaussian kernel density estimation. Estimated
clusters under galaxy data for non-repulsive (II) and repulsive (III) priors and under
acidity data for non-repulsive (V) and repulsive (VI) priors

28

20

15

10

5

0
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

Figure 2.4: Density of sum of extra weights under k=6 for non-repulsive (solid)
and repulsive (dash) and k=10 components for non-repulsive (dash-dot) and repulsive
(dot)

29

3
Dictionary Learning for Conditional Distributions

Estimation of the conditional distribution of a response given high-dimensional features is a challenging problem arising in a number of important application areas,
including neuroscience, genetics, and video processing. For example, one might desire
automated estimation of a predictive density for a continuous or categorical neurologic phenotype of interest, such as intelligence or a creativity score, on the basis
of available data for a patient including neuroimaging. The challenge is to estimate
the probability density function of the phenotype based on a high-dimensional image of the subjectâ€™s brain. In real data applications, often the relationship between
predictors and response is non linear so that it is important to allow not only the
mean but also the variance and shape of the response density to change flexibly with
features, which are massive dimensional. In this chapter, we propose a novel stick
breaking multiresolution that can flexibly and efficiently characterize the density of a
response variable given high dimensional predictors. The algorithm scales efficiently
to massive numbers of features, and can be implemented with slice sampling.

30

3.1 Methodology
We aim to build a flexible and scalable model for the density of y
set of predictors. Let x

P

predictor random variable.

P

< given a

Â„ <p be a p-dimensional Euclidean vector-valued
Let f pxq denote the marginal probability density of x.
X

We assume that f pxq concentrates around a lower-dimensional, possibly nonlinear,

subspace M. For example, M could be a union of affine subspaces, or a smooth
compact Riemannian manifold. Let y

P Y Â„ < be a real-valued target variable. Let

x and y be sampled from some true but unknown joint distribution. We would like
to learn f py |xq. We assume that we obtain n independently and identically sampled

observations, pyi , xTi qT for i P t1, . . . , nu.
3.1.1

Model Overview

We propose a general modular approach to learn the conditional distribution of y
given an high dimensional vector of predictors x consisting in two components: (i)
a tree decomposition of the feature space, (ii) an assumed form of the conditional
probability model. A tree decomposition T yields a multiscale partition of the data
or the ambient space in which the data live. Starting from the coarsest scale, corresponding to the entire set, each set is split into two or more mutually exclusive
subsets. This process continues until some convergence criteria is satisfied, e.g. the
number of observations allocated to the finest scales is below some chosen threshold.
Figure 3.1(i) shows a dyadic partition of the predictor space where a generic set Cj,s
is partitioned into two subsets Cj
Cj,s

 Cj

1,s1

Â”

1,s1

Cj

1,s2

 0 as the root node/cell.

Cj

,

such that
1,s1

Â“

Cj

1,s2

H

 tCj,suKs1 provides a partition of X . We define
For each j Â¡ 0, each Cj,s has a unique parent node
j

For each scale j, the set of cells Cj
j

1,s2

and Cj

31

Cj 1,s1 containing Cj,s , and conversely, any Cj,s

Â„ Cj1,s1 is called a child of Cj1,s1 .

Let the set of ancestors and descendants of Cj,s be respectively defined as:

Aj,s

 tpj 1, s1q : j 1

j, Cj,s

Â„ Cj1,s1 u

, Dj,s

 tpj 1, s1q : j 1 Â¡ j, Cj1,s1 Â„ Cj,su

Considering a given tree decomposition T of X , each xi

PX

(3.1)

has an associated

path characterized by the sets including xi (see figure 3.1(iii)). We assume that
the density of yi depends on xi through this tree partition. Specifically, for each
node pj, sq in the partition tree, we define a weight Ï€j,s and dictionary density fj,s

as shown in figure 3.1(ii). Then, the conditional density f pyi |xi q will be a mixture
of densities with components depending on the sets contained in the path of xi (see
figure 3.1(iv)). In the extreme case in which two predictor values x and x1 belong to
the same leaf partition sets, the conditional distributions f py 1 |x1 q and f py |xq will be
identical. If the two paths differ only in the final generation or two, the conditional
densities will typically be similar but not identical.
3.1.2

Model Specification

Assuming that the number of levels in the partition tree is k, we define the conditional density f py |xq as the convex combination of densities tfj,sj pxq ukj1 with weights

tÏ€j,s pxqukj1, i.e.
j

f py |xq 

k
Â¸



Ï€j,sj pxq fj,sj pxq ,

(3.2)

j 1

with sj pxq being the subset located at level j containing x, pÏ€j,sj pxq , fj,sj pxq q being
the weight and dictionary density associated to node pj, sj pxqq, 0

Â°k

Â¤

Ï€j,sj pxq and

 Ï€j,sj pxq  1. According to model 3.2, only observations with predictors allocated

j 1

to node pj, sq, i.e. tyi : xi

P Cj,su, will have a mixture components with weight Ï€j,s
32

C11

(i)

@
R
@

C21

C31

rp21 , f21 s

C22

@
R
@

@
R
@

C32 C33
...

rp11 , f11 s

(ii)

@
R
@

rp22 , f22 s

@
R
@

rp31 , f31 s rp32 , f32 s

C34
...

...

@
R
@

...
...

C11

(iii)

xi

@
R
@

PC

C22

@
R
@

C33
...
(iv)

...

f pyi |xi q  p11 f11

p22 f22

p33 f33

...

Figure 3.1: Partition tree schematic: (i) Multiscale partition of the data. (ii)
Estimate dictionary density and weight associated to each set. (iii) Nodes along the
tree containing xi P <p . (iv) Conditional density of yi given xi defined as a convex
combination of densities associated to the nodes containing xi .

33

and density fj,s . Notice that, as the weight associated to the first level of resolution
approaches one, a non predictor-dependent density for y is obtained.
According to model (3.2), one observation can lie in subsets located at different
resolution levels. This is critical in achieving a good compromise between bias and
variance through borrowing information across different resolution levels. Though
the proposed approach is reminiscent of a mixture of experts model (Jacobs et al.,
1991), the two approaches are quite different, since under (3.2), neither mixture
weights nor dictionary densities directly depend on predictors. This allows our model
to scale efficiently to high dimensional predictors.
We let weights in 3.2 be generated by a stick-breaking process (Sethuraman,
1994b). For each node pj, sq in the partition tree, we define a stick length Vj,s
Betap1, Î±q. Then, we define weights in 3.2 as follows
Ï€j,s

 Vj,s

Â¹

pj 1 ,s1 qPAj,s

r1  Vj1,s1 s ,

for j



k

 1 for all s P t1, . . . , Kk u. This condition will
 1 for any path s  ts1, . . . , sk u. We refer to this prior as a

with Aj,s defined as in 3.1 and Vk,s
ensure that,

Â°k

 Ï€j,sj

j 1

multiresolution stick-breaking process. The parameter Î± encodes the complexity of
the model, with Î±  0 corresponding to the case in which f py |xq  f py q.

3.2 Estimation
We desire a strategy that estimates posteriors over all potential marginal distributions so as to automatically obtain estimates of uncertainty. Moreover, we would
like a procedure with a few hyper-parameters as possible. These motivate using a
fully Bayesian strategy. A fully Bayesian approach would construct a large number of partitions, and integrate over them to obtain our posteriors. However, such a
fully Bayesian is computational intractable for the ultrahigh-dimensionality problems
34

that motivate this work (p

P Op106q), so we adopt a hybrid strategy.

This hybrid

strategy is based on a two stage algorithm where first the observations are allocated
to different subsets in a tree fashion using an efficient partitioning algorithm and
then, considering the partition as fixed, a multiresolution stick-breaking process is
estimated using Bayesian methods.
Specifically, we employ METIS (Karypis and Kumar, 1999), a well-known relatively efficient graph partitioning algorithm with demonstrably good empirical performance on a wide range of graphs. We construct a weighted graph as done for the
construction of diffusion maps (Coifman and Lafon, 2006). In practice, we add an
edge between each pairs pxv , xu q and assign to any such edge weight eÏuv , where Ïuv
2

is a given metric. In all applications below, Ïuv is defined as the Euclidean distance
between predictors xu and xv . Starting from the coarse scale, subsets will be split
using METIS until the number of observations in the subsets located at the finest
scale will drop below some chosen threshold Ï„ . More formally let us define the number of levels k as the one satisfying the following conditions
Â°n

 1pxi P Cj,sj q Â¥ Ï„ ,

@Cj,s

i 1

Â°n

 1pxi P Ck

i 1

1,sk

1

q

j

Ï„,

f.s. sk

with j
1

Â¤k

P t1, . . . , Kk 1u

where 1pq is the indicator function and n is the number of observations. The above
conditions imply that each subset located at the finest scale will have at least Ï„
observations. Once the tree is constructed, we define the conditional density of yi as
the mixture density in 3.2. Though more complicated densities can be considered,
dictionary densities fj,s will be estimated by assuming a normal form, i.e. fj,s



N pÂµj,s , Ïƒj,s q. In particular, densities corresponding to a particular partition set will

be estimated considering only observations belonging to that partition set. To be
specific, for estimating density fj,s , we use observations Yj,s
35

 tyi : xi P Cj,su.

3.2.1

Full Conditionals

Parameters involved in the dictionary densities can be estimated using either frequentist or Bayesian methods. Bayesian methods are appealing since they can avoid
singularities associated with traditional maximum likelihood inference, the prior has
an appealing role as a regularizer, and we can characterize uncertainty in dictionary
learning through the resulting posterior. Hence, parameters involved in dictionary
densities will be estimated through Bayesian methods and inference on stick breaking weights and dictionary density parameters will be carried out using the Gibbs
sampler.
For this purpose, introduce the latent variable Si

P t1, . . . , ku, for i P t1, . . . , nu,

denoting the multiscale level used by the ith subject. Assuming data are normalized

 N p0, I q and Ïƒj,s  IG pa, bq for the means and variances of the dictionary densities associated to node pj, sq. Let nj,s be the number of
Â°
observations allocated to node pj, sq, i.e. nj,s  ni1 1pxi P Cj,s q1pSi  j q. Define
Ij,s  ti : xi P Cj,s , Si  j u. Each Gibbs sampler iteration can be summarized in the
prior to analysis, we let Âµj,s

following steps.

Step 1. Update Si by sampling from the multinomial full conditional with
ppSi

 j | q  Â°k Ï€j,s px qfj,s px qpyiq
h1 Ï€h,s px q fh,s px q pyi q
j

i

h

Step 2. Update Vj,s , for all s P t1, . . . , Kj u and j
p pVj,s |q  BetapÎ²p , Î±p q,

Î²p

1

j

i

i

h

i

P t1, . . . , ku, by sampling from
nj,s

Î±p

Î±

Â¸

P p q

n`

` D j,s

Step 3. Update Âµj,s , for all s P t1, . . . , Kj u and j

P t1, . . . , ku, by sampling from

ppÂµj,s |q  N pMj,s nj,s {Ïƒj,s yÌ„j,s , Mj,s q
36

with Mj,s

 p1

nj,s {Ïƒj,s q1 , yÌ„j,s

 Â°iPI

j,s

yi .

Step 4. Update Ïƒj,s , for all s P t1, . . . , Kj u and j
ppÏƒj,s |q  IG pa
where sÌ„ 

3.2.2

Â°

P

i Ij,s

P t1, . . . , ku, by sampling from

nj,s {2, b

pyi  Âµj,sq2.

Predictions

Consider the case we want to predict the response yn
on predictors xn
y pnq

sÌ„{2q

1

 py1, . . . , ynq.

1

for a future observation based

and previous observations pxpnq , y pnq q with xpnq

 px1, . . . , xnq and

Because the partitioning strategy that we adopted lacks an el-

egant out-of-sample embedding function (unlike other partitioning strategies), we
adopt a Voronoi expansion procedure by which the new vector of features xn

1

is al-

located to Cj,k â€™s having the closest centers with respect to some metric Ï. Summaries
of the predictive density of yn
(i) for each scale j

1

will be computed as follows:

Â¤ k, allocate predictors xn

1

to Cj,k â€™s having the closest centers

with respect to Ï
(ii) run the Gibbs sampler for H iterations, and at the hth iteration:
!

a) sample parameters

phq phq phq

)

Ïƒj,s , Âµj,s , Ï€j,s

@j,@s

from the posterior, following the

procedure explained in Â§3.2.1
b) sample yÌ‚nh

1

from

yÌ‚ns 1



k
Â¸



j 1



phq

phq

phq

Ï€j,spxn 1 q N Âµj,spxn 1 q , Ïƒj,spxn 1 q

(iii) given the sequence yÌ‚nh

(H
1 h 1,



summaries of the predictive density of the

response variable such as mean, variance and quantiles can be computed.

37

3.3 Simulation Studies
In order to assess the predictive performance of the proposed model, different simulation scenarios were considered. Let n be the number of observations, y

P < the

response variable and x P <p a set of predictors. The Gibbs sampler was run consid-

ering 20, 000 as the maximum number of iterations with a burn-in of 1, 000. Gibbs
sampler chains were stopped testing normality of normalized averages of functions of
the Markov chain (Chauveau and Diebolt, 1998). Parameters pa, bq and Î± involved

in the prior density of parameters Ïƒj,s s and Vj,s s were set respectively equal to p3, 1q
and 1. The threshold Ï„ used to determine the number of levels was set equal to
5. Let the metric utilized to allocate new predictors (defined as Ï in Â§3.2.2) be the
Euclidean distance.
In all simulation scenarios, predictors were assumed to belong to an r-dimensional
space, either a lower dimensional plane or a non linear manifold, with r

p. For

each synthetic dataset, the proposed model was compared with CART and lasso in
terms of mean squared error. For CART and Lasso standard Matlab packages were
utilized and the regularization parameter of Lasso was chosen based on the AIC.
3.3.1

Illustrative Example

Consider
xi
where Î¨

 N pÏˆpÂµiq, Ïƒ2I q,

 tÏˆ : M Ã‘ <pu, Âµi P M, Ïƒ P p0, 8q, I is the p  p dimensional identity

matrix. Let M be a smooth compact Riemannian manifold. For simplicity, let us
assume that M is a curve. Let Ïˆ pÂµq



1Âµ with 1 being a p-dimensional vector

with all elements equal to 1. Define the conditional f py |xq as a function of Âµ, i.e.

a mixture density with mixture weights depending on Âµ. We will show that our
construction facilitates an estimate of the density of y.
38

Specifically, we created an equally spaced grid of points ti
we let Î·i
xij

 Î·i

P t0, . . . , 20u.

Then,

 sinptiq and predictors be a linear function of Î·i plus Gaussian noise, i.e.
ij with ij  N p0, 0.1q for j P t1, . . . , pu. In particular, we set p  1, 000.

The response was drawn from the following mixture of Gaussians
yi
with wi

 |Î·i|.

 wiN p2, 1q p1  wiqN p2, 1q

(3.3)

Figure 3.2 shows the estimated density of four data points. These

estimates were obtained by performing leave-one-out prediction for different number
of observations in the training set. As the figure clearly shows our construction
facilitates an estimate of the density y approaching the true density as the number
of observations in the training set increases.
3.3.2

Linear Lower Dimensional Space

In this section, the vector of predictors is assumed to lie close to a lower dimensional
plane. In practice, predictors were modeled through a factor model as follows

xi
with Î£0

 Î›Î·i

i

i

 Npp0, Î£0q

Î·i

 Nr p0, I q

 diagpÏƒ1, . . . , Ïƒpq, Î› being a p  r matrix and r

(3.4)
p. In the first

simulation scenario the response y was assumed to be a function of the latent variable
Î· so that the dependence between response and predictors was induced by the shared
dependence on the latent factors. In practice, the vector zi

 pyi, xTi qT

was jointly

sampled from a factor model. The loading matrix was derived as the product of a
matrix with orthogonal columns and a diagonal matrix with positive elements on the
diagonal, i.e. Î›  Î“Î˜. In particular, the columns of Î“ were uniformly sampled from
the Stiefel manifold while the diagonal matrix of Î˜ were sampled from an inverse
Gamma with shape and rate parameters p1, 4q.

39

(Ia)

(Ib)

0.5
0
âˆ’4

0.5

âˆ’2

0

2

4

0
âˆ’4

0.5

âˆ’2

(IIa)

âˆ’2

0

2

4

0

2

4

0
âˆ’4

âˆ’2

0

2

4

0
âˆ’4

2

4

0
âˆ’4

0

2

4

âˆ’2

0

2

4

2

4

2

4

(IIIc)

0.5

âˆ’2

0

2

4

0
âˆ’4

âˆ’2

0
(IVc)

0.5

0

0
âˆ’4

(IVb)

0.5

âˆ’2

âˆ’2

(IIc)

0.5

âˆ’2

0
âˆ’4

0.5

(IVa)

f(y|x)

4

(IIIb)

0.5

0
âˆ’4

2

0.5

(IIIa)

0
âˆ’4

0
(IIb)

0.5

0
âˆ’4

(Ic)

0.5

âˆ’2

0

2

4

0
âˆ’4

âˆ’2

0

y

Figure 3.2: Illustrative example: Plot of true density (red line) and estimated
density (50th percentile: solid line, 2.5th and 97.5th percentiles: dashed lines) for
four data points pI, II, III, IV q considering different training set size (a:100, b:200,
c:300).

In the second simulation scenario, x was sampled from the factor model above,
while y was sampled from a normal with location and scale parameter p1, 1q if the

Â¡ 0, and from a normal with location and scale
p1, 1q otherwise. In both examples, an inverse Gamma prior with parameters p1, 4q
were utilized for Ïƒj with j P t1, . . . , pu.
first variable was positive, i.e. x1

40

3.3.3

Non-Linear Lower Dimensional Space

In this section predictors were assumed to lie close to a lower dimensional non-linear
manifold. In the first simulation study, predictors and response were jointly sampled
from an N components mixture of factor analyzers so that the vector of predictors
and response were assumed to lie close to N lower dimensional planes. For each
mixture components, the loading matrix and variances were sampled as in the first
simulation scenario in Â§3.3.2, while mixture weights were sampled from a Dirichlet
distribution with parameter Î±j

 1 for j P t1, . . . , N u. The number of latent factors

was considered to be increasing in the number of components. In particular, we let
the hth mixture component be modeled through h factors.
In the other two simulation scenarios predictors were assumed to lie close to the
Swissroll and the S-manifold, all two dimensional manifold embedded in <p , while
the response was sampled from a normal with mean equal to one of the coordinates
of the manifold and standard deviation one. Figure 3.3 shows the Swissroll and the
S-manifold embedded in <3 .

Figure 3.3: Swissroll-Manifold and S-Manifold embedded in R3

41

3.3.4

Results

For each simulation scenario in Â§3.3.2 and Â§3.3.3, we sampled M



20 datasets

involving up to 300 observations and for each method we performed leave-one-out
predictions. Table 3.1, 3.2, 3.3 and 3.4 show mean squared errors under the proposed
approach, CART and lasso for each data scenario. As shown, in almost all data
scenarios, our model is able to perform as well as or better than the model associated
to the lowest mean squared error. In particular, when the response is a non linear
function of predictors, CART performs better than Lasso (table 3.2), while when a
linear relationship is assumed lasso outperforms CART ( table 3.1, 3.3, and 3.4). The
tables also report the mean of CPU usage to predict a single point as a function of the
number of features. In particular, CPU time is expressed in seconds and codes have
been running on our workstation (Intel Core i7-2600K Quad-Core Processor memory
8192 MB). Clearly, the proposed model scale substantially better than others to high
dimensional predictors.
Beside running simulations and reporting the distribution of performance for each
W
algorithm, we compare the algorithms per simulation. Define rm
defined as
W
rm

 Ï†pM SB q{Ï†pW q,

where Ï† is the quantity of interest (for example, CPU time in seconds or mean
squared error), MSB is our approach and W is the competitor algorithm. To obtain
mean-squared error estimates from MSB, we select our posterior mean as a pointestimate (the comparison algorithms do not generate posterior predictions, only point
estimates). For each simulation scenario, we sampled multiple datasets and compute
W
the matched distribution of rm
. This provides a much more informative indication of

algorithmic performance, in that we indicate the fraction of simulations one algorithm
outperforms another on some metric. This is akin to power gained by matched twosample tests. For each example, we sampled 20 datasets to obtain estimates of the
42

W
distribution over rm
.

Figures 3.4 and 3.5 depict the relative mean-squared error and CPU time in seconds of our approach, versus CART (red) and Lasso (black) for different simulation
scenarios. The three simulation scenarios are: linear subspaces, union of linear subspaces (MFA) and the swissroll. MSB outperforms both CART and Lasso in all
W
three scenarios regardless of ambient dimension (rmse

1 for all p).

Figure 3.4: Numerical results for various simulation scenarios. Top plot depicts
the relative mean-squared error of MSB (our approach), versus CART (red) and
Lasso (black) as a function of ambient dimension of x. Bottom plot depicts the
ratio of CPU time as a function of ambient dimension of x. The simulation scenario
considered is the linear subspace. MSB outperforms both CART and Lasso regardless
W
of ambient dimension (rmse
1 for all p). MSB compute time is relatively constant
as p increases, whereas Lassoâ€™s compute time increases, thus, as n or p increase,
MSB CPU time becomes less than Lassoâ€™s. MSB was always significantly faster than
CART and PC regression, regardless of n or p. For all panels, n  100 when p varies,
and p  300k when n varies, where k indicates 1000, e.g., 300k 3  105 .

43

Figure 3.5: Numerical results for various simulation scenarios. Top plots depict
the relative mean-squared error of MSB (our approach), versus CART (red) and
Lasso (black) as a function of ambient dimension of x. Bottom plots depict the
ratio of CPU time as a function of sample size. The two simulation scenarios are:
MFA (left) and Swissroll (right). MSB outperforms both CART and Lasso in all two
W
1 for all p). MSB compute time
scenarios regardless of ambient dimension (rmse
is relatively constant as n or p increase, whereas Lassoâ€™s compute time increases,
thus, as n or p increase, MSB CPU time becomes less than Lassoâ€™s. MSB was always
significantly faster than CART and PC regression, regardless of n or p. For all panels,
n  100 when p varies, and p  300k when n varies, where k indicates 1000, e.g.,
300k 3  105 .

3.4 Real Application
We assessed the predictive performance of the proposed method on two very different
neuroimaging datasets. First, we consider a structural connectome dataset collected
at the Mind Research Network. Data were collected as described in Jung et al. (2010).

44

For the analysis, all variables were normalized by subtracting the mean and dividing
by the standard deviation. The prior specification and Gibbs sampler described in
Â§3.3 were utilized.
In the first experiment we investigated the extent to which we could predict
creativity (as measured via the Composite Creativity Index (Arden et al., 2010)).
For each subject, we estimate a 70 vertex undirected weighted brain-graph using
the Magnetic Resonance Connectome Automated Pipeline (Gray et al., 2010) from
diffusion tensor imaging data (Mori and Zhang, 2006). Because our graphs are
undirected and lack self-loops, we have a total of p 



70
2

 2, 415 potential weighted

edges. The p-dimensional feature vector consists of the natural logarithm of the
weight for each edge.
The second dataset comes from a resting-state functional magnetic resonance
experiment as part of the Autism Brain Imaging Data Exchange. We selected the
Yale Child Study Center for analysis. Each brain-image was processed using the
Configurable Pipeline for Analysis of Connectomes (Sikka et al., 2012). For each
subject, we computed a measure of normalized power at each voxel called fALFF (Zou
et al., 2008). To ensure the existence of nonlinear signal relating these predictors, we
let yi correspond to an estimate of overall head motion in the scanner, called mean
framewise displacement (FD) computed as described in Power et al. (2012). In total,
there were p  902, 629 voxels.
Table 3.5 shows mean and variance squared error based on leave-one-out predictions. For each data example, we report the mean and standard deviation (s.d.)
across subjects of squared error, and CPU time (in seconds). For the first data example, we compared our approach (multiscale stick-breaking; MSB) to CART, lasso
and random forests. Table 3.5 shows that MSB outperforms all the competitors in
terms of mean square error; this is in addition to yielding an estimate of the entire
conditional density for each yi . It is also significantly faster that random forests, the
45

next closest competitor, and faster than lasso. For this relatively low-dimensional
example, CART is reasonably fast. For the second data application, given the huge
dimensionality of the predictor space, we were unable to get either CART or random
forest to run to completion, yielding memory faults on our workstation (Intel Core
i7-2600K Quad-Core Processor memory 8192 MB). We thus only compare performance to lasso. As in the previous example, MSB outperforms lasso in terms of
predictive accuracy measured via mean-squared error, and significantly outperforms
lasso in terms of computational time.

46

Table 3.1: Linear manifold example 1: Mean and standard deviations of squared
errors under multiscale stick-breaking (MSB), CART and Lasso for sample size 50
and 100 for different simulation scenarios. Variable time indicates the mean of CPU
usage to predict a single point, p is the dimensionality of the predictor space, n is the
sample size and k indicates 1, 000, i.e. 300k=300,0000. Bold indicates best MSE,
 indicates best CPU time. As shown, MSB outperforms both CART and Lasso
regardless of ambient dimension and sample size.

p

n
mse

10k

50
time

100k

0.25

0.22

0.58

0.22

(0.32)

(0.30)

(0.42)

(0.24)

(0.54)

(0.30)

3

2

3

3

1

0.26

0.20

0.41

0.52

(0.46)

(0.78)

time

5

5

2

(0.23)

5

5

1

mse

0.35

0.45

0.89

0.16

0.33

0.20

(0.53)

(0.77)

(1.04)

(0.21)

(0.46)

(0.31)

3

25

13

27

50

2

2

0.43

0.88

0.52

0.17

0.50

0.31

(0.59)

(1.29)

(0.70)

(0.75)

(0.49)

time

7

50

5

(0.24)

7

51

5

mse

0.11

0.16

0.15

0.83

2.26

0.92

(0.15)

(0.24)

(0.19)

(1.01)

(2.60)

(3.69)

90

11

121

10

100

50

100

5

5

0.003

0.17

0.08

0.13

1.37

1.06

(0.16)

(0.13)

(1.12)

(1.50)

214

43

8

(1.80)

time

10

(0.23)

227

42

mse

1.70

1.48

1.47

0.66

1.65

1.07

(2.18)

(2.47)

(1.63)

(0.87)

(1.49)

(0.95)

121

12

151

13

50
time
mse

500k

0.31

(0.46)

mse

700k

0.18

1

r  10
cart lasso

0.27

time

500k

msb

(0.42)

mse

500k

lasso

0.18

100

time

100k

r5
cart

(0.26)

mse

10k

msb

100
time

6

7

0.69

1.36

0.82

0.78

1.52

1.43

(0.94)

(1.47)

(1.28)

(1.03)

(1.34)

(2.11)

321

41

325

44

13

47

12

Table 3.2: Linear manifold example 2: Mean and standard deviations of squared
errors under multiscale stick-breaking (MSB), CART and Lasso for sample size 50
and 100. Variable time indicates the mean of CPU usage to predict a single point,
p is the dimensionality of the predictor space, n is the sample size and k indicates
1, 000, i.e. 300k=300,0000. In this case, given the non-linear relationship between
response and predictors, CART outperforms Lasso. However, our model results in
the lowest mean squared errors.

msb

r2
cart

lasso

msb

r5
cart

lasso

1.54

1.78

2.37

0.84

1.25

1.62

(1.70)

(1.72)

(0.89)

(1.38)

(1.35)

(1.47)

0.76

0.97

1.77

0.88

1.53

1.43

(1.04)

(1.21)

(3.13)

(1.00)

(1.59)

(2.73)

0.77

1.01

1.61

0.67

0.46

0.97

(0.94)

(1.13)

(1.85)

(0.82)

(0.61)

(1.16)

p

n

10k

100

mse
std

50k

100

mse
std

100k

100

mse
std

200k

100

mse
std

0.86

0.90

1.41

0.74

1.09

0.78

(1.30)

(1.35)

(1.41)

(0.95)

(1.98)

(0.95)

48

Table 3.3: Non-linear manifold - MFA: Mean and standard deviations of squared
errors under multiscale stick-breaking (MSB), CART and Lasso for sample size 50
and 100 for different simulations sampled from a mixture of factor analyzers. Variable
time indicates the mean of CPU usage to predict a single point, p is the dimensionality
of the predictor space, n is the sample size and k indicates 1, 000, i.e. 300k=300,0000.
Bold indicates best MSE,  indicates best CPU time. As shown, MSB outperforms
both CART and Lasso regardless of ambient dimension and sample size.

p

50k

n

msb

mse

0.23

0.42

(0.34)

(0.59)

5

24

100
time

100k

0.17

0.43

0.22

(0.43)

(0.18)

(0.69)

(0.23)

7

27

3

3

0.27

0.17

0.22

0.20

(0.23)

(0.38)

(0.25)

time

10

51

8

(0.19)

12

56

7

mse

0.67

1.35

1.32

0.15

0.17

0.22

(1.04)

(2.26)

(1.36)

(0.23)

(0.19)

(0.23)

9

47

6

44

100

200

6

5

0.64

1.37

0.85

0.15

0.26

0.15

(0.95)

(1.29)

(0.24)

(0.24)

99

15

11

(0.42)

time

14

(1.77)

89

15

mse

0.26

0.39

0.31

0.63

1.40

1.01

(0.39)

(0.51)

(0.52)

(0.80)

(1.24)

(1.46)

125

18

145

17

100

mse

300k

0.36

0.42

time

300k

lasso

(0.56)

mse

300k

N 5
cart

0.23

200

time

100k

msb

(0.33)

mse
50k

N  10
cart lasso

sim

200

9

9

0.25

0.47

0.26

0.63

1.17

0.92

(0.36)

(0.43)

(0.80)

(1.04)

262

40

13

(2.11)

time

15

(0.88)

283

43

mse

0.25

0.30

0.30

0.62

1.42

0.70

(0.36)

(0.41)

(0.48)

(0.89)

(1.85)

(0.94)

463

73

465

89

300
time

15

49

16

Table 3.4: Non-linear manifold - Swissroll and S-Manifold: Mean and standard deviations of squared errors under multiscale stick-breaking (MSB), CART and Lasso
for sample size 50 and 100 for different simulation scenarios. Variable time indicates
the mean of CPU usage to predict a single point, p is the dimensionality of the predictor space, n is the sample size and k indicates 1, 000, i.e. 300k=300,0000. Bold
indicates best MSE,  indicates best CPU time. As shown, MSB outperforms both
CART and Lasso regardless of ambient dimension and sample size.

p

n

msb
mse

100k

50
time
mse

100k

200k

100

0.44

0.25

0.38

0.38

0.84

(0.24)

(0.42)

(0.29)

(0.40)

(0.35)

(0.80)

3

22

5

7

2

1

0.24

0.43

0.17

0.25

0.30

0.70

(0.26)

(0.22)

(0.22)

(0.50)

time

6

48

7

7

50

7

mse

0.24

0.67

0.29

0.35

0.40

0.73

(0.23)

(0.50)

(0.29)

(0.22)

(0.30)

(0.40)

38

5

40

5

50

100

4

3

0.25

0.78

0.33

0.37

0.37

0.70

(0.26)

(0.36)

(0.25)

(0.55)

96

13

6

(0.27)

time

6

(0.74)

98

14

mse

0.17

0.47

0.23

0.16

0.20

0.35

(0.23)

(0.43)

(0.22)

(0.20)

(0.19)

(0.40)

126

10

130

15

50
time
mse

500k

0.24

(0.25)

mse

500k

S-Manifold
msb
cart lasso

(0.55)

time

200k

Swissroll
cart lasso

100
time

5

5

0.17

0.33

0.19

0.11

0.25

0.56

(0.21)

(0.46)

(0.23)

(0.14)

(0.20)

(0.61)

230

25

254

27

11

50

10

Table 3.5: Neuroscience application quantitative performance comparisons. Squared
error predictive accuracy per subject (using leave-one-out) was computed. We report
the mean and standard deviation (s.d.) across subjects of squared error, and CPU
time (in seconds). We compare multiscale stick-breaking (MSB), CART, Lasso and
random forest (RF). MSB outperforms all the competitors in terms of predictive
accuracy and scalability. Only MSB and Lasso even ran for the  106 dimensional
application. Bold indicates best MSE,  indicates best CPU time.

data
creativity

n
108

p
2,415

model
MSB
CART
Lasso
RF

mse (s.d.)
0.56 p0.85q
1.10 p1.00q
0.63 p0.95q
0.57p0.90q

time (s.d.)
1.1 p0.02q
0.9 p0.01q
0.40 p0.10q
78.2 p0.59q

movement

56

 106

MSB
Lasso

0.76 p0.90q
1.02 p0.98q

20.98 p2.31q
96.18 p9.66q

51

4
Bayesian factor trees

In this chapter, we focus on the problem of modeling the density of a vector of obser-

P Y Â„ <p given a set of predictors x P X Â„ <q . One natural approach to
estimating conditional multivariate densities is to let z  py T , xT qT and then model
vations y

the combined vector as iid from an unknown density g, with the conditional density
f py |xq obtained as a byproduct. This joint modeling trick has been widely used for
univariate response (Muller et al., 1996; Shahbaba and Neal, 2009; Hannah et al.,
2011), and is just as applicable in the multivariate case. To characterize the unknown
density g pz q, one can use a mixture of multivariate Gaussian densities. However, as
p

q increases, the curse of dimensionality prevents one from obtaining adequate

performance. This problem can be overcome by incorporating dimensionality reduction within each component by using a factor model, leading to a mixture of factor
analyzers (MFA) (Tipping and Bishop, 1997). Under MFA the pp

q q-dimensional

vector is modeled as
g pz q 

Â»

Np

q

z |Âµ, Î›Î›T



Î£ dP pÂµ, Î›, Î£q

(4.1)

Model 4.1 can efficiently characterize the density of high-dimensional observations,
52

while conducting dimensionality reduction through learning the lower-dimensional
data manifold using a piecewise planar approximation (Chen et al., 2010). Unfortunately, using the joint modeling trick to induce an estimate for f py |xq from an

estimate for g pz q, with z

 pyT , xT qT , tends to have poor performance in practice,

particularly when x is higher dimensional than y. One pays a heavy computational
price for estimating the high-dimensional nuisance parameter corresponding to the
marginal density of x, and additionally learning of a parsimonious low-dimensional
structure tends to be driven largely by the x marginal, leading to relatively poor performance in estimating f py |xq. One can alternatively use (4.1) for the conditional

density f py |xq directly, with x dependence incorporated in the mixing measure or
factor analytic parameters. This can be accomplished using previous nonparametric
Bayes machinery (Ren et al., 2011; Rodriguez and Dunson, 2011; Hatjispyros et al.,
2011). However, as p and q increase, computation rapidly becomes prohibitively
slow. In this chapter, we propose a Bayesian factor tree model that can flexibly and
efficiently learn the density of a p dimensional response given an high dimensional
vector of features.

4.1 Methodology
4.1.1

Model Structure

We aim to reduce dimensionality for tractability in building a flexible and scalable
model for the predictor-dependent density of y

P <p. The density of y will depend on

covariates through the multiscale representation of the data presented in Â§3.1.1 (refer
to this section for further details). Given the multiscale partition, the conditional
density f py |xq is defined as
f py |xq 

k
Â¸



Ï€j,sj pxq fj,sj pxq ,

j 1

53

(4.2)

Â¤ Ï€j,s pxq and Â°kj1 Ï€j,s pxq  1. The dictionary densities are chosen as multivariate normal, fj,s  Np pÂµj,s , Î¨j,s q, with a factor analytic form chosen for the
with 0

j

j

j

j

j

covariance to reduce dimensionality
Î¨j,sj

 Î˜j,s Î˜Tj,s
j

with Î˜j,sj being a p  `j,sj matrix and Î£j,sj

j

Î£j,sj

 diag

(4.3)



p
1
Ïƒj,s
, . . . , Ïƒj,s
. The covariance
j
j

decomposition in 4.3 can be induced through the latent factor model
yi

 Âµj,s

j

Î˜j,sj Î·i

i ,

Î·i

 N` p0, I q,
j,sj

i

 Npp0, Î£j,s q.
j

(4.4)

Therefore, for each node pj, sj q, fj,sj will be induced marginalizing out the latent
variable Î·i in the factor model specific to that node. The number of columns of Î˜j,sj
(number of factors) varies across nodes and is estimated through an adaptive Gibbs
sampler (see Â§4.2.2).
For the probability weights on the dictionary densities corresponding to each path
through the tree, we choose the novel stick-breaking process defined in chapter 3. For
each node pj, sj q in the partition tree, define a stick length Vj,sj

 Betap1, Î±q for nodes

pj, sj q located from generation 1 to k  1. The parameter Î± encodes the complexity
of the model, with Î±  0 corresponding to the case in which f py |xq  f py q. We
relate the weights in (4.2) to the stick-breaking random variables as follows:
Ï€j,sj

 Vj,s

j

Â¹

P p

Î¶ A j,sj

q

r1  VÎ¶ s ,

 1 for any sj P t1, . . . , Kj u. This condition will ensure that Â°kj1 Ï€j,s  1
for any path ts1 , . . . , sk u.
with Vk,sj

j

54

4.2 Estimation
We first partition observations in a tree fashion applying recursively METIS (Karypis
and Kumar, 1999) until the stopping criteria presented in chapter 3 is satisfied (refer to section Â§3.2 for further details). Then, the sequence of dictionary densities

tfj,s u and stick-breaking weights tVj,s u are estimated using Bayesian methods and
j

j

inference is carried out using the Gibbs sampler. Under model 4.4, estimating the
(

sequence of dictionary densities is equivalent to estimate sequences Î›j,sj , Âµj,sj

(

(

and Î£j,sj .
4.2.1

Prior Specification

Following Bhattacharya and Dunson (2011), we will consider a shrinkage priors for
hÎ¹
as the ph, Î¹q element of Î˜j,sj
the columns of the factor loadings in 4.4. Define Î¸j,s
j

and consider the following prior specification

phÎ¹q  N
Î¸
j,sj



pÎ¹q
0, pÎ¹q phÎ¹q , Ï„j,sj
Ï„j,sj Ïj,sj

phÎ¹q  Gapa , a q, Ï†p1q
1 2
j,sj

Ïj,sj

Choosing a4

1

phq

 Gapa3, 1q,

Â¡ 1 implies that Ï„j,spÎ¹q

j

Ï†j,sj



Î¹
Â¹



pdq

Ï†j,sj

d 1

 Gapa4, 1q @h Â¡ 1

stochastically increases with Î¹, shrinking the

elements of Î˜j,sj toward zero increasingly as the number of columns grows. This
prior specification allows an adaptive choice of the number of factors. We assign a
standard normal density for the intercept parameter Âµj,sj associated to each node

pj, sj q, i.e. Âµj,s  Npp0, I q. Finally, we specify the prior for Î£j,s via the usual inverse
phq
gamma priors on its diagonal elements, i.e. Ïƒj,s  IG pÎ±Ïƒ , Î²Ïƒ q for h P t1, . . . , pu.
j

j

j

55

4.2.2

Selection of the Number of Factors

In order to select the number of factors, we directly apply the method proposed by
Bhattacharya and Dunson (2011). Basically, the number of factors will be adapted
as the Gibbs sampler progresses, with adaptation designed to satisfy the diminishing adaptation condition in Theorem 5 of Roberts and Rosenthal (2007). Directly
following Bhattacharya and Dunson (2011), at the hth iteration, adaptation occurs
with probability pphq

 exp ppt1

t2 hqq and pt1 , t2 q chosen such that adaptation

occurs every ten iterations at the beginning of the chain and decreases exponentially
in the number of iterations. When adaption occurs, the number of columns having
all elements in some a priori chosen neighborhood of zero, i.e.

pËœ, Ëœq with Ëœ close

to zero, are counted. We can intuitively assume that the factors corresponding to
such columns have a negligible contribution, therefore we discard these columns and
continue the sampler with a reduced number of factors. Otherwise, if the number of
such columns drops to zero we may be missing important factors, therefore we add
a column to the loading. The other parameters are modified accordingly and, when
a factor is added, the new parameters are sampled from the prior. This adaptation
scheme was thought for a single factor model but it can be easily implemented for a
mixture of factor analyzers and consequently for our model. When adaptation occurs
columns of all loading matrices Î˜j,sj s will be monitored and the number of factors
of each loading will be either decreased or increased.
4.2.3

Full Conditionals and Gibbs Sampler Steps

For ease of explanation, we assume in this section a fixed number of factors. The
number of levels k and, consequently the number of subsets in T , strictly depends on
the number of observations. Therefore, especially for large sample sizes, the dimensionality of the parameter space may become huge and it may lead to computational
problems. To solve this computational issue, we implement the slice sampling al56

gorithms proposed by Kalli et al. (2011). For each observation a latent variable
u P r0, 1s is introduced and the vector py T , uqT is modeled as

f py, u|xq 

k
Â¸





Ï€j,sj pxq fj,sj pxq ,

1 u

(4.5)

j 1

with 1pAq being equal to one if the event A occurs. Notice that marginalizing out
the latent variable u, the mixture density in 4.5 is recovered. Under model 4.5,
components with weights close to zero, and therefore considered unnecessary, will be

P t1, . . . , ku be the indicator variable indicating the
level used by observation yi . Define Ij,m  ti : gi  j, xi P Cj,m u with nj,m being the

automatically excluded. Let gi

cardinality of Ij,m . Considering model 4.5 and the prior specification in Â§4.2.1, the
Gibbs sampler iterates through the following steps.
Step 1. Sample ui for i P t1, . . . , nu
Prpui | q  U n 0, pgi ,sgi



phq the
 t1, . . . , ku and s P t1, . . . , Kj u, denote Î¸j,s
phq
jth row of Î˜j,s and sample Î¸j,s , for h P t1, . . . , pu from:
Step 2. For each node pj, sq, with j



phq | q  N

PrpÎ¸j,s

`j,s

DÌ„1
h

Â¸

P



phq  Âµphq

Î·i y i

j,s

{Ïƒjphq, s, DÌ„h1

i Ij,s

phq



phq being the hth element of y , DÌ„
i
h

with Âµj,s being the hth element of Âµj,s , yi

Dh

phq

Ai {Ïƒj , s with Dh

 diag



ph1q p1q

phkq phq and A  Â°
T
i
iPIj,s Î·i Î·i .

Ïj,s Ï„j,s , . . . , Ïj,s Ï„j,s

Step 3. Update Î·i , for i P t1, . . . , nu, from:
57





1
1
PrpÎ·i | q  N`gi ,sgi DÌ„Î·1 Î˜Tgi ,sgi Î£
gi ,sgi pyi  Âµgi ,sgi q, DÌ„Î·

with DÌ„Î·





I

1
Î˜Tgi ,sgi Î£
gi ,sgi Î˜gi ,sgi .

phq

Step 4. Update Ïƒj,s , for h P t1, . . . , pu and j


 t1, . . . , ku and s P t1, . . . , Kj u, from:



phq | 

Pr Ïƒj,s

 IG aÏƒ

Â¸ 

nj,s {2, bÏƒ

phq  Î¸phq Î·

yih  Âµj,s

P

j,s

2

i

i Ij,s

Step 5. Update Âµj,s , for j

 t1, . . . , ku and s P t1, . . . , Kj u, from


1
Pr pÂµj,s | q  Np  nj,s Î£
j,s

I

1 Â¸

P

pyi  Î˜j,sÎ·iq ,

1
nj,s Î£
j,s

I

1

i Ij,s

Step 6. Update gi by sampling from the multinomial full conditional with
Prpgi
with fj,sj pxi q

N



 j | q91



pj,sj pxi q fj,sj pxi q pyi q

ui

Âµj,sj pxi q , Î˜j,sj pxi q Î˜Tj,sj pxi q

Î˜j,sj pxi q

Step 7. Update stick-breaking random variable Vj,s from


PrpVj,s | q  Beta 1

nj,s , Î±

Â¸

P p q

nv

v D j,s

 1. Let Vk,s  1 for all s and let
Â±
Ï€j,s  Vj,s Ï…PApj,sq p1  VÏ… q. In particular, Apj, sq and Dpj, sq are defined as the set
of ancestors and descendants of node pj, sq as in chapter 3.

for any set j, s located from tree level 1 to k

ph,Î¹q

Step 8. Update Ïj,s , for h Â¤ p, Î¹ Â¤ `j,s , j


ph,Î¹q
PrpÏj,s | q  G a1

Â¤ k and s P t1, . . . , Kj u from

1{2, a2
58

i
Ï„j,s



ph,Î¹q

Î¸j,s

2

{2

Step 9. Update Ï†1j,s , j

Â¤ k and s P t1, . . . , Kj u from


p1q
PrpÏ†j,s | q  G a3

d Â¤ `j,s , j

Step 10. Update Ï†dj,s , for 1


pdq
PrpÏ†j,s | q  G a4

p`j,s {2, 1

p`j,s  d

p

Â¸
p
hq
p
whq
pwhq
Ï„j,s
Ïpjsq Î¸j,s
w 1
h1
`j,s
Â¸

2

{2

Â¤ k and s P t1, . . . , Kj u from

1qp{2, 1

p

Â¸
p
hq
pw,hq pwhq
Ï„j,s
Ïpjsq Î¸j,s
w 1
hd
`j,s
Â¸

2

{2

In order to proceed with the chain, it is not required to sample all pÂµj,s , Î›j,s , Î£j,s qs.
We only need to sample parameters necessary to do Step 6 exactly. Therefore,
only parameters involved in components with non-negligible weights will need to be
sampled. This will reduce dramatically the computational burden of the proposed
algorithm.

4.3 Synthetic Example
In the following simulation examples, we test the predictive performance of the proposed model relative to competing alternatives. We initially consider the case in
which xi is a two dimensional vector defined over a bounded set and then move to
the more general case in which xi

P <q . In all examples, the proposed model will be

compared to mixture of factor analyzers (MFA) and covariate dependent mixture of
factor analyzers (dMFA). The latter provides additional flexibility by allowing the
mixing weights to change flexibly with covariates. In particular, covariate dependent
weights will be modeled through the probit stick breaking process (Rodriguez and
Dunson, 2011). The three models will be compared in terms of mean squared error in leave-one-out cross validation. Predictions will be carried out using the same
methodology described in Â§3.2.2. We used 10, 000 Gibbs sampling iterations and a
burn-in of 1, 000. We set the hyperparameters at the following fixed non-optimized
59

values for all simulations and real data experiments and do not tune: a1

 a2  2,

 1, a4  4, Î±Ïƒ  1, Î²Ïƒ  0.3 and Î±  1. We set parameters used to select the
number of factors as Ëœ  0.01, t1  1 and t2  5  104 .

a3

4.3.1

Two Dimensional Predictors

We initially assume xi is sampled iid from a uniform distribution overr0, 1s2 . In
the first simulation scenario, yi is drawn from a feature-dependent Gaussian density.
Specifically,
yi

 NppÂµ0pxiq, Î¨0pxiqq,

Î¨0 pxi q  Î›0 pxi qÎ›0 pxi qT

with Î›0 being a p  `0 matrix, Î£0 a p  p identity matrix and `0

Î£0
p. We generated

Âµ0 pxi q and Î›0 pxi q as follows

pÂµ0hpx1q, . . . , Âµ0hpxnqq iid
 Nn p0, C pxqq ,


Î»0js px1 q, . . . , Î»0js pxn q

 Nn p0, C pxqq ,

iid

j

h P t1, . . . , pu

P t1, . . . , pu , s P t1, . . . , `0u

with Âµ0h pxi q and Î»0js pxi q being respectively the hth element of Âµ0 pxi q and the pj, sqth

element of Î›0 pxi q. We define the pj, hqth element of C pxq as Cjh pxq  Ï… exptdpxj , xh qu
with dpxj , xh q being the Euclidean distance between vectors xj and xh , Ï…
the number of factors equal to `0

 5 or `0  10.

 2, and

In the second simulation scenario, yi is drawn from a mixture of factor analyzers
with feature-dependent mixture weights. Let k0

P t3, 5u be the number of mixture

components and let Î¦pq be the cumulative distribution function of the standard
normal density. Mixture weights were derived through a probit stick breaking specification (Rodriguez and Dunson, 2011). Model stick breaking weights as
Ï€0j pxi q  V0j pxi q

j 1
Â¹



s 1

60

p1  V0spxiqq

with V0j pxi q
Î›0j Î›10j

 Î¦ pÏ‘j1xi1

Ï‘j2 xi2 q for j

k0 and V0k0

 1.

Let Âµ0j and Î¨0j



Î£0j be respectively the intercept and the covariance of the jth mixture

component, with Î›0j being a p  `0 loading matrix and Î£0j a p  p diagonal matrix
with positive entries on the diagonal. In particular, sample each element of the

P t1, . . . , k0u, independently from a normal with mean m0j
and unitary variance. For k0  3, we set

loading Î›0j and Âµ0j , for j

m0
while for k0

 p2, 0, 2q

,

pÏ‘11, Ï‘21q  p0.5, 0q

,

pÏ‘12, Ï‘22q  p0.5, 0q

 5 we set
m0

 p2, 0, 2, 1, 1q

,

pÏ‘11, Ï‘21, Ï‘31, Ï‘41q  p0.5, 0, 1, 1q

pÏ‘12, Ï‘22, Ï‘32, Ï‘42q  p0.5, 0, 1, 1q
The diagonal elements of Î£0j are drawn from an inverse Gamma density with scale
and rate parameter equal to 5 and 3 respectively. The number of factors was set
equal to 5.
Table 4.1 shows mean squared errors based on leave-one-out predictions. In
essentially every case considered, our Bayesian factor trees (BFT) approach produced
the lowest MSE followed by dMFA, with MFA having the worst performance. In
the second simulation scenario, when observations are sampled from a dMFA, our
approach and dMFA perform similarly.
To visualize the performance of our model, we define a grid of 100 evenly spaced
points in U

 r0, 1s2.

We sampled yi

P <p with p  50 and i P U

from the above

three component mixture of factor analyzers. Figure 4.1 shows the true value and
the estimate of five variables in yi for each i

P U.

At each Markov Chain iteration,

we sampled the five variables conditionally on the other p  5 variables, then in figure
4.1 we plot the mean of those values over Markov chain iterations. As shown, the
proposed model performs similarly to dMFA in estimating elements of yi , while MFA
61

is not able to capture most of the spatial structure. The associated mean squared
errors were pË†2M F A , Ë†2dM F A , Ë†2BF T q

 p0.33, 0.27, 0.27q, revealing that we were able to

perform similarly to dMFA.
Table 4.1: Two dimensional predictors: Mean and standard deviations of squared
errors under our bayesian factor tree (BFT), a mixture of factor analyzers (MFA)
and covariate dependent mixture of factor analyzers (dMFA) under the first (1) and
second (2) simulation scenario. Bold indicates best MSE. As shown, in almost all
data scenarios, BFT leads to the lowest MSE.

sim

p

n

bft

k0  3
mfa

dmfa

bft

k0  5
mfa

dmfa

(1)

100

50

0.11

0.15

0.11

0.14

0.19

0.20

(0.09)

( 0.12)

(0.07)

(0.08)

(0.17)

(0.15)

0.06

0.33

0.13

0.08

0.17

0.13

(0.02)

(0.46)

(0.06)

(0.03)

(0.14)

(0.07)

100

500

50

100

0.08

0.16

0.11

0.08

0.21

0.20

(0.06)

(0.12)

(0.13)

(0.06)

(0.16)

(0.11)

0.05

0.12

0.10

0.14

0.20

0.18

(0.05)

(0.11)

(0.09)

(0.14)

(0.16)

(0.16)

sim

p

n

bft

`0  5
mfa

dmfa

bft

(2)

100

100

0.14

0.25

0.25

0.26

0.27

0.29

(0.09)

(0.06)

(0.09)

(0.14)

(0.18)

(0.28)

0.16

0.20

0.19

0.17

0.19

0.19

(0.11)

(0.16)

(0.16)

(0.08)

(0.10)

(0.13)

0.14

0.16

0.15

0.22

0.23

0.26

(0.06)

(0.05)

(0.05)

(0.12)

(0.14)

(0.17)

200

500

100

200

`0  10
mfa
dmfa

0.11

0.15

0.12

0.25

0.26

0.19

(0.06)

(0.07)

(0.04)

(0.21)

(0.24)

(0.10)

62

x1

x1

x1

x2

x1

x2

x1

x2

x2

x1

x1

x2

x1

x2

x1

x2

x2

x1

x1

x2

x1

x2

x1

x2

x2

x1

x1

x2

x1

x2

x1

x2

x2

x1

BFT
x2

dMFA
x2

MFA
x2

x2

True value

x1

x1

Figure 4.1: True value and estimate under MFA, dMFA and BFT of five variables
of yi given xi  pxi1 , xi2 qT for i  t1, . . . , 100u. Each row correspond to a different
element of the response vector y, while each column correspond to a different method
utilized to predict y. As shown, BFT performs similarly to dMFA in estimating the
five elements of y, while a simple MFA (not depending on covariates) is not able to
capture most of the spatial structure.

4.3.2

Higher Dimensional Predictors

In this section we consider examples involving a large number of predictors. In all
scenarios, the response variable yi was sampled from a mixture of factor analyzers
with feature-dependent weights (see Â§4.3.1). Two different models are considered for
the feature vector. First, we assume xi
as Î¨sj

 Nq p0, Î¨q, with ps, j q-element of Î¨ defined

 j, s%|sj|, %  0.9 and j, s  1.

Given xi , yi is sampled from a mixture of
63

factor analyzers with stick breaking weights depending only on the first covariate xi1 ,
i.e. V0j pxi q

 Î¦pÏ‘j xi1q for j

k0 and V0k0 pxi q

 1.

In the second data scenario, xi

is sampled from the Swissroll manifold, a two dimensional manifold embedded in <q
(see figure 3.3(a)). In this case, stick breaking weights are assumed to depend only
on one coordinate of the Swissroll, i.e. V0j pxi q  Î¦pÏ‘j si1 q for j

k0 and V0k0 pxi q  1

with si1 being a coordinate of the manifold. For both scenarios, we consider k0
and pÏ‘1 , Ï‘2 q

 p2, 2q.

3

The intercepts and loading matrices are drawn considering

the same model as in Â§4.3.1.
We sampled 20 datasets for each data scenario. Table 4.2 shows mean squared
errors based on leave-one-out predictions under experiments involving different combination of pn, p, q q. As shown, in almost all data scenarios, our model is able to
perform as well as or better than the model associated to the lowest mean squared
error. As expected, the dependent MFA leads to better MSE compared to MFA.
Figure 4.2 shows the relative CPU time of our approach versus dMFA. The relaA
tive CPU time was computed as in chapter 3, i.e. rm

 Ï†pM SB q{Ï†pAq, where Ï†

is the CPU time in seconds and A is the competitor algorithm. As shown, our
approach can scale substantially better than dMFA to large number of predictors.
Notice, that in this section we have considered examples involving few thousands of
predictors. However, in many real world applications the number of predictors can
grow up to hundreds of thousands. In this framework, a dependent MFA becomes
computationally prohibitive.

4.4 Real Application
In order to test the predictive performance of the proposed model, we considered
two datasets involving a moderately high number of features. The first real data
experiment comprises 40 genes (response) involved in the isoprenoid pathway of
Arabidopsis thaliana. This set of genes were found to be highly correlated with 795
64

Table 4.2: Higher dimensional predictors: Mean and standard deviations of squared
errors under our bayesian factor tree (BFT), a mixture of factor analyzers (MFA)
and covariate dependent mixture of factor analyzers (dMFA). Bold indicates best
MSE.

p

q

n

100

1,000

50

100

5,000

50

100

10,000

50

100

500

1,000

50

100

5,000

50

100

10,000

50

100

Normal model
bft
mfa
dmfa

bft

Swissroll
mfa
dmfa

0.10

0.31

0.29

0.09

0.28

0.13

(0.09)

(0.29)

(0.27)

(0.06)

(0.32)

( 0.19)

0.18

0.49

0.29

0.15

0.41

0.20

(0.20)

(0.45)

(0.30)

(0.28)

(0.73)

(0.35)

0.14

0.26

0.20

0.21

0.37

0.29

(0.10)

(0.24)

( 0.84)

(0.25)

(0.63)

(0.45)

0.18

0.40

0.33

0.14

0.44

0.20

(0.29)

(0.57)

(0.21)

(0.09)

(0.66)

(0.61)

0.08

0.56

0.44

0.25

0.47

0.36

(0.03)

(0.98)

(0.56)

(0.23)

(0.20)

(0.24)

0.30

0.44

0.35

0.26

0.59

0.42

(0.22)

(0.32)

(0.32)

(0.14)

(0.45)

(0.30)

0.45

0.57

0.47

0.43

0.93

0.76

(0.17)

(0.31)

(0.68)

(0.22)

(0.64)

(0.96)

0.40

0.83

0.79

0.44

0.67

0.50

(0.28)

(0.61)

(0.47)

(0.23)

(0.48)

(0.90)

0.62

0.59

0.58

0.92

0.96

0.94

( 0.63)

(0.37)

(0.33)

(0.34)

(0.31)

(0.34)

0.35

0.59

0.59

0.85

0.88

0.73

(0.15)

(0.23)

(0.66)

(0.28)

(0.28)

(0.33)

0.33

0.58

0.42

0.29

0.83

0.70

(0.32)

(0.34)

(0.35)

(0.16)

(1.08)

(0.68)

0.33

0.36

0.35

0.40

0.34

0.32

(0.27)

(0.25)

(0.26)

(0.30)

(0.23)

(0.21)

65

Figure 4.2: Plots depicts the relative CPU time of BFT (our approach), versus
dMFA as a function of ambient dimension of x, under the normal and the swissroll
simulation scenario with q  500 and n  100. The x-axis is the number of predictors
involved in the experiment, where k equals 1 thousand, so that 2k=2,000. BFT
outperforms dMFA regardless of ambient dimension (rcpu 1 for all p).

genes (predictors) from 56 other metabolic pathways in Arabidopsis thaliana (Wille
et al., 2004). All variables have been log-transformed and standardized to zero mean
and unit variance.
The second dataset is a large population dataset involving 518 subjects from the
capital region of Finland. For each subject a set of 138 metabolites and about 35, 000
genes are measured. Inouye et al. (2010) identified a set of highly correlated genes,
the lipidleukocyte (LL) module, as having a prominent role in over 80 metabolites.
Therefore, gene levels should be informative about the two third of metabolites. All
variables were standardized to zero mean and unit variance.
The predictive accuracy of each method was estimated by leave-one-out cross66

validation. Table 4.3 shows percentiles of squared errors for the two data examples
above. For the first data example, we compare our model to MFA and dMFA. For
the second data example, given the ultra-high dimensionality of the predictors space,
we only compared our approach to MFA. As shown, our approach leads to the best
predictive performance.
Table 4.3: Real dataset: Percentiles (2.5%, 50% and 97.5%) of squared errors under
our Bayesian Factor Tree (BFT), a mixture of factor analyzers (MFA) and covariate
dependent mixture of factor analyzers (dMFA). For the second data example, given
the ultra-high dimensionality of the predictor space, we compared our approach only
to MFA.

n

p

q

(1)

118

40

(2)

518

138

795

mfa
(2.5%, 50%, 97.5%)
(0.50, 0.97, 1.15)

bft
(2.5%, 50%, 97.5%)
(0.40, 0.74, 0.90)

35k

(0.60, 0.87, 1.20)

(0.50, 0.71, 0.95)

67

dmfa
(2.5%, 50%, 97.5%)
(0.45, 0.89, 0.95)

5
Concluding Remarks and Future Direction

To summarize, we have dealt with two problems in this thesis. For the first problem
we have proposed a new repulsive mixture modeling framework, which should lead
to substantially improved unsupervised learning (clustering) and density estimation
performance in general applications. A key aspect is soft penalization of components
located close together to favor, without sharply enforcing, well separated clusters that
should be more likely to correspond to the true missing labels. We have focused on
Bayesian MCMC-based methods, but there are numerous interesting directions for
ongoing research, including fast optimization-based approaches for learning mixture
models with repulsive penalties.
The other problem, we have dealt with, is to learn a the density of a response
variable given high dimensional features. In chapter 3, we have introduced a general
formalism to estimate conditional distributions via multiscale dictionary learning.
We developed a novel multiresolution stick breaking process that can scale substantially better than other existing algorithms to massive number of features, while
resulting in good predictive performance. An important property of any such strategy is the ability to scale up to ultrahigh-dimensional predictors. We considered
68

simulations and real-data examples where the dimensionality of the predictor space
exceeded several thousands. To our knowledge, no other approach to learn conditional distributions can run at this scale. Our approach explicitly assumes that the
posterior f py |xq can be well approximated by projecting x onto a lower-dimensional
space. Note that this assumption is much less restrictive than assuming that x
is close to a low-dimensional space; rather, we only assume that the part of f pxq
that â€œmattersâ€ to predict y lives near a low-dimensional subspace. Because a fully
Bayesian strategy remains computationally intractable at this scale, we developed
an empirical Bayes approach, estimating the partition tree based on the data, but
integrating over scales and posteriors.
We demonstrate that even though we obtain posteriors over the conditional distribution f py |xq, our approach, dubbed multiscale stick-breaking (MSB), outperforms
standard machine learning algorithms in terms of both predictive accuracy and computational time, as the sample size and ambient dimension increase. In future work,
we will extend these numerical results to obtain theory on posterior convergence.
Indeed, while multiscale methods benefit from a rich theoretical foundation (Allard
et al., 2012), the relative advantages and disadvantages of a fully Bayesian approach,
in which one can estimate posteriors over all functionals of f py |xq at all scales, remains relatively unexplored.
In chapter 4, we have extended the multiresolution stick breaking model proposed in chapter 3 to handle multivariate responses. For this purpose, dictionary
densities were defined as multivariate normal with a factor analytic form chosen for
the covariance to reduce dimensionality. The proposed model results in a mixture of
factor analyzers defined over different levels of resolution. As illustrated, inference on
component-specific parameters is carried out using Gibbs sampler. Our model leads
to good predictive performance and can scale to high number of features. However,
the proposed model may face computational problems as the number of response
69

variables increases. In fact, at each Gibbs sampler step and for each observation,
the likelihood function (a mixture of multivariate Gaussians) needs to be computed.
This step can become computational prohibitive as the number of response variables
increase, reducing dramatically the efficiency of our model. There are a variety of
real worlds applications involving a large number of response variables depending on
huge number of features. For this applications, a more efficient algorithm relying
on some likelihood approximation needs to be considered. In future works, we will
extend our algorithm to efficiently handle situations in which not only the predictors
but also the response is high dimensional.
Another possible direction for future work is using parallelized and distributed
systems to estimate the proposed multiresolution stick breaking model. Though this
model can scale substantially better than competitors to high dimensional features,
we may gain more efficiency by using parallelized and distributed systems. For this
purpose, we should adopt other estimation techniques rather than Bayesian method
relying on Markov chain Monte Carlo. In fact, given the serial structure of MCMC
algorithms, they cannot fully be learned using parallelized systems. Alternatively,
we may use an hybrid model where dictionary densities are estimated in parallel
using frequentist methodologies, such as maximum likelihood estimation, and then
stick breaking weights are estimated through Markov chain Monte Carlo.

70

Appendix A
Chapter 2: Theory

A.1 Cited Theorems and Assumptions
Assumptions of theorem 3.1 in Scricciolo (2011)
(i) The prior on Ïƒ has a continuous and positive Lebesgue density Ïˆ on an interval
containing Ïƒ0 and its distribution function Î¨, for constants e1 , e2 , e3

Â¡ 0, satisfies

Î¨psq Â¤ exppe1 se2 q as s Ã‘ 0 and 1  Î¨psq Â¤ se3 as s Ã‘ 8
(ii) The prior for the number of components is such that, for constants d1 , d2
0

Ï‘pk q Â¤ d1 exppd2 k q for all k

Â¡ 0,

PN

(iii) For each k, the prior for the weights is a Dirichlet with parameters pÎ±1 , . . . , Î±k q
such that, for constants a1 , a2

Â¡ 0, a3 Â¥ 1 and for 0
a2 a1

 Â¤ 1{pa3 k q and j

 1, . . . , k

Â¤ Î± j Â¤ a3

Assumptions B1-B5
Assumptions B1-B5 corresponds to assumptions A1-A5 in Rousseau and Mengersen

71

(2011). Assumptions differ only in the conditions concerning the prior on the componentspecific parameters in assumption A5. In condition B5, we assume that Ï€ is defined
as (2) and h is defined as either (3) or (4). For the sake of clarity, let us state
assumption B1:
B1) There exists a q

Â¥ 0 such that for Î´n  plog nqq n1{2 the following holds

lim lim sup En0 tÎ  p}f

M

Ã‘8

Ã‘8

n

 f0}1 Â¥ M Î´n|Ynqu  0

Ghosal et al. (2000)â€™s Theorem
Theorem 8. Let Ï€n be a sequence of priors on a class of densities F equipped
with a metric d that can be either the Hellinger or the one induced by the L1 -norm.

Ã‘ 0 such that n minpÂ¯n, Ëœnq Ã‘ 8, constants

Assume that for positive sequences Â¯n , Ëœn
d1 , d2 , d3 , d4

Â¡ 0 and sets Fn Â„ F, we have

log DpÂ¯n , Fn , dq Â¤ d1 nÂ¯2n
Ï€n pF zFn q Â¤ d3 exp
(

Ï€n BK L pf0 ; Ëœ2n q

pd2

4qnËœ2n

(A.1)
(

Â¥ d4 exppd2nËœ2nq

Â³

Â³

where BK L pf0 ; Ëœ2n q  f : f0 logpf0 {f q Â¤ Ëœ2n ; f0 logpf0 {f q2
Then, for n

(A.2)
(A.3)

Â¤ Ëœ2n

(

.

 maxpÂ¯n, Ëœnq and a sufficiently large constant M Â¡ 0, the posterior

probability
Ï€n tf : dpf, f0 q Â¡ M n |Yn u Ã‘ 0
in P0n probability, as n Ã‘ 8.

A.2 Proofs
Throughout the appendix we write all constants whose values are of no consequence
to be equal to 1.
72

Proof of lemma 1. By assumption B0, Ï‘pk

 k0q Â¡ 0.

We consider the case f is

a finite mixture with k0 components. By assumption A1, for each Î·
is a corresponding Î´
with |Î³1

Â¡

0 such that, for any given y

P

Â¡

0 there

Y and for all Î³1 , Î³2

P

Î“

 Î³2| Î´, we have that |Ï†py; Î³1q  Ï†py; Î³2q| Î·. Let SÎ´  PÎ´  Î“Î´ with
Î“Î´  tÎ³ : |Î³j  Î³0j | Â¤ Î´, j Â¤ k0 u and PÎ´  tp : |pj  p0j | Â¤ Î´, j Â¤ k0 u. By assumption
A1 and A2, for any given y and for any Î· Â¡ 0, there is a Î´ Â¡ 0 such that |f0  f | Â¤ Î·
if Î¸ P SÎ´ . This means that, f Ã‘ f0 as Î¸ Ã‘ Î¸0 , for any given y. Equivalently, we can
say that | logpf0 {f q| Ã‘ 0 pointwise as Î¸ Ã‘ Î¸0 . Notice that
|log pf0{f q| Â¤


"
*

log sup Ï† Î³


P

Î³ D0

pq 

"

*

log inf Ï† Î³ 
Î³ PD0

pq

By assumption A3 and applying the dominated convergence theorem, for any  Â¡ 0
there is a Î´

Â¡ 0 such that

Â³

f0 logpf0 {f q

 if Î¸

P SÎ´ .

By the independence of the

weights and the parameters of the kernel,
Î pKLpf0 , f q

q Â¥ Î»pPÎ´ qÏ€ pÎ“Î´ q

Assumption A4 combined with the fact that tÎ³ : ||Î³

 Î³0||1 Â¤ Î´u Â„ Î“Î´ result in
Ï€ pÎ“Î´ q Â¡ 0. Finally, since Î»  DirichletpÎ±q, it can be shown that Î»pPÎ´ q Â¡ 0.
Proof of lemma 2. Recall that, under assumptions in lemma 1, Î³ is a vector of only
location parameters. For any given x

P Î“k , define Dx  tÎ³ : ||Î³  x||1

Ï… {2u. By

the assumptions on h, for any given x satisfying condition A4 in lemma 2, hpÎ³ q Â¡ 0
for Î³ such that dpÎ³s , xs q

D

Ï… {2 for s  1, . . . , k. Since,

Â„ tÎ³ : dpÎ³s, xsq

Ï… {2; s  1, . . . , k u,

it follows that hpÎ³ q Â¡ 0 on D. By assumption, g0 is positive on Î“, therefore it follows
that Ï€ pÎ³ q Â¡ 0 on D.

73

Proof of lemma 3. To prove lemma 3 we need to show that the three conditions of
theorem 2.1 in Ghosal et al. (2000) are satisfied. First, define Dp, F, ds q as the
maximum number of points in F such that the distance, with respect to metric ds ,
between each pair is at least . Let ds be either the Hellinger metric or the one
induced by the L1-norm. For given sequences kn , an , un
Fnpkq



#

f :f



k
Â¸



pj Ï†pÎ³j , Ïƒ q, Î³

Ã’ 8 and bn Ã“ 0 define
+

P pan, anqk , Ïƒ P pbn, unq

j 1

 Ykj1Fnpjq. As it is shown in Scricciolo (2011), for constants f2 Â¥ f1 Â¡ 0 and
l1 , l2 , l3 Â¡ 0, derived below to satisfy condition (2) and (3) in Ghosal et al. (2000),
1 1{e
1 1{2
, bn  l1 plog Â¯
and defining f1 log n Â¤ kn Â¤ f2 log n, an  l3 plog Â¯
and
n q
n q
l
un  Â¯
Â¯n , Fn , ds q Ã€ nÂ¯2n with Â¯n  n1{2 log n.
n , log D p
Let An,j  pan , an qj . In order to show condition (2) of theorem 2.1. in Ghosal
et al. (2000), we need to show that there is a constant q1 Â¡ 0 such that Ï€ pAC
n,k q Ã€
exppq1 a2n q. From the exchangeability assumption it follows
and Fn

n

2

2

prpAC
n,k |k

 sq  Â°sj1 j!pss!jq! Ï€

AC
n,j

Â¤ s Â°sj1 pjp1sq!p1sq! jq! Ï€

 An,sj
AC
n,j




 An,sj Â¤ sÏ€mpACn,1q

Therefore, condition C1 implies that, for a positive constant q1 we have Ï€ pAC
n,k q Ã€

E pk q exppq1 a2n q with E pk q

8 by condition (ii).

Given a positive constant z2

chosen to satisfy condition (3) in theorem 2.1 of Ghosal et al. (2000), let f1

pz2 4q{d2, l1 Â¤ te1{4pz2

4qu1{e2 , l2

Â¥ 4pz2 4q{e3 and l3 Â¥ t4pz2

Â¥

4q{q1 u1{2 . Under

these values of f1 , l1 , l2 and l3 , following Scricciolo (2011), assumptions (i), (ii) and
assumption C1 imply Î pF zFn q Ã€ exp tpz2

4qnËœ2n u with Ëœn

 n1{2plog nq1{2.

To show condition (3) of theorem 2.1 in Ghosal et al. (2000), we can again follow
the proof of theorem 3.1. in Scricciolo (2011). The only thing we need to show is
74

that, there are constants u1 , u2 , u3
Ï€ p||Î³  Î³0 ||1

Â¡ 0 such that for any n Â¤ u3

Â¤ nq Â¥ u1 exp tu2k0 logp1{nqu

that is guaranteed by condition C2. Therefore, it can be easily showed that, for
sufficiently large n, z2

Â¡ 0 and Ëœn  n1{2plog nq1{2, Î  tBKLpf0, Ëœ2nqu Ã exppz2nËœ2nq.

Proof of lemma 4. First, let us check that condition C1 is satisfied. Clearly, under
the assumptions on h, Ï€ leads to exchangeable atoms. Under the assumptions on Ï€,
the following holds
Ï€m p|Î³1 | Â¥ tq 

Â»

|Î³1 |Â¥t

Ï€m pÎ³1 qdÎ³1

Â¤ c1c2

Â»

|Î³1 |Â¥t

g0 pÎ³1 qdÎ³1

with c1 and c2 defined as in (2). It follows that there exists a constant n1
that Ï€m p|Î³1 | Â¥ tq Ã€ exppn1 t2 q.

Â¡ 0 such

Now let us verify condition C2. Assumptions on h imply that for any 0
there is a corresponding 0
for all Î³ satisfying mintps,j q:s

0

 min r1{2, g pÎ´1qs

 1p1  1{k0q. By assumption 
Î´1 . Let us define M pÎ³, xq and N pÎ³, xq as follows,

with 1 defined as in assumption B0 and Î´1

"

M pÎ³, xq  Î³ :

*

min

tps,j q:s j u

1

 g1pq and constants w1 Â¡ 0 such that hpÎ³ q Â¥ w1k
j u dpÎ³j , Î³s q Â¥ Î´. Let u3 be defined as

Î´

u3

and therefore Î´



dpÎ³j , Î³s q Â¥ x ,

N pÎ³, xq  tÎ³ : |Î³j

Then,

75

u3

 Î³0j | Â¤ x; j  1, . . . , k0u

Ï€ p||Î³  Î³0 ||1

Â¤ q Â¥

Â³

Ã

Â³

Ã

Â³

t||Î³ Î³0 ||1 Â¤uXM pÎ³,Î´q Ï€ pÎ³ qdÎ³
k0

t||Î³ Î³0 ||1 Â¤uXM pÎ³,Î´q 
k0

p { qXM pÎ³,Î´1 q 

N Î³, k0

Â±k0

 g0 pÎ³j qdÎ³

j 1

Â±k0

 g0 pÎ³j qdÎ³

j 1

Now let us show that N pÎ³, {k0 q

Â„ M pÎ³, Î´1q. Consider pairs ps, j q with s  j.
Without loss of generality assume Î³0s Â¡ Î³0j . Now, consider the possible values of
pÎ³j , Î³sq contained in the set N pÎ³, {k0q. The smallest distance between values of Î³s
and Î³j contained in N pÎ³, {k0 q is
pÎ³0s  {k0q  pÎ³0j

{k0 q Â¥ 1  2{k0

Â¥ 1p1  1{k0q  Î´1

Since the previous holds for any pair ps, j q with s

M pÎ³, Î´1 q. Therefore,
Ï€ p||Î³  Î³0 ||1

Â¤ q Ã

Â³

k0

p { q

N Î³, k0

Ã k

0

for a constant g1

Â±k0

 g0 pÎ³j qdÎ³

j 1

exp tg1 k0 logp1{qu

Ã exp tpg1

Â¡ 0.

 j, we have N pÎ³, {k0q Â„

1qk0 logp1{qu

Proof of theorem 6. Only for this proof and for ease of notation the density f will

 tÎ¸ : fÎ¸  f0u. In
order to define each vector in T , let 0  t0 t1 t2 . . . tk Â¤ k and Î³j  Î³0i for
Â°
j P Ii  tti1 1, ti u. Let p0i  tj t  1 pj and pj  0 for j Â¡ tk . Define qj  pj {p0i

be referred as fÎ¸ . Define the non identifiability set as T

0

i

for j

P Ii. Define An 

M Î´n u. Let Dn



Â³

!

0

i 1

minÏƒPSk

t}f f0 }1

Î´n

Â°
kk0



i 1

pÏƒ p iq

Â¡ Î´nMn

)

and A1n

 An Xt}f  f0}1 Â¤

u exppln pÎ¸q ln pÎ¸0 qqdpÏ€  Î»qpÎ¸q with ln pÎ¸0 q being the log-

likelihood evaluated at Î¸0 . Along the line of Rousseau and Mengersen (2011)â€™s proof,
to prove theorem 1 we need to show that for any 
76

Â¡ 0 there are positive constants

m1 , m2 and a permutation Ïƒ

P Sk such that
Dn

Â¥ m1nspk ,Î±q{2

(A.4)

0

Î pA1n q Â¤ m2 Î´nspk0 ,Î±q MnÎ±Ì„m{2r2
with spk0 , Î±q



k0

1

(A.5)

Â°kk0

 Î±Ïƒpj q . Following Rousseau and Mengersen

mk0

j 1

(2011)â€™s proof, we can show that, under condition B5, (A.4) is satisfied for sufficiently
large n. Concerning (A.5), Rousseau and Mengersen (2011) showed that on A1n , there
is a set Ii containing indices j1 and j2 such that

|Î³j  Î³0i| Â¤ pÎ´n{qj q1{2 , |Î³j  Î³0i| Â¤ pÎ´n{qj q1{2
1

with qj1

1

2

2

Â¡ {k0 and qj Â¡ Î´nMn{2. The triangle inequality implies
2

|Î³j  Î³j | Â¤ 2 tÎ´n{ minpqj , qj qu1{2
1

2

1

2

1{2 .

Now, for sufficiently large n, minpqj1 , qj2 q Â¡ Î´n Mn {2 and therefore |Î³j1  Î³j2 | Ã€ Mn

Since g is bounded above by a positive constant, it exists a constant c Â¡ 0 such that
hpÎ³ q Â¤ cg tdpÎ³j1 , Î³j2 qu Â¤ cg Mn1{2

P A1.
Î»qpÎ³  pq.

for Î³



(A.6)

Let the prior probability of the set A1n be defined as Î pA1n q



Â³

A1n

dpÏ€ 

To find an upper bound for this integral, directly apply the proof of

Rousseau and Mengersen (2011) showing that Î pA1n q


1{2

By assumption, for sufficiently large n, g Mn
m{2  Î±Ì„, it follows

77

n



1{2 Î´ spk0 ,Î±q M Î±Ì„m{2 .
n
n

Mn

Â¤ r1Mnr .

sr2 spk0 ,Î±q
Î´

Î pA1n q Â¤ Mn

Â¤g

2

Letting sr2

 r2

Appendix B
Chapter 2: Additional Results

B.1 Synthetic examples
Densities in figure 2.2 were defined as follows. Density pIaq is a standard normal den-

sity, density pIbq is a two components mixture of Gaussians with weights p0.7, 0.3q,

location parameters p0, 0q and scale parameters p0.2, 2q. Density pIcq is a Students

t density with eight degrees of freedom. Density pIIaq is a two-components mixture
of Gaussians with mixture weights p0.3, 0.7q, location parameters

p0.8, 0.8q

and

variances p0.2, 0.2q. Density pIIbq is a mixture having the same weights and scale
parameters as density pIIaq but location parameters

p1.5, 1.5q, resulting in better

separated clusters. Density pIIIaq is a mixture of a Gaussian with mean 0.7, vari-

0.7, variance 0.2, weight
0.3, skewness parameter 0.5 and kurtosis parameter 3. Density pIIIbq is a mixture

ance 0.2 and weight 0.7 and a Pearson density with mean

having the same weights, scale parameters, skewness and kurtosis parameters as density pIIIaq but having location parameter

p1.2, 1.2q, resulting in better separated

clusters. Density pIV q is a bivariate mixture of two Gaussians with weight 0.5, lo-

cation parameters p0, 0q and p2, 1q, variances p0.2, 0.2q and p0.1, 0.1q and correlation
78

coefficients 0.7 and 0.
Hyperparameters paÏƒ , bÏƒ q for the density of the scale parameter were set to p3, 1q.
Parameters Î±j s were all set equal to the same value Î±Ìƒ and in accordance with
Rousseau and Mengersen (2011)â€™s specification for the density of the weights. For
the non-repulsive model, the kernel locations were given independent standard normal priors. For the repulsive model, we considered a repulsion function defined as
(4), with g defined as (5) and we chose g0 to be the standard normal. The distance
involved in the repulsion function was chosen to be the symmetric K-L divergence
for repulsive priors satisfying definition 1(i) and the Euclidean distance for repulsive
priors satisfying definition 1(ii). We chose parameters Ï„ as described in Â§2.2.2. In
particular, the separation level c used to calibrate Ï„ was fixed at six.
Section 2.3 presents results such as misclassification errors and K-L divergences.
These quantities were derived as follows. The misclassification error was calculated
based on the posterior similarity matrix. Letting n be the number of observations,
the similarity matrix is defined as a n-dimensional square matrix with pi, j q element
equal to one if the ith and the jth observation belong to the same group and zero
otherwise. Let S be the true similarity matrix and SÌ‚h be the similarity matrix
obtained at the hth Markov chain Monte Carlo iteration. Let S pi, j q be the pi, j q
element of the matrix S and define the misclassification error mh as

mh



n
n
1 Â¸ Â¸ 
1 SÌ‚h pi, j q  S pi, j q
np i1 j i 1

with np being the number of distinct pairs in which n observations may be combined
and 1pq the indicator function. The approximation of the K-L divergence at the hth
iteration was calculated through

klh



s
Â¸



log tf0 py0j q{f py0j ; Î¸h qu

j 1

79

with f0 being the true density, f the fitted density, Î¸h the posterior sample at the
hth iteration of parameters involved in f and y0

 ty01, . . . , y0su being s draws from

the true density f0 . In all the experiments s was chosen to be 10, 000.

B.2 Additional results
As mentioned in Â§2.3, knowing that the smoothing parameter Î±Ìƒ directly affects the
behavior of the mixture weights, it might be argued that under an accurate choice
of Î±Ìƒ, the non-repulsive prior may perform as well as the repulsive prior in emptying
the extra components. Hence, we ran the non-repulsive model for different values of
Î±Ìƒ. This comparison was done by utilizing 1, 000 draws from density IIb. The upper
bound on the number of components was chosen to be six and the repulsive prior
was chosen to satisfy definition 1(ii). The slice sampler was run for 10, 000 iterations
with a burn-in of 5, 000. The chain was thinned by keeping every 10th draw. Table
B.1 provides posterior summary statistics for parameters involved in the repulsive
model and non-repulsive model for different choices of Î±Ìƒ. Clearly, as Î±Ìƒ decreases,
the non-repulsive model empties the extra components. However, we also see that
the 95% credible interval of the location parameters now does not include the true
value. This might be explained by the fact that as lower values of Î±Ìƒ are considered,
the posterior can concentrate on too few components leading to degenerate results
in terms of estimates of specific component parameters.
Table B.2 shows extra components weights and K-L divergence for datasets drawn
from density pIIa, IIbq under repulsive and non-repulsive atoms with six and ten
components. As the number of components increases, the probability weight on the
extra components remains close to zero under repulsive mixture priors, while the
probability weight can grow substantially under non-repulsive priors. Hence, the
degraded performance in clustering reported for non-repulsive mixtures relative to
repulsive mixtures for the k

 6 case becomes more pronounced in the k  10 case.
80

Table B.1: Percentiles 2.5th and 97.5th of sum of extra weights (sew q and location
parameters involved in the two components with highest weights (Âµ1 , Âµ2 ) under repulsive and non-repulsive atoms for different values of Î±Ìƒ considering 1, 000 draws
from density IIb

true
Âµ1
Âµ2
sew

1.50
1.50
0.00

N-R

1{3
2.5%
97.5%

Î±Ìƒ

1.51
1.51
0.00

1{10
2.5%
97.5%

1.54
1.44
0.14

1.52
1.50
0.00

1.53
1.47
0.02

R

1{100
2.5%
97.5%

1.52
1.49
0.00

1{3
2.5%
97.5%

1.52
1.47
0.01

1.49
1.54
0.00

1.52
1.43
0.01

Concerning the estimation performance, the K-L divergences resulting from repulsive
and non-repulsive mixtures are very similar for high sample sizes.

Table B.2: Mean and standard deviations of the total probability weight placed on
extra components (more than used in generating data) and K-L divergence under
non-repulsive and repulsive mixtures in different synthetic data cases.
k=6
Data
Model
Extra weights
n  100
n  1000
K-L
n  100
n  1000

IIa

k=10
IIb

IIa

IIb

N-R

R

N-R

R

N-R

R

N-R

R

0.21

0.08

0.09

0.02

0.34

0.23

0.15

0.06

0.21

0.09

0.03

0.00

0.32

0.15

0.05

0.01

0.03

0.05

0.07

0.08

0.03

0.06

0.08

0.10

0.01

0.01

0.01

0.01

0.01

0.01

0.01

0.01

p0.11q p0.07q p0.07q p0.02q p0.11q p0.09q p0.09q p0.04q
p0.11q p0.06q p0.04q p0.01q p0.11q p0.08q p0.04q p0.01q
p0.01q p0.02q p0.02q p0.03q p0.02q p0.03q p0.02q p0.04q
p0.00q p0.00q p0.03q p0.03q p0.00q p0.00q p0.00q p0.00q

The value of Ï„ in the repulsive prior 2.4 relies upon the choice of the separation
level c. In order to assess the sensitivity of results to this choice, the K-L divergence
was computed for different separation levels. For this comparison, observations were
drawn from densities pIIaq and pIIbq. Mixtures of six and ten components were
81

fitted using a Gibbs sampler. The slice sampler was run for 10, 000 iterations with
a burn-in of 5, 000. The chain was thinned by keeping every 10th draw. Figure B.1
shows the median of the K-L divergence between the true and the estimated density.
Clearly, as the separation level increases, the K-L divergence remains stable.

KL Divergence

(I)
0.09

0.1

0.08

0.09

0.07

0.08

(II)

0.07

0.06

0.06
0.05
0.05
0.04

0.04

0.03

0.03

0.02

0.02

0.01

0.01

0
1

2

3

4
c

5

6

0
1

7

2

3

4
c

5

6

7

Figure B.1: Plot of K-L divergence under six and ten components p6 : I, 10 : II q
for different choice of separation level c under density pIIaq for different sample sizes
(100:solid ; 1000:dash) and density pIIbq for different sample sizes (100:dash-dot;
1000:dot)

82

Bibliography
Aguilar, O. and West, M. (2000), â€œBayesian dynamic factor models and portfolio
allocation,â€ Journal of Business and Economic Statistics, 18, 338â€“357.
Allard, W., Chen, G., and Maggioni, M. (2012), â€œMultiscale geometric methods for
data sets II: geometric wavelets,â€ Applied and Computational Harmonic Analysis,
32, 435â€“462.
Arden, R., Chavez, R. S., Grazioplene, R., and Jung, R. E. (2010), â€œNeuroimaging
creativity: a psychometric view.â€ Behavioural brain research, 214, 143â€“156.
Bernardo, J. M., Bayarri, M. J., Berger, J. O., Dawid, A. P., Heckerman, D., Smith,
A. F. M., and West, M. (2003), â€œBayesian factor regression models in the large p
small n paradigm,â€ Bayesian Statistics, 7, 733â€“742.
Bhattacharya, A. and Dunson, D. B. (2011), â€œSparse Bayesian infinite factor models,â€ Biometrika, 98, 291â€“306.
Bishop, C. M. and Svensen, M. (2003), â€œBayesian hierarchical mixtures of experts,â€
Nineteenth Conference on Uncertainty in Artificial Intelligence , pp. 57â€“64.
Breiman, L. (1996), â€œBagging predictors,â€ Machine Learning, 24, 123140.
Breiman, L. (2001), â€œRandom forests,â€ Machine Learning, 45, 5â€“32.
Breiman, L., Friedman, J., Stone, C. J., and Olshen, R. A. (1984), Classification and
regression trees, Chapman & Hall/CRC.
Chauveau, D. and Diebolt, J. (1998), â€œAn automated stopping rule for MCMC convergence assessment,â€ Computational Statistics, 14, 419â€“442.
Chen, M., Silva, J., and Paisley, J. (2010), â€œCompressive Sensing on Manifolds Using a Nonparametric Mixture of Factor Analyzers: Algorithm and Performance
Bounds,â€ IEEE TRANSACTIONS ON SIGNAL PROCESSING , 58, 6140â€“6155.
Chipman, H. A., George, E. I., and McCulloch, R. E. (1993), â€œBayesian CART model
search,â€ Journal of the American Statistical Association, 443, 935â€“948.

83

Chiu, T., Leonard, T., and Tsui, K. (1996), â€œThe matrix-logarithmic covariance
model,â€ Journal of the American Statistical Association, 91, 198â€“210.
Chung, Y. and Dunson, D. B. (2009), â€œNonparametric Bayes conditional distribution
modeling with variable selection,â€ Journal of the American Statistical Association,
104, 1646â€“1660.
Coifman, R. and Lafon, S. (2006), â€œDiffusion Maps,â€ Applied and Computational
Harmonic Analysis, 21, 5â€“30.
Cron, A. J. and West, M. (2011), â€œEfficient Classification-Based Relabeling in Mixture Models,â€ The American Statistician, 65, 16â€“20.
Daley, D. J. and Vere-Jones, D. (2008), An Introduction to the Theory of Point
Processes, Springer.
Dasgupta, S. (1999), â€œLearning Mixtures of Gaussians,â€ Proceedings of the 40th
Annual Symposium on Foundations of Computer Science, pp. 633â€“644.
Dasgupta, S. and Schulman, L. (2007), â€œA Probabilistic Analysis of EM for Mixtures
of Separated, Spherical Gaussians,â€ The Journal of Machine Learning Research,
8, 203â€“226.
Davis, D. T. and Hwang, J. N. (1998), â€œExpanding Gaussian Kernels for Multivariate
Conditional Density Estimation,â€ IEEE Transactions on Signal Processing, 46,
269â€“275.
Death, G. (2002), â€œMultivariate Regression Trees: A New Technique for Modeling
Species-Environment Relationships,â€ Ecology, 83, 1105â€“1117.
Dempster, A., Laird, N., and Rubin, D. (1977), â€œMaximum likelihood from incomplete data via the EM algorithm,â€ Journal of the Royal Statistical Society, Series
B, 39, 1â€“38.
Dunson, D. B., Pillai, N., and Park, J. H. (2007), â€œBayesian density regression,â€
Journal of the Royal Statistical Society Series B-Statistical Methodology, 69, 163â€“
183.
Escobar, M. D. and West, M. (1995), â€œBayesian Density Estimation and Inference
Using Mixtures,â€ Journal of the American Statistical Association, 90, 577â€“588.
Fan, J. Q. and Yim, T. H. (2004), â€œA crossvalidation method for estimating conditional densities,â€ Biometrika, 91, 819â€“834.
Fan, J. Q., Yao, Q. W., and Tong, H. (1996), â€œEstimation of conditional densities and
sensitivity measures in nonlinear dynamical systems,â€ Biometrika, 83, 189â€“206.

84

Figueiredo, M. A. T. and Jain, A. K. (2002), â€œUnsupervised Learning of Finite
Mixture Models,â€ IEEE transactions on pattern analysis and machine intelligence,
24, 381â€“396.
Fraley, C. and Raftery, A. E. (2002), â€œModel-Based Clustering, Discriminant Analysis, and Density Estimation,â€ Journal of the American Statistical Association, 97,
611â€“631.
Fu, G., Shih, F. Y., and Wang, H. (2011), â€œA kernel-based parametric method for
conditional density estimation,â€ Pattern recognition, 44, 284â€“294.
Fyshe, A., Fox, E., and Dunson, D. (2012), â€œHierarchical Latent Dictionaries for
Models of Brain Activation,â€ Proceedings of the International Conference on Artificial Intelligence and Statistics.
Gelfand, A., Schmidt, A., Banerjee, S., and Sirmans, C. (2004), â€œNonstationary
multivariate process modeling through spatially varying coregionalization,â€ Test,
13, 263â€“312.
Geweke, J. F. and Zhou, G. (1996), â€œMeasuring the pricing error of the arbitrage
pricing theory,â€ Review of Financial Studies, 9, 557â€“587.
Ghahramani, Z. and Beal, M. J. (2000), â€œVariational inference for Bayesian mixtures
of factor analyzers,â€ Neural Information Processing Systems 12, 52, 449â€“455.
Ghahramani, Z. and Hinton, G. E. (1997), â€œThe EM algorithm for factor analyzers,â€
Technical Report Number CRG-TR-96-1, The University of Toronto, Toronto.
Ghosal, S., Ghosh, J. K., and Van Der Vaart, A. W. (2000), â€œConvergence Rates of
Posterior Distributions,â€ The Annals of Statistics, 28, 500â€“531.
Gray, W. R., Bogovic, J. A., Vogelstein, J. T., Landman, B. A., Prince, J. L., and
Vogelstein, R. J. (2010), â€œMagnetic resonance connectome automated pipeline: an
overview.â€ IEEE pulse, 3, 42â€“8.
Griffin, J. E. and Steel, M. F. J. (2006), â€œOrder-based dependent Dirichlet processes,â€
Journal of the American Statistical Association, 101, 179â€“194.
Hannah, L. A., Blei, D. M., and Powell, W. B. (2011), â€œDirichlet Process Mixtures
of Generalized Linear Models,â€ Journal of Machine Learning Research, 1, 1â€“33.
Hastie, D. and Green, P. J. (2012), â€œModel choice using reversible jump Markov
chain Monte Carlo,â€ Statistica Neerlandica, 66, 309â€“338.
Hatjispyros, S. J., Nicoleris, T., and Walker, S. G. (2011), â€œDependent mixtures of
Dirichlet processes,â€ Computational Statistics & Data Analysis, 55, 2011â€“2025.

85

Hoff, P. D. and Niu, X. (2012), â€œA Covariance Regression Model,â€ Statistica Sinica,
22, 729â€“753.
Holmes, M. P., Gray, G. A., and Isbell, C. L. (2010), â€œFast kernel conditional density
estimation: a dual-tree Monte Carlo approach,â€ Computational statistics & data
analysis, 54, 1707â€“1718.
Hothorn, T., Hornik, K., and Zeileis, A. (2006), â€œUnbiased Recursive Partitioning:
A Conditional Inference Framework,â€ Journal of Computational and Graphical
Statistics, 15, 651â€“674.
Huber, M. L. and Wolpert, R. L. (2009), â€œLikelihood-Based Inference for Matern
Type-III Repulsive Point Processes,â€ Advances in Applied Probability, 41, 958â€“
977.
Inouye, M., Kettunen, J., Soininen, P., Silander, K., Ripatti, S., Kumpula, L. S.,
Hamalainen, E., Jousilahti, P., Kangas, A. J., Mannisto, S., Savolainen, M. J.,
Jula, A., Leiviska, J., Palotie, A., Salomaa, V., Perola, M., Ala-Korpela, M., and
Peltonen, L. (2010), â€œMetabonomic, transcriptomic, and genomic variation of a
population cohort,â€ Molecular Systems Biology, 6, 1744â€“4292.
Ishwaran, H. and James, L. F. (2001), â€œGibbs Sampling Methods for Stick-Breaking
Priors,â€ Journal of the American Statistical Association, 96, 161â€“173.
Ishwaran, H. and Zarepour, M. (2002), â€œDirichlet Prior Sieves in Finite Normal
Mixtures,â€ Statistica Sinica, 12, 941â€“963.
Ishwaran, H., James, L. F., and Sun, J. (2001), â€œBayesian Model Selection in Finite
Mixtures by Marginal Density Decompositions,â€ Journal of American Statistical
Association, 96, 1316â€“1332.
Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. (1991), â€œAdaptive
mixture of local experts,â€ Neural Computation, 3, 79â€“87.
Jasra, A., Holmes, C. C., and Stephens, D. A. (2005), â€œMCMC and the Label Switching Problem in Bayesian Mixture Models,â€ Statistical Science, 20, 50â€“67.
Jiang, W. X. and Tanner, M. A. (1999), â€œHierarchical mixtures-of-experts for exponential family regression models: approximation and maximum likelihood estimation,â€ Annals of Statistics, 27, 987â€“1011.
Jordan, M. I. and Jacobs, R. A. (1994), â€œHierarchical mixtures of experts and the
EM algorithm,â€ Neural Computation, 6, 181â€“214.
Jung, R. E., Grazioplene, R., Caprihan, A., Chavez, R. S., and Haier, R. J. (2010),
â€œWhite matter integrity, creativity, and psychopathology: Disentangling constructs with diffusion tensor imaging,â€ PloS one, 5, e9818.
86

Kalli, M., Griffin, J. E., and Walker, S. G. (2011), â€œSlice Sampling Mixture Models,â€
Statistics and Computing, 21, 93â€“105.
Karypis, G. and Kumar, V. (1999), â€œA fast and high quality multilevel scheme
for partitioning irregular graphs,â€ SIAM Journal on Scientific Computing 20, 1,
359392.
Krauthausen, P. and Hanebeck, U. D. (2010), â€œRegularized non-parametric multivariate density and conditional density estimation,â€ IEEE Conference on Multisensor Fusion and Integration, pp. 180â€“186.
Larsen, D. R. and Speckman, P. L. (2004), â€œMultivariate Regression Trees for Analysis of Abundance Data,â€ Biometrics, 60, 543â€“549.
Lavine, M. and West, M. (1992), â€œA Bayesian Method for Classification and Discrimination,â€ Canadian Journal of Statistics, 20, 451â€“461.
Lawson, A. and Clark, A. (2002), Spatial Cluster Modeling, Chapman & Hall CRC,
London, UK.
Locarek-Junge, H. and Weihs, C. (2009), Classification as a Tool for Research,
Springer.
Lopes, H. F. and West, M. (2004), â€œBayesian model assessment in factor analysis,â€
Statistica Sinica, 14, 41â€“67.
Lopes, H. F., Gamerman, D., and Salazar, E. (2011), â€œGeneralized spatial dynamic
factor models,â€ Computational Statistics & Data Analysis, 55, 1319â€“1330.
Lutz, R. W. and Buhlmann, P. (2006), â€œBoosting for high multivariate responses in
high dimensional linear regression,â€ Statistica Sinica, 16, 471â€“494.
Mallick, B. K. (1998), â€œBayesian CART model search,â€ Biometrika, 85, 363â€“377.
McLachlan, G. J. and Peel, D. (2000), Finite Mixture Models, vol. 299, Wiley Series
in Probability and Statistics.
Meeds, E. and Osindero, S. (2006), â€œBayesian hierarchical mixtures of experts,â€
Advances in Neural Information Processing Systems.
Minka, T. (2001), â€œAutomatic choice of dimensionality for PCA,â€ Advances in neural
information processing systems, pp. 598â€“604.
Mori, S. and Zhang, J. (2006), â€œPrinciples of diffusion tensor imaging and its applications to basic neuroscience research.â€ Neuron, 51, 527â€“39.
Mossavat, I. and Amft, O. (2011), â€œSparse bayesian hierarchical mixture of experts,â€
IEEE Statistical Signal Processing Workshop (SSP).
87

Muller, P., Erkanly, A., and West, M. (1996), â€œBayesian curve tting using multivariate normal mixtures,â€ Biometrika, 83, 67â€“79.
Muthen, B. and Shedden, K. (1999), â€œFinite Mixture Modeling with Mixture Outcomes Using the EM Algorithm,â€ Biometrics, 55, 463â€“469.
Neal, R. M. (2003), â€œSlice Sampling,â€ The Annals of Statistics, 31, 705â€“767.
Norets, A. and Pelenis, J. (2012), â€œBayesian modeling of joint and conditional distributions,â€ Journal of Econometrics, 168, 332â€“346.
Nott, D. J., Tan, S. L., Villani, M., and Kohn, R. (2012), â€œRegression density estimation with variational methods and stochastic approximation,â€ Journal of Computational and Graphical Statistics, 21, 797â€“820.
Onatski, A. (2005), â€œDetermining the number of factors from empirical distribution
of eigenvalues,â€ The Review of Economics and Statistics, 9, 557â€“587.
Pourahmadi, M. (1999), â€œJoint mean-covariance models with applications to longitudinal data: unconstrained parameterisation,â€ Biometrika, 86, 677â€“690.
Power, J. D., Barnes, K. A., Stone, C. J., and Olshen, R. A. (2012), â€œSpurious but
systematic correlations in functional connectivity MRI networks arise from subject
motion,â€ Neuroimage, 59, 2142â€“2154.
Raftery, A. E. and Fraley, C. (1998), â€œHow Many Clusters? Which Clustering
Method? Answers via Model-Based Cluster Analysis,â€ The Computer Journal,
41, 578â€“588.
Rahman, I. U., Drori, I., Stodden, V. C., and Donoho, D. L. (2005), â€œMultiscale
representations for manifold- valued data,â€ SIAM J. Multiscale Model, 4, 1201â€“
1232.
Rasmussen, C. E. and Ghahramani, Z. (2002), â€œInfinite mixtures of Gaussian process
experts,â€ Advances in neural information processing systems 14.
Ren, L., Du, L., Carin, L., and Dunson, D. B. (2011), â€œLogistic stick-breaking process,â€ Journal of Machine Learning Research, 12, 203â€“239.
Richardson, S. and Green, P. (1997), â€œOn Bayesian Analysis of Mixtures with an
Unknown Number of Components,â€ Journal of the Royal Statistical Society B, 59,
731â€“758.
Roberts, G. O. and Rosenthal, J. S. (2007), â€œCoupling and ergodicity of adaptive
Markov chain Monte Carlo algorithms,â€ Journal of Applied Probability, 44, 458â€“
475.

88

Rodriguez, A. and Dunson, D. B. (2011), â€œNonparametric Bayesian models through
probit stick-breaking processes,â€ Bayesian Analysis, pp. 145â€“178.
Rousseau, J. and Mengersen, K. (2011), â€œAsymptotic Behaviour of the Posterior
Distribution in Over-Fitted Models,â€ Journal of the Royal Statistical Society B,
73, 689â€“710.
Scricciolo, C. (2011), â€œPosterior Rates of Convergence for Dirichlet Mixtures of Exponential Power Densities,â€ Electronic Journal of Statistics, 5, 270â€“308.
Seber, G. A. F. (2004), Multivariate observations, Wiley.
Sethuraman, J. (1994a), â€œA Constructive Denition of Dirichlet Priors,â€ Statistica
Sinica, 4, 639â€“650.
Sethuraman, J. (1994b), â€œA constructive denition of Dirichlet priors,â€ Statistica
Sinica, 4, 639â€“650.
Shahbaba, B. and Neal, R. (2009), â€œNon linear models using Dirichlet process mixtures,â€ Journal of Machine Learning Research, 10, 1829â€“1850.
Shapire, R., Freund, Y., Bartlett, P., and Lee, W. (1998), â€œBoosting the margin: a
new explanation for the effectiveness of voting methods,â€ Annals of Statistics, 26,
16511686.
Sikka, S., Vogelstein, J. T., and Milham, M. P. (2012), â€œTowards Automated Analysis
of Connectomes: The Configurable Pipeline for the Analysis of Connectomes (CPAC),â€ in Organization of Human Brain Mapping, Neuroinformatics.
Stephens, M. (2000a), â€œBayesian Analysis of Mixture Models with an Unknown Number of Components - An Alternative to Reversible Jump Methods,â€ The Annals
of Statistics, 28, 40â€“74.
Stephens, M. (2000b), â€œDealing with label switching in mixture models,â€ Journal of
the Roya; statistical society B, 62, 795â€“810.
Sugar, C. and James, G. (2003), â€œFinding the number of clusters in a data set: an
information theoretic approach,â€ Journal of the American Statistical Association,
98, 750â€“763.
Tipping, M. E. and Bishop, C. M. (1997), â€œMixtures of Principal Component Analysers,â€ In Proceedings IEE Fifth International Conference on Articial Neural Networks, pp. 13â€“18.
Tipping, M. E. and Bishop, C. M. (2012), â€œProbabilistic principal component analysis,â€ Journal of the Royal Statistical Society, Series B, 61, 611â€“622.

89

Tokdar, S. T., Zhu, Y. M., and Ghosh, J. K. (2010), â€œBayesian density regression
with logistic Gaussian process and subspace projection,â€ Bayesian Analysis, 5,
319â€“344.
Tran, M. N., Nott, D. J., and Kohn, R. (2012), â€œSimultaneous variable selection
and component selection for regression density estimation with mixtures of heteroscedastic experts,â€ Electronic Journal of Statistics, 6, 1170â€“1199.
Utsugi, A. and Kumagai, T. (2011), â€œBayesian analysis of mixtures of factor analyzers,â€ Neural Computation, 13, 993â€“1002.
Wang, J. (2010), â€œConsistent selection of the number of clusters via crossvalidation,â€
Biometrika, 97, 893â€“904.
Wille, A., Zimmermann, P., VranovaÌ, E., FuÌˆrholz, A., Laule, O., Bleuler, S., Hennig,
L., Prelic, A., von Rohr, P., Thiele, L., et al. (2004), â€œSparse graphical Gaussian
modeling of the isoprenoid gene network in Arabidopsis thaliana,â€ Genome Biol,
5, R92.
Williams, P. M. (1996), â€œUsing Neural Networks to Model Conditional Multivariate
Densities,â€ Neural Computation, 8, 843â€“854.
Wu, Y., Tjelmeland, H., and West, M. (2007), â€œBayesian CART: Prior Specification
and Posterior Simulation,â€ Journal of Computational and Graphical Statistics, 16,
44â€“66.
Yao, W. and Lindsay, B. G. (2009), â€œBayesian mixture labeling by highest posterior
density,â€ Journal of the American Statistical Association, 104.
Zhou, X. and Liu, X. (2008), â€œThe EM algorithm for the extended finite mixture
of the factor analyzers model,â€ Computational Statistics and Data Analysis, 52,
3939â€“3953.
Zou, Q.-H., Zhu, C.-Z., Yang, Y., Zuo, X.-N., Long, X.-Y., Cao, Q.-J., Wang, Y.-F.,
and Zang, Y.-F. (2008), â€œAn improved approach to detection of amplitude of lowfrequency fluctuation (ALFF) for resting-state fMRI: fractional ALFF.â€ Journal
of neuroscience methods, 172, 137â€“141.

90

Biography
Francesca Petralia was born in Vigevano, Italy on July 5, 1983. She graduated with
honors from the University of Pavia with a Bachelor of Science in Economics in June
2006. From the same university she earned her Master of Science in Finance in June
2008. She then joined the Department of Statistical Science at Duke University to
attend the Ph.D. program. In May 2012 she earned a Master of Science in Statistics.

91

