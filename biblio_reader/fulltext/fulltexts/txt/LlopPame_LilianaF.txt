Density estimation for spatial-temporal
models

Liliana Forzani, Ricardo Fraiman &
Pamela Llop

TEST
An Official Journal of the Spanish
Society of Statistics and Operations
Research
ISSN 1133-0686
Volume 22
Number 2
TEST (2013) 22:321-342
DOI 10.1007/s11749-012-0313-3

1 23

Your article is protected by copyright and all
rights are held exclusively by Sociedad de
EstadÃ­stica e InvestigaciÃ³n Operativa. This eoffprint is for personal use only and shall not
be self-archived in electronic repositories. If
you wish to self-archive your article, please
use the accepted manuscript version for
posting on your own website. You may
further deposit the accepted manuscript
version in any repository, provided it is only
made publicly available 12 months after
official publication or later and provided
acknowledgement is given to the original
source of publication and a link is inserted
to the published article on Springer's
website. The link must be accompanied by
the following text: "The final publication is
available at link.springer.comâ€.

1 23

Author's personal copy
Test (2013) 22:321â€“342
DOI 10.1007/s11749-012-0313-3
O R I G I N A L PA P E R

Density estimation for spatial-temporal models
Liliana Forzani Â· Ricardo Fraiman Â· Pamela Llop

Received: 18 December 2011 / Accepted: 30 September 2012 / Published online: 14 December 2012
Â© Sociedad de EstadÃ­stica e InvestigaciÃ³n Operativa 2012

Abstract In this paper a k-nearest neighbor type estimator of the marginal density
function for a random field which evolves with time is considered. Considering dependence, the consistency and asymptotic distribution are studied for the stationâˆš
ary and nonstationary cases. In particular, the parametric rate of convergence T
is proven when the random field is stationary. The performance of the estimator is
shown by applying our procedure to a real data example.
Keywords Spatio-temporal data Â· Density estimation Â· Local times
Mathematics Subject Classification 91B72 Â· 62G07 Â· 60J55

1 Introduction
In the last decade there has been a significant growth on research of functional data
as well as spatial-temporal data (see Tang et al. 2008). However, not all the theory
developed for curves has been extended to random fields as is the case of nonparametric marginal density estimation which so far has only been performed for RN -valued
random fields. For instance, for mixing stationary random fields, Tran and Yakowitz
(1993) proved the asymptotic normality of the k-nearest neighbor estimator. Tran
L. Forzani Â· P. Llop ()
Facultad de IngenierÃ­a QuÃ­mica and Instituto de MatemÃ¡tica Aplicada del Litoral, UNL-CONICET,
Santa Fe, Argentina
e-mail: lloppamela@gmail.com
R. Fraiman
Departamento de MatemÃ¡tica and Ciencias, Universidad de San AndrÃ©s, Buenos Aires, Argentina
R. Fraiman
CMAT, Universidad de la RepÃºblica, Montevideo, Uruguay

Author's personal copy
322

L. Forzani et al.

(1990) obtained the asymptotic normality of kernel type estimator, while Carbon et al.
(1996) studied its L1 convergence. The uniform consistency of this kind of estimator
was shown by Carbon et al. (1997) and further extensions were studied by Hallin
et al. (2001, 2004).
For functional data, the problem of nonparametric marginal density estimation
has been considered by several authors in different setups. The case when a single
sample path is observed over an increasing interval [0, T ] as T grows to infinity has
been studied by Rosenblatt (1970), Nguyen (1979), and Castellana and Leadbetter
(1986). In particular, the latter showed that for continuous time processes a parametric speed of convergence is attained by kernel type estimators. More recently, some
extensions have been obtained by Blanke and Bosq (1997), Blanke (2004), Kutoyants
(2004) among others. In particular, Labrador (2008) proposed a k-nearest neighbor
type estimator using local time ideas. Later, Llop et al. (2011) redefined the k-nearest
neighbor estimator for the case when an independent sample is observed, obtaining
parametric rates of convergence and its asymptotic normality.
This work addresses the problem of nonparametric marginal density estimation
for random fields which evolve in time. More precisely, given the following random
field:
X (s) = Î¼(s) + e(s),

s âˆˆ S âŠ‚ Rd ,

(1)

we estimate its marginal density function using a k-nearest neighbor type estimator
defined via local time when a dependent sample of identically distributed random
fields X1 , . . . , XT is given. For this estimator we study its asymptotic properties. The
functional nature of the data (a random surface) plays a fundamental role which allows us to obtain parametric rates of convergence of this density estimator in the
stationary case, contrary to what generally happens in nonparametric problems. This
kind of data appears naturally when analyzing the evolution of some measurements
in a geographical area (such as the Amazon), or when recording responses from the
brain during a time interval, among other interesting practical problems.
This paper is organized as follows: in Sect. 2 we recall some well-known dependence notions and give a new one which will be used in this work. Section 3 is dedicated to theoretical results. More precisely, in Sect. 3.2 we introduce the estimator for
the stationary case, prove its consistency obtaining strong rates of convergence, and
show its asymptotic normality. In Sect. 3.3, we extend the definition given in Sect. 3.2
to nonstationary random fields, and also obtain its rates of convergence. Section 4 is
devoted to numerically show the performance of our estimation methods. In Sect. 4.1
a simulated example is presented for d = 2, and in Sect. 4.2 a real data example of
fMRI images corresponding to the brain in the resting state is considered. Main and
auxiliary proofs are given in Appendices B and C, respectively.

2 Dependence notions
In this section we will review some known dependence notions and introduce a new
one which will be used in this work to find convergence rates of our density estimators. We will start with the classical Î±-mixing condition, introduced by Rosenblatt
(1956) whose definition is the following:

Author's personal copy
Density estimation for spatial-temporal models

323

Definition 1 We say that a sequence of random variables {Xt }tâˆˆN is Î±-mixing if there
exists a sequence {Î±(r), r âˆˆ N} decreasing to zero at infinity such that for each r,


P (A âˆ© B) âˆ’ P (A)P (B) â‰¤ Î±(r),
for all A âˆˆ Mttu1 and B âˆˆ Mllv1 with Mba the Ïƒ -field generated by the random variables
{Xt }bt=a , where 1 â‰¤ t1 â‰¤ tu < tu + r = l1 â‰¤ lv â‰¤ âˆ. If in addition there exist 0 <
Ï < 1 and a > 0 such that Î±(r) â‰¤ aÏ r then, we say that the sequence {Xt }tâˆˆN is
geometrically Î±-mixing.
Although this is one of the weakest notions of dependence, Doukhan and Louhichi
(1999) observed that certain processes which are of interest in statistics are not Î±mixing. This is the case of the autoregressive process of order one (AR(1)) given by
Xt = Î¸ Xtâˆ’1 + t , t âˆˆ Z, where t are independent and P (t = 1) = P (t = âˆ’1) = 12 ,
0 < |Î¸ | â‰¤ 12 . Inspired by this problem, the authors introduced a new notion of dependence which was called weak dependence (Definition 2), and using the AR(1) process
they showed that this definition is even weaker than the Î±-mixing dependence (see
Lemma 1).
Definition 2 (Doukhan and Louhichi 1999) We say that a sequence of random variables {Xt }tâˆˆN is (G, Î±, Ïˆ)-weakly dependent if there exist a class G of real-valued
functions, a sequence {Î±(r), r âˆˆ N} of positive numbers decreasing to zero at infinity,
and a function Ïˆ : G 2 Ã— N2 â†’ R such that for any u-tuple (t1 , . . . , tu ) and for any
v-tuple (l1 , . . . , lv ) with 1 â‰¤ t1 â‰¤ tu < tu + r = l1 â‰¤ lv â‰¤ âˆ,
 

cov f (Xt , . . . , Xt ), g(Xl , . . . , Xl )  â‰¤ Ïˆ(f, g, u, v)Î±(r),
u
v
1
1
for all functions f, g âˆˆ G defined on Ru and Rv , respectively.
Lemma 1 (Doukhan and Louhichi 1999) Let us consider the process AR(1), Xt =
Î¸ Xtâˆ’1 + t with t independent and P (t = 1) = P (t = âˆ’1) = 12 , 0 < |Î¸ | â‰¤ 12 . Then,
Xt is (G, Î±, Ïˆ)-weakly dependent with G the set of the bounded Lipschitz functions.
Here we will use the (G, Î±, Ïˆ)-weak dependence for G = {IE : E âˆˆ A} the class
of the indicator functions over measurable sets and Ïˆ(f, g, u, v) = Ï†(u, v) with Ï† :
N Ã— N â†’ R+ a function greater or equal to one. This weak dependence will be called
(Î±, Ï†)-weak dependence as specified in the following definition:
Definition 3 We say that a sequence of random variables {Xt }tâˆˆN is (Î±, Ï†)-weakly
dependent if there exist a sequence {Î±(r), r âˆˆ N} decreasing to zero at infinity and a
function Ï† : N Ã— N â†’ R+ with Ï†(u, v) â‰¥ 1 such that


P (A âˆ© B) âˆ’ P (A)P (B) â‰¤ Î±(r)Ï†(u, v),
for all A âˆˆ Mttu1 and B âˆˆ Mllv1 with Mba the Ïƒ -field generated by the random variables {Xt }bt=a , where 1 â‰¤ t1 â‰¤ tu < tu + r = l1 â‰¤ lv â‰¤ âˆ. If, in addition, there exist

Author's personal copy
324

L. Forzani et al.

constants L1 , L2 < âˆ and Î¼ â‰¥ 0 such that
âˆ

(j + 1)k Î±(j ) â‰¤ L1 Lk2 (k!)Î¼ ,

âˆ€k â‰¥ 0,

j =0

then, we say that the sequence {Xt }tâˆˆN is geometrically (Î±, Ï†)-weakly dependent.
The following lemma, whose proof is given in Appendix C, shows the relationship
between the (geometric) Î±-mixing and the (geometric) (Î±, Ï†)-weak dependence.
Lemma 2 If the sequence {Xt }tâˆˆN is (geometric) Î±-mixing then it is (geometric)
a
1
, L2 = 1âˆ’Ï
, Î¼ = 1 and any Ï† â‰¥ 1.
(Î±, Ï†)-weakly dependent with L1 = 1âˆ’Ï
Observe that we need to prove only the geometric case since Rosenblatt (1956)
showed the explicit weak dependence structure of the Î±-mixing process for G = Lâˆ
and Ïˆ(f, g, u, v) = 4 f âˆ g âˆ .

3 Density estimation
In this section we will introduce the density estimators for the stationary and nonstationary cases and present some asymptotic results. We will start the section by
introducing the setting of our work and some useful notation.
3.1 Setting and notation
.
Let S1 , S2 , . . . , Sd âŠ‚ R be finite intervals and let S = S1 Ã— S2 Ã— Â· Â· Â· Ã— Sd âˆˆ Rd be
a rectangle of measure |S|. Let us consider the first-order stationary random field
{e(s, Ï‰) âˆˆ R : s âˆˆ S, Ï‰ âˆˆ Î©} defined in a probability space (Î©, A, P ) with unknown
marginal density function fe , and which admits a local time. More precisely, if Î» is
the occupation measure of e(s, Ï‰),




. 
Î»(A, e) = Î» A, e(s, Ï‰) = IA e(s, Ï‰) ds, A âˆˆ B(R), Ï‰ âˆˆ Î©,
S

where B(R) stands for the Borel sigmaâ€“algebra on R, and if Î» is absolutely continuous with respect to the Lebesgue measure, then the local time is defined as a regular
.
version of the Radonâ€“Nikodym derivative lT (Â·, e) = lT (Â·, e(s, Ï‰)) for almost all Ï‰
(from now on e(s)). In this case we can write

Î»(A, e) = lT (u, e) du.
A

Given a sequence of random fields {et (s, Ï‰)}Tt=1 of e(s, Ï‰) (from now on e(s) and
et (s)), I(x,r) = [x âˆ’ r, x + r] the interval of center x and radius r, and {kT } being a

Author's personal copy
Density estimation for spatial-temporal models

325

sequence of real numbers such that kT /T < |S|, we define the random variable heT (x)
such that the time spend by {et (s)}Tt=1 at I(x,heT (x)) be kT . That is,
kT =

T 

t=1 S



II(x,he (x)) et (s) ds.

(2)

T

Lemma 3 If X (s) admits a local time, then the random variable heT exists and it is
unique.
Next we state some assumptions which will be used in this work.
H1 {et (s), s âˆˆ S}Tt=1 is a sequence of random fields with the same distribution as
e(s), where e(s) is a first-order stationary random field with unknown strictly
positive density function fe which admits a local time;
H2 the density fe is a strictly positive Lipschitz function with constant K;
H2 the density fe has two bounded derivatives;
H3 for each s fixed, {et (s)}Tt=1 is geometrically (Î±, Ï†)-weakly dependent with Ï†
some of the following functions:
â€“
â€“
â€“
â€“

Ï†(u, v) = 2v,
Ï†(u, v) = u + v,
Ï†(u, v) = uv,
Ï†(u, v) = Ï(u + v) + (1 âˆ’ Ï)uv, for some Ï âˆˆ (0, 1);

H3 for each s fixed, {et (s)}Tt=1 is geometrically (Î±, Ï†)-mixing;
H4 {kT } and {vT } are sequences of positive integers which converge to infinity such
that vT ( kTT ) â†’ 0 and T1 ( vkTT )2 â†’ âˆ.
H4 {kT } and {vT } are sequences of positive integers which converge to infinity such
that vT ( kTT ) â†’ 0 and vkTT â†’ âˆ.
H5 for each s fixed, {et (s)}Tt=1 is Î±-mixing with the mixing coefficients Î±(r) verifying
N

âˆ


Î±(r) â†’ 0 as N â†’ âˆ.

r=N

H6 For each c > 0,
  
âˆ’2 âˆ’2
c cT




S S {x:|uâˆ’x|â‰¤ccT } {x:|vâˆ’x|â‰¤ccT }

â†’
SÃ—S



fsr (u, v) âˆ’ fX (u)fX (v) du dv ds dr



.
fsr (x, x) âˆ’ fe2 (x) ds dr = c02 (x) > 0,

where fsr is the joint density of (e(s), e(r)) and cT = kTT with {kT } is a sequence
of positive integers going to infinity such that kTT â†’ 0.
H7 For some c > 0,
  







 â‰¤ cc4 ,
f
(u,
v)
âˆ’
f
(u)f
(v)
du
dv
ds
dr
sr
e
e
T


S S {x:|uâˆ’x|â‰¤ccT } {x:|vâˆ’x|â‰¤ccT }

Author's personal copy
326

L. Forzani et al.

where again fsr is the joint density of (e(s), e(r)) and cT = kTT with {kT } is a
sequence of positive integers going to infinity such that kTT â†’ 0.
Remark 1 The choice of the function Ï† in H3 is related with the exponential inequality (Theorem 4) used in the proofs (see Doukhan and Neumann 2006).
The symbol C will denote generic constants whose value may be different in each
occurrence.
3.2 The stationary case: Î¼(s) = Î¼ constant
If in model (1) we assume that the deterministic unknown mean function Î¼(s) is
constant with respect to the space (that is, Î¼(s) = Î¼) then the process X (s) (with
density fX ) inherits the stationarity of e(s). In this case, given a sample sequence of
random fields {Xt (s)}Tt=1 and a sequence {kT } of real numbers such that kT /T < |S|
we define the estimator of the density function fX as
.
fX (x) =

kT
2T |S|hX
T (x)

,

(3)

where the random variable hX
T (x) is defined as in (2) but with X instead of e.
Remark 2 From Lemma 3, if X (s) admits a local time then fX (x) is well defined.
If {Xt (s)}Tt=1 is an iid sample of X (s), a direct extension to Rd of the results given
in Llop et al. (2011) will allow us obtain the consistency of the estimator (3). However, in this work we will not assume independence of the data but the processes will
be (Î±, Ï†)-weakly dependent and therefore we will need another exponential inequalities to obtain rates of convergence.
Theorem 1 (Rates of convergence) For each x âˆˆ R we have


lim vT fX (x) âˆ’ fX (x) = 0 a.co.
T â†’âˆ

in the following two cases:
(a) weakly dependent case: for X fulfilling H1, H2, H3 and H4;
(b) Î±-mixing case: for X fulfilling H1, H2, H3 and H4 .
Remark 3
(a) Weakly dependent case: Our assumptions imply that we can choose kT such that
vT = T Î³ for any Î³ < 14 . More precisely, if kT = T Î² and vT = T Î³ , in order that
conditions


1 kT 2
kT
vT
â†’ 0 and
â†’âˆ
T
T vT

Author's personal copy
Density estimation for spatial-temporal models

327

are met, it enough that Î² âˆ’1+Î³ < 0 and Î² âˆ’Î³ âˆ’ 12 > 0 or equivalently, Î² < 1âˆ’Î³
and Î² > Î³ + 12 from which follows that given Î³ < 14 we can choose Î² such that
the conditions hold.
(b) Î±-mixing case: Our assumptions imply that we can choose kT such that vT = T Î³
for any Î³ < 12 . More precisely, if kT = T Î² and vT = T Î³ , in order that conditions

kT
kT
â†’ 0 and
â†’âˆ
vT
T
vT
are met, it enough that Î² âˆ’ 1 + Î³ < 0 and Î² âˆ’ Î³ > 0 or equivalently, Î² < 1 âˆ’ Î³
and Î² > Î³ from which follows that given Î³ < 12 we can choose Î² such that the
conditions hold.
Next we show the asymptotic normality of our estimator assuming the classical
Î±-mixing dependence condition. The result can still be shown for (G, Î±, Ïˆ)-weak
dependent sequences (using for instance Theorem 6.1 in Neumann and Paparoditis
2008), although it requires some technical hypotheses which we would like to avoid.
On the other hand, for (Î±, Ï†)-weak dependent sequences there is not yet a Central
Limit Theorem (CLT from now on) available. As in Castellana and Leadbetter (1986)
and Llop et al. (2011), the functional
nature of the data makes possible to attain a
âˆš
parametric rate of convergence T for our density estimator as stated in the following
result:
Theorem 2 (Asymptotic normality) Assume that H1, H2 and H5 hold for X (s).
Choose a sequence {kT } of positive real numbers going to infinite such that
kT2 /T 3/2 â†’ 0

and kT2 /T â†’ âˆ.

(4)

For that kT assume that H6 and H7 hold. Then, for all x âˆˆ R

âˆš 

2|S|

in distribution.
T fX (x) âˆ’ fX (x) â†’ N 0,
co
Remark 4 Observe that for this result we ask Î± to verify H5 which is weaker than the
condition for the (Î±, Ï†)-weak dependence required in Theorem 1. However, for this
result we require H7, which is another condition on the decay of the covariances.
3.3 The nonstationary case: Î¼(s) any function
If in model (1) we assume that the mean function Î¼(s) is any function then the process X (s) (with density fXs ) is not stationary any more. In this case, given a sample
sequence of random fields {Xt (s)}Tt=1 and a sequence {kT } of real numbers such that
kT /T < |S| we define the estimator of the density function fXs as


fXs (x) = fu x âˆ’ XÌ„T (s) ,
being
.
fu (x) =

kT
2T |S|huT (x)

(5)

Author's personal copy
328

L. Forzani et al.

with u = {UT 1 , . . . , UT T } given by UT t (s) = Xt (s) âˆ’ XÌ„T (s) = et (s) âˆ’ eÌ„T (s).
Here {e1 (s), . . . , eT (s)} is a sequence with the same distribution as e(s), eÌ„T (s) =
T
1
u
T
t=1 et (s) and hT is defined as in (2) replacing {et (s)}t=1 by u.
T
Remark 5 From Lemma 3, if X (s) admits a local time then fXs (x) is well defined.
For every s fixed, the random variables {UT 1 (s), . . . , UT T (s)} with E(UT t (s)) = 0
are identically distributed but not necessarily (Î±, Ï†)-weakly dependent. Therefore,
we cannot use directly the results given in Sect. 3.2. However, we can still prove the
complete convergence of the estimator of fXs and obtain rates of convergence.
Theorem 3 (Rates of convergence) Assume H1â€“H3 and choose two sequences {kT }
and {vT } of positive real numbers converging to infinity such that, for each fixed s,
vT (T /kT )|eÌ„T (s)| â†’ 0 a.co. For that sequences, suppose that H4 holds. Then for each
xâˆˆR


lim vT fXs (x) âˆ’ fXs (x) = 0 a.co.
T â†’âˆ

Remark 6 If for s fixed the sequence {et (s)}Tt=1 is geometric Î±-mixing with
|e(s)| < M for some constant M > 0, we can choose kT such that vT = T Î³ for
any Î³ < 14 . More precisely, let kT = T Î² and vT = T Î³ . In order that conditions

vT

kT
T

â†’ 0 and

1
T



kT
vT

2

â†’âˆ

are met, it is sufficient that Î² < 1 âˆ’ Î³ and Î² > Î³ + 12 . In addition, in order that
vT (T /kT )|eÌ„T (s)| â†’ 0 a.co., as eÌ„T (s) = o(T âˆ’Î± ) with Î± < 12 it would be Î² > Î³ + 12
from which follows that given Î³ < 14 we can choose Î² such that the conditions hold.
Remark 7 In the stationary case, we got a faster rate of convergence in the geometric Î±-mixing case than in the geometric (Î±, Ï†)-weak case. However, we are not
able to do the same in the nonstationary case since here we need the condition
vT (T /kT )|eÌ„T (s)| â†’ 0 to hold and therefore vT = T Î³ for Î³ < 14 .

4 Numerical examples
4.1 A simulated example
In this section we illustrate the performance of our method by estimating the marginal
density function of a nonstationary random field process Xt (s) : R2 â†’ R defined by
Xt (s) = Î¼(s) + et (s) + Î¸ etâˆ’1 (s),

Î¸ âˆˆ [0, 1],

s = (s1 , s2 ) âˆˆ S = (0, 1] Ã— (0, 1],

t â‰¥ 1,

Author's personal copy
Density estimation for spatial-temporal models

329

Fig. 1 Estimated (dashed curve) and theoretical (solid curve) density functions of Xt (s) for Î¼(s1 , s2 ) =
s1 + s2 , (s1 , s2 ) âˆˆ (0, 1] Ã— (0, 1] for Î¸ = 1/2

where Î¼(s) = Î¼(s1 , s2 ) = s1 + s2 and, for each t, et (s) is a Gaussian stationary random field with E(et (s)) = 0, var(et (s)) = 1, et (s) independent of el (s), for t = l and
cov(et (s), et (r)) = exp {âˆ’d(s, r)/0.25} with d(s, r) the Euclidean distance between
s and r.
We use the estimator defined in (5) for with 100 equispaced points in time and
100 Ã— 100 in space and compute the L1 -norm between the estimated (computed with
cross-validated parameter) and the true density for different for different values of Î¸ :
Î¸

L1 -norm

0
0.21
0.5
0.32
0.99 0.57

kT
19.1
26.4
29.2

As expected, the good behavior of the estimator decreases as Î¸ increases. In Fig. 1
we plot the estimator (dashed line) and the theoretical density (solid line) for some
points of S for Î¸ = 0.5. As we can see, the estimator fits very well the true density
except in the tails where, due to the nature of the field and classical border effects,
we are not able to perform a good estimation.
4.2 A real data example: brain activity
In neuroscience, it is well known that BOLD (blood-oxygen-level-dependence) activity in gray matter and white matter of the brain are different. The activity in areas

Author's personal copy
330

L. Forzani et al.

Fig. 2 (Left) Estimated mean. (Right) Estimated scale function

of gray matter is higher than in the white matter, which is reflected in the difference
between the mean Î¼gray (s, t) and Î¼white (s, t), the first being greater than the second
one. One question that arises is whether the fluctuations around the mean may be
detectable at the level of the distribution, i.e. if the density of the centered data is
different from the BOLD activity at the gray and the white matter. To do this type
of analysis, functional magnetic resonance imaging (from now fMRI) is used with
two types of experimental paradigm: the common stimulus response paradigm and
the â€œnonstimulus experimentâ€. When the brain is in absence of stimulus, we say that
it is in a â€œresting stateâ€. The brain at this condition has rich dynamics (see Fox and
Raichle 2007), so this state is the most useful to study brain activity. Here we analyze a two dimensional (slice) axial cut of the brain at a resting state condition.
The data studied correspond to one subject from the 1000 Functional Connectomes
Project (see Chao-Gan and Yu-Feng 2010a, 2010b). For the same axial cut T = 225
images of the brain were measured. Each image contains the same s = 1532 spatial
points (voxels) at different times. Among the 1532 voxels 617 of them corresponds
to the brain white matter and 884 to brain gray matter. The plots of the mean and
scale functions are given in the next two figures (Fig. 2). We first standardize the data
subtracting the mean value image and dividing by the scale image. Next we estimate
the stationary density functions of the white and gray matter separately using the
corresponding cross-validated parameters kT . The estimates obtained for the brain
gray matter and the white gray matter (for the standardized data) are very close (see
Fig. 3), which suggests that the main differences are just explained by the mean and
the scale functions.

5 Conclusions
In this paper the problem of estimating the marginal density function of a random
field that evolves in time was studied. For this purpose, a nearest-neighbor type estimator based on the occupation measure was defined for stationary and nonstationary
random fields. Under dependence, strong rates of convergence were obtained and, for
the stationary case, the asymptotic normality was also proven. The functional
strucâˆš
ture of the data allowed us to obtain parametric rates of convergence T contrary

Author's personal copy
Density estimation for spatial-temporal models

331

Fig. 3 Estimated stationary
density functions of brain gray
(kT = 63.6) and white matter
(kT = 46.7)

to what generally happens in nonparametric problems. A small simulation study was
performed and a real data example was analyzed to check the performance of the
estimator.
Acknowledgements We are most grateful to Daniel Fraiman for his very helpful insights to analyze the
brain fMRI data. We would also like to thank Roberto Scotto for helpful discussions. The authors were
supported by PICT2008-0921, PICT2008-0622, PI 62-309 and PIP 112-200801-0218.

Appendix A: Auxiliary results
Theorem 4 (Doukhan and Neumann 2006) Suppose that X1 , X2 , . . . , XT are realvalued random variables defined on a probability space (Î©, A, P ) with E(Xt ) = 0
and P (|Xt | â‰¤ M) = 1, for all t = 1, . . . , T and some M < âˆ. Let ST = Tt=1 Xt
and Ï† : N2 â†’ R+ be one of the following functions:
â€“
â€“
â€“
â€“

Ï†(u, v) = 2v;
Ï†(u, v) = u + v;
Ï†(u, v) = uv;
Ï†(u, v) = Ï(u + v) + (1 âˆ’ Ï)uv, for some Ï âˆˆ (0, 1).

We assume that there exist constants K, L1 , L2 < âˆ, Î¼ â‰¥ 0 and a nonincreasing
sequence of real coefficients {Î±(n)}nâ‰¥1 such that, for all u-tuples (t1 , . . . , tu ) and all
v-tuples (l1 , . . . , lv ) with 1 â‰¤ t1 â‰¤ tu < tu + r = l1 â‰¤ lv â‰¤ âˆ the following inequality
is fulfilled:


cov(Xt Â· Â· Â· Xt , Xl Â· Â· Â· Xl ) â‰¤ K 2 M u+vâˆ’2 Ï†(u, v)Î±(r),
u
v
1
1
where
âˆ

(j + 1)k Î±(j ) â‰¤ L1 Lk2 (k!)Î¼ ,
j =0

âˆ€k â‰¥ 0.

Author's personal copy
332

L. Forzani et al.

Then,


P (ST â‰¥ ) â‰¤ exp âˆ’

 2 /2
1/(Î¼+2) (2Î¼+3)/(Î¼+2)
Î£T + Î“T


,

where Î£T can be chosen as any number greater than or equal to ÏƒT2 = var(ST ) and

Î“T = 2(K âˆ¨ M)L2

24+Î¼ T K 2 L1
Î£T

âˆ¨1 .

Theorem 5 (Doukhan et al. 1984) Let X1 , X2 , . . . , XT be a sequence of Î±-mixing
random variables verifying E(Xt ) = 0 and |Xt | â‰¤ 1, for all t = 1, . . . , T . Let ST =
T
Î³ 1/Î³ }. Then, there
t=1 Xt and denote Î³ = 2/(1 âˆ’ Î¸ ) and Ïƒ = sup1â‰¤tâ‰¤T {E(|Xt | )
exist constants C1 and C2 which depend only on the mixing coefficients, such that for
0 < Î¸ < 1,

C2,T  1/2
1
P (ST â‰¥ ) â‰¤ C1 exp âˆ’ 1/4 1/2 ,
Î¸
T Ïƒ
where C2,T = C2 if T 1/2 Ïƒ â‰¤ 1 and C2,T = C2 T 1/4 Ïƒ 1/2 if T 1/2 Ïƒ > 1.
Theorem 6 (Robinson 1983) Let {VtT }Tt=1 be a triangular array of zero mean random variables and {bT }T â‰¥1 a sequence of positive constants such that:
(i) for each T , VtT , t = 1, . . . , T are identically distributed and Î±-mixing with the
mixing coefficients Î±(r) verifying
N

âˆ


Î±(r) â†’ 0 as N â†’ âˆ;

r=N

(ii)
(iii)
(iv)
(v)

there exists M > 0 such that P (|VtT | â‰¤ M) = 1 for all t = 1, . . . , T ;
bT â†’ 0 and T bT â†’ âˆ as T â†’ âˆ;
2 )/b â†’ Ïƒ 2 as T â†’ âˆ;
there exists Ïƒ 2 > 0 such that E(VtT
T
E(|VtT V(t+s)T |) â‰¤ CbT2 for s â‰¥ 1, t = 1, . . . , T where C is independent of T .

Then
T


1 
VtT â†’ N 0, Ïƒ 2
âˆš
T bT t=1

in distribution.

Appendix B: Proof of main results
Proof of Theorem 1 Let x âˆˆ R be fixed. By definition of complete convergence we
need to show that for all  > 0,
âˆ



 
P vT fX (x) âˆ’ fX (x) >  < âˆ.
T =1

Author's personal copy
Density estimation for spatial-temporal models

333

Using the definition of the estimator fX (x), it is enough to prove that
âˆ


P (AT ) < âˆ and

T =1

.
where for T =


vT

âˆ


P (BT ) < âˆ,

(6)

T =1

, the sets AT and BT are defined by

.
AT = AT (x) = hX
T (x) <

kT
2T |S|(fX (x) + T )

and
. {hX
T (x) >
BT = BT (x) =
âˆ…

kT
2T |S|(fX (x)âˆ’T ) }

si fX (x) > T
si fX (x) â‰¤ T .

To prove the left-side inequality of (6) (the proof of the right-side inequality is iden.
kT
tical and it will be omitted), let us define aT = aT (x) = 2T |S|(fX
(x)+T ) so that
AT = {hX
T < aT }. From the equivalence
hX
T < aT â‡”

T 

t=1



I(xâˆ’aT ,x+aT ) Xt (s) ds > kT ,
S


.
=Y
Tt

we have


P (AT ) = P

T



(7)

YT t > kT .

t=1

.
.
Next we define Y T t = YT t âˆ’ E(YT t ) and the probability pT = P (Xt (s) âˆˆ (x âˆ’ aT ,
x + aT )). Since E(YT t ) = |S|pT , using the definition of aT we get
P (AT ) = P

 T


YTt


> kT 1 âˆ’

t=1

1
pT
fX (x) + T 2aT


.

(8)

pT
=
By the Mean Value Theorem there exists xT âˆˆ (x âˆ’ aT , x + aT ) for which 2a
T
fX (xT ). In addition, by condition H2, the definition of aT and the fact that T â†’ 0
we have


 pT
 


 = fX (xT ) âˆ’ fX (x) â‰¤ K|xT âˆ’ x| â‰¤ KaT â‰¤ T ,
âˆ’
f
(x)
X
 2a

2
T

from which it follows that
1âˆ’

pT
2aT

â‰¤ fX (x) +

T
2

and then

pT
1
T /2
1
â‰¥
â‰¥ C(x, ) .
fX (x) + T 2aT
fX (x) + T
vT

Author's personal copy
334

L. Forzani et al.

Therefore, with this inequality in (8) we get
 T


kT
YTt > C
.
P (AT ) â‰¤ P
vT

(9)

t=1

(a) Weakly dependent case: in order to use a Bernstein type inequality in (9), we need
the following Lemma which will be proved in Appendix B.
Lemma 4 Under H3 for X (s) we have
(i) for any u-tuple (t1 , . . . , tu ) and any v-tuple (l1 , . . . , lv ), 1 â‰¤ t1 â‰¤ tu < tu + r =
l1 â‰¤ lv â‰¤ T we have
 


cov(Y T t Â· Â· Â· Y T t , Y T l Â· Â· Â· Y T l ) â‰¤ 2|S| u+v Ï†(u, v)Î±(r),
1

u

1

v

where 2|S| is such that |Y T t | â‰¤ 2|S| for all t, Î±(r) â†’ 0 with Ï† any of the functions given in H3;
(ii) for some constant C,
 T


Y T t â‰¤ CT .
var
t=1

Therefore, Lemma 4(i) implies that the sequence {Y T t }Tt=1 is (G, Î±, Ïˆ)-weakly
with f : Ru â†’ R and g : Rv â†’ R given by f (Y T t1 , . . . , Y T tu ) = Y T t1 Â· Â· Â· Y T tu and
g(Y T l1 , . . . , Y T lv ) = Y T l1 . . . Y T lv , respectively, and Ïˆ : G 2 Ã— N2 â†’ R+ defined by
Ïˆ(f, g, u, v) = (2|S|)u+v Ï†(u, v) with Ï† any of the four functions given in H3. Therefore, applying Theorem 4 with K = (2|S|)2 , M = 2|S|, Î“T = C and Î£T = CT in (9)
we have

âˆ
âˆ


CkT2
P (AT ) â‰¤ T0 +
exp âˆ’ 2
CvT T + CvT2 ( vkTT )(2Î¼+3)/(Î¼+2)
T =T0
T =1

âˆ

C
,
exp âˆ’ vT 2
â‰¤ T0 +
C( kT ) T + C( vkTT )1/(Î¼+2)
T =T0

and the result follows since (2Î¼ + 3)/(Î¼ + 2) < 2 and H4 ( vkTT )2 T â†’ 0 and, as a
consequence, vkTT â†’ 0.
(b) Î±-mixing case: under H1, H2, H3 and H4 for X (s). Since Xt verifies H5,
Y T t inherits the same condition and then we apply the Bernstein inequality given in
. tT
Theorem 5 to the random variables ZT t = Y2|S|
with |ZT t | â‰¤ 1 and E(ZT t ) = 0. For
Î³ =

2
1âˆ’Î¸

with 0 < Î¸ < 1, it is easy to verify that E(|ZT t |Î³ )1/Î³ â‰¤ (2|S|)1âˆ’Î³ so that

Ïƒ â‰¤ (2|S|)1âˆ’Î³ and then, since

C2,T
T 1/4 Ïƒ 1/2

â‰¥ C in (9) we have


âˆ
âˆ

kT
1 
,
P (AT ) â‰¤ T0 + C
exp âˆ’C
Î¸
vT

T =1

which is finite by H4 .

T =T0



Author's personal copy
Density estimation for spatial-temporal models

335

Proof of Theorem 2 For a fixed x âˆˆ R we define
ST =

T


sT2 = var(ST ) and aT =

YT t ,

t=1

kT
2T |S|(fX (x) +

âˆšz
T

)

.

From the definition of fX (x) and analogously to (7) we have




âˆš 
P T fX (x) âˆ’ fX (x) â‰¤ z = P hX
T â‰¥ aT

ST âˆ’ E(ST ) kT âˆ’ T |S|pT
=P
.
â‰¤
sT
sT
Then the proof will be completed if we prove:
ST âˆ’E(ST )
â†’ N (0, 1)
sT
kT âˆ’T |S|pT
â†’ 2|S|
sT
co z.

(a)
(b)

in distribution;

(a) It will a consequence of
sT2
T aT2

STâˆš
âˆ’E(ST )
T aT

=

âˆš1
T aT

T
t=1 Y tT

â†’ N (0, c02 ) and

â†’ c02 where c02 is given in H6. To prove the second part observe that

sT2 = var(ST ) =

=

T


var(YT t ) + 2

T
âˆ’1

T


t=1

t=1 s=t+1

T


T
âˆ’1 T
âˆ’t


var(YT t ) + 2

t=1

cov(YT t , YT s )

cov(YT t , YT t+l )

(taking l = s âˆ’ t).

t=1 l=1

Then,
sT2
T aT2

=

T
T âˆ’1 T âˆ’t
1 
1 
.
var(Y
)
+
2
cov(YT t , YT l+l ) = I + II.
Tt
T aT2 t=1
T aT2 t=1 l=1

(10)

Since aT âˆ¼ kT /T , from H6 we get
1
var(YT t )
aT2
=

1
aT2

  



S S {u:|uâˆ’x|â‰¤aT } {v:|vâˆ’x|â‰¤aT }



fst (u, v) âˆ’ fX (u)fX (v) du dv dt ds â†’ c02 > 0,

(11)
from which follows that I â†’ c02 as T â†’ âˆ. On the other hand, for N integer we
write,
T âˆ’1 N âˆ’1
T âˆ’1 T âˆ’t

.
1   
1   

cov(YT t , YT t+l ) +2 2
cov(YT t , YT t+l ) = III+IV,
|II| â‰¤ 2 2
T aT t=1 l=1
T aT t=1 l=N

Author's personal copy
336

L. Forzani et al.

where the term IV is considered zero if T âˆ’ t < N . Since
 
 


 

P At (s) âˆ© At+l (r) âˆ’ P At (r) P At+l (r) ds dr,
cov(YT t , YT l+l ) =
S S

.
.
and At (s) = {Xt (s) âˆˆ (x âˆ’ aT , x + aT )} âˆˆ Mtt and At+l (r) = {Xt+l (s) âˆˆ (x âˆ’ aT ,
t+l
x + aT )} âˆˆ Mt+l for each fixed s and r, hypothesis H5 implies that
 

 



 

P At (s) âˆ© At+l (r) âˆ’ P At (r) P At+l (r)  ds dr
cov(YT t , YT l+l ) â‰¤
S S

â‰¤ |S|2 Î±(l),
from which follows
âˆ
T âˆ’1 T âˆ’t
âˆ

C 
C 
C
IV â‰¤
Î±(l) â‰¤ 2
Î±(l) =
N
Î±(l).
T aT2 t=1 l=N
aT l=N
N aT2 l=N

On the other hand, from H7 we have


 cov(YT t , YT l+l )
 



 


 


=
P At (s) âˆ© At+l (r) âˆ’ P At (r) P At+l (r) ds dr
S S 







=
fsr (u, v) âˆ’ fX (u)fX (v) du dv ds dr
S S {x:|uâˆ’x|â‰¤ccT } {x:|vâˆ’x|â‰¤ccT }
â‰¤ CaT4

(12)

for some constant C which implies that III â‰¤ 2CN aT2 and hence
|II| â‰¤ 2CNaT2
Let  > 0 fixed and N = 


aT2

âˆ

C
+
N
Î±(l).
N aT2 l=N

. For this choice we get |II| â‰¤ C + C N

âˆ
l=N

Î±(l) for

T large enough (since aT âˆ¼ kTT â†’ 0). From hypothesis H5 there exists N0 such that
2
if M â‰¥ N0 , M âˆ
l=M Î±(l) <  . On the other hand, again since aT â†’ 0, there exists

2
T0 such that if T â‰¥ T0 ,  2  â‰¥ N0 and then  2  âˆ
l= 2  Î±(l) < 2 which implies
aT
aT
a
T

that II â‰¤ C which implies

sT2
T aT2

â†’ c02 .

(13)

T
2
To prove that STâˆšâˆ’E(ST ) = âˆš 1
t=1 Y tT â†’ N (0, c0 ) we will show that the variT aT
T aT
.
ables VT t = Y tT verify the assumptions (i)â€“(v) of the Lindeberg version of the CLT
given in Theorem 6 with bT = aT2 . As before, since Xt verifies H5, Y T t inherits the
same condition from which (i) follows. Condition (ii) holds for M = 2|S|. (iii) follows from (4) (since kT â†’ âˆ and aT âˆ¼ kTT â†’ 0), H5 implies (iv) (see (11)) and H7
implies (v) (see (12)). Therefore, from Theorem 6 we get the result since Ïƒ 2 = c02 .

Author's personal copy
Density estimation for spatial-temporal models

337

To prove (b) we use the definition of aT to get
âˆš
 x+a
2T aT |S|(fX (x) + z/ T ) âˆ’ T |S| xâˆ’aTT fX (u) du
kT âˆ’ T |S|pT
=
sT
sT
 x+aT


2T aT |S|z
.
fX (x) âˆ’ fX (u) du +
= sTâˆ’1 T |S|
âˆš
sT T
xâˆ’aT

(14)

By Taylor Theorem and since f has two derivatives bounded, for x âˆ— between x and
u we have
 x+a
 


T


 âˆ—
   1 x+aT
2



 â‰¤ Cs âˆ’1 T a 3 .
âˆ’
=
f
x
(u
âˆ’
x)
(x)
âˆ’
f
(u)
du
f
du
X
X
T
X
T

  2

xâˆ’aT

xâˆ’aT

Therefore, in (14) we have


2|S|z
kT âˆ’ T |S|pT
= O sTâˆ’1 T aT3 +
.
âˆš
sT
sT / T aT
Finally, by hypothesis (4) and (13) sTâˆ’1 T aT3 â†’ 0 then from (11) we get (b).



Proof of Theorem 3 This proof will be an immediate consequence of Theorem 1 and
of the following lemma which was proved in Llop et al. (2011, Lemma 5, p. 85). 
Lemma 5 Assume H1â€“H3 and choose two sequences {kT } and {vT } of positive real
numbers converging to infinity such that, for each fixed s, vT (T /kT )|eÌ„T (s)| â†’ 0 a.co.
For this sequences, suppose that H4 hold. Then for each x âˆˆ R
 



lim vT fu x âˆ’ XÌ„T (s) âˆ’ fe x âˆ’ Î¼(s) = 0, a.co.
nâ†’âˆ

where fe is the estimator of fe .

Appendix C: Proof of the auxiliary lemmas
Proof of Lemma 2 We need to prove that if Î±(r) â‰¤ aÏ r with 0 < Ï < 1 and a > 0
k
k
Î¼
then, âˆ
j =0 (j + 1) Î±(j ) â‰¤ L1 L2 (k!) , âˆ€k â‰¥ 0. For that, let us suppose that Î±(r) â‰¤
aÏ r for some 0 < Ï < 1 and a > 0. If f (k) denotes the kth derivative of f then
âˆ
âˆ


(j + 1)k Î±(j ) â‰¤ a
(j + 1)k Ï j
j =0

j =0
âˆ

â‰¤a
(j + k)(j + k âˆ’ 1) Â· Â· Â· (j + 1)Ï j
j =0

=a

âˆ

j =0

(k)
Ï

j +k


= a Ïk

1
1âˆ’Ï

(k)

.

(15)

Author's personal copy
338

L. Forzani et al.

Let C(k, j ) be denote the combinatorial number
k


(fg)(k) =

n
k

. Since

C(k, j )f (kâˆ’j ) g (j ) ,

j =0

 k (j )
Ï
=



k!
Ï kâˆ’j
(k âˆ’ j )!

and

1
1âˆ’Ï

(j )

=

j!
(1 âˆ’ Ï)j +1

we have

a Ïk

(k)

1
1âˆ’Ï

=a

k


 (kâˆ’j )
C(k, j ) Ï k

j =0

=a

k


C(k, j )

j =0

=a



1
1âˆ’Ï

(j )

j!
k! j
Ï
j ! (1 âˆ’ Ï)j +1


k
Ï
k! 
C(k, j )
1âˆ’Ï
1âˆ’Ï

j

1kâˆ’j

j =0



=

1
a
1âˆ’Ï 1âˆ’Ï

k

k!

where in the last equality we have used the Binomial Theorem. Then, with this inequality in (15), we get
âˆ

(j + 1)k Î±(j ) â‰¤
j =0

Therefore, taking L1 =

a
1âˆ’Ï ,

L2 =


1
a
1âˆ’Ï 1âˆ’Ï

k

k!

and Î¼ = 1 we get the result.

1
1âˆ’Ï



Proof of Lemma 3 For T and x fixed, the function
T 
. 1
G(r) =
lT (u, Xt ) du,
T
I(x,r)
t=1

is strictly increasing with G(0) = 0. On the other hand, due to the existence of local
time we can write
G(r) =

T
1
Î»(I(x,r) , Xt ),
T
t=1

then, G(r) â†’ |S| when r â†’ âˆ and therefore, the existence and uniqueness of hX
T (x)
is ensured. For a further reading on local times see Geman and Horowitz (1980). 

Author's personal copy
Density estimation for spatial-temporal models

339

Proof of Lemma 4 To prove part (i) let us consider the u-tuple (t1 , . . . , tu ) and the
v-tuple (l1 , . . . , lv ) for 1 â‰¤ t1â‰¤ tu < tu + r = l1 â‰¤ lv â‰¤ T and let C(k, j ) be denote
n
the combinatorial number k . Since E(YT t ) = |S|pT then,
u


Y T tk =

k=1

u
u
k


 . 

uâˆ’k 
YT tk âˆ’ pT |S| =
C(u, k) âˆ’pT |S|
YT ti .
k=1

k=0

i=1

Therefore, taking absolute value and considering that |pT | â‰¤ 1 we get


cov(Y T t Â· Â· Â· Y T t , Y T l Â· Â· Â· Y T l )
u
v
1
1
  u

v






= cov
Y T tk ,
Y T lm 


k=1

m=1

  u

v
k
m





uâˆ’k 

vâˆ’m 


= cov
C(u, k) âˆ’pT |S|
YT ti ,
C(v, m) âˆ’pT |S|
YT lj 


k=1

i=1

m=1

  k

u
v
m








â‰¤
C(u, k)
C(v, m)|S|u+vâˆ’(k+m) cov
YT ti ,
YT lj .


k=1

m=1

i=1

j =1

(16)

j =1

To compute
  k
   k

 k
 m

m
m

  







 

YT ti ,
YT lj  = E
YT ti
YT lj âˆ’ E
YT ti E
YT lj ,
cov

 

j =1

i=1

j =1

i=1

i=1

we will consider the following events:

. 
Ati (s) = Ï‰ âˆˆ Î© : Xti (s, Ï‰) âˆˆ (x âˆ’ aT , x + aT ) ,
and


. 
Alj (r) = Ï‰ âˆˆ Î© : Xlj (r, Ï‰) âˆˆ (x âˆ’ aT , x + aT ) ,

In addition, we will denote

Su

.
dsu =



i = 1, . . . , u

j = 1, . . . , v.


Â·Â·Â·

S

j =1

ds1 Â· Â· Â· dsu .
S

Then,

E

k

i=1

=E

YT ti

m



YT lj

j =1

 k 

i=1

S

m 





I(xâˆ’aT ,x+aT ) Xti (s) ds
I(xâˆ’aT ,x+aT ) Xlj (r) dr
j =1

S



Author's personal copy
340

L. Forzani et al.

 
=

Su

E

Sv

P
Su


m




I(xâˆ’aT ,x+aT ) Xti (s)
I(xâˆ’aT ,x+aT ) Xlj (s) dsu drv

i=1



 
=

 k


Sv

k




m


Ati (s) âˆ©

j =1

Alj (r) dsu drv ,

(17)

j =1

i=1

and similarly

E

k


 
YT ti E

m



YT lj

=

j =1

i=1



 
P
Su

Sv

k


 
Ati (s) P

m



Alj (r) dsu drv .

j =1

i=1

Hence,
  k

m






YT ti ,
YT lj 
cov


j =1

i=1


 k
 m

   
k
m






â‰¤
Ati (s) âˆ©
Alj (r) âˆ’ P
Ati (s) P
Alj (r)  dsu drv
P


u
v
S S
â‰¤

Su

Sv

j =1

i=1

 

j =1

i=1

Î±(r)Ï†(u, v) dsu drv = |S|u+v Î±(r)Ï†(u, v),

(18)

where in the last inequality we have used hypothesis H4 since, for each fixed s, r,
since k â‰¤ u and m â‰¤ v,
k


Ati (s) âˆˆ Mttu1

and

m

j =1

i=1

Alj (r) âˆˆ Mllv1 ,

with 1 â‰¤ t1 â‰¤ tu < tu + r = l1 â‰¤ lv â‰¤ T . Finally, with inequality (18) in (16) we get
  k

m






Y T ti ,
Y T lj 
cov


j =1

i=1

â‰¤

u


C(u, k)

k=1

â‰¤

u


v


C(v, m)|S|

m=1

C(u, k)

k=1

â‰¤ |S|u+v

v


  k

m





YT ti ,
YT lj 
cov



u+vâˆ’(k+m) 

i=1

C(v, m)|S|u+vâˆ’(k+m) |S|k+m Î±(l1 âˆ’ ti )Ï†(u, v)

m=1
u

k=1

j =1

C(u, k)

v

m=1


u+v
â‰¤ 2|S|
Î±(r)Ï†(u, v).

C(v, m)Î±(r)Ï†(u, v)

Author's personal copy
Density estimation for spatial-temporal models

341

To prove part (ii) of this lemma, first observe that since var(YT t ) â‰¤ E(YT2 t ) â‰¤
|S|2 pT2 â‰¤ |S|2 we may write

var

T

t=1


YTt

â‰¤

T


var(YT t ) + 2

t=1

T
âˆ’1

T



cov(Y T t , Y T l )

t=1 l=t+1

â‰¤ T |S|2 + 23 |S|2 Ï†(1, 1)

T
âˆ’1

T


Î±(l âˆ’ t)

t=1 l=t+1

â‰¤ CT .



References
Blanke D (2004) Adaptive sampling schemes for density estimation. J Stat Plan Inference 136(9):2898â€“
2917
Blanke D, Bosq D (1997) Accurate rates of density estimators for continuous-time processes. Stat Probab
Lett 33(2):185â€“191
Carbon M, Hallin M, Tran L (1996) Kernel density estimation for random fields: the L1 theory. J Nonparametr Stat 6(2â€“3):157â€“170
Carbon M, Hallin M, Tran L (1997) Kernel density estimation for random fields (density estimation for
random fields). Stat Probab Lett 36(2):115â€“125
Castellana JV, Leadbetter MR (1986) On smoothed probability density estimation for stationary processes.
Stoch Process Appl 21(2):179â€“193
Chaoâ€“Gan Y, Yu-Feng Z (2010a) DPARSF: a MATLAB toolbox for â€œpipelineâ€ data analysis of restingâ€“
state fMRI. Frontiers Syst Neurosci 4:1â€“7
Chaoâ€“Gan Y, Yu-Feng Z (2010b). http://www.nitrc.org
Doukhan P, Leon J, Portal F (1984) Vitesse de convergence dans le thÃ©orÃ©me central limite pour des
variables aleatoires mtlangeantes a valeurs dans un espace de Hilbert. C R Acad Sci Paris 289:305â€“
308
Doukhan P, Louhichi S (1999) A new dependence condition and applications to moment inequalities.
Stoch Process Appl 84:313â€“342
Doukhan P, Neumann M (2006) Probability and moment inequalities for sums of weakly dependent random variables, with applications. Stoch Process Appl 117:878â€“903
Fox MD, Raichle ME (2007) Spontaneous fluctuations in brain activity observed with functional networks.
Nat Rev Neurosci 8:700â€“711
Geman D, Horowitz J (1980) Occupation densities. Ann Probab 8(1):1â€“67
Hallin M, Lu Z, Tran L (2001) Density estimation for spatial linear processes. Bernoulli 7(4):657â€“668
Hallin M, Lu Z, Tran L (2004) Kernel density estimation for spatial processes: the L1 theory. Ann Stat
88:61â€“75
Kutoyants Y (2004) On invariant density estimation for ergodic diffusion processes. SORT 28(2):111â€“124
Labrador B (2008) Strong pointwise consistency of the kT -occupation time density estimator. Stat Probab
Lett 78(9):1128â€“1137
Llop P, Forzani L, Fraiman R (2011) On local times, density estimation and supervised classification from
functional data. J Multivar Anal 102(1):73â€“86
Nguyen H (1979) Density estimation in a continuous-time stationary Markov process. Ann Stat 7(2):341â€“
348
Neumann M, Paparoditis E (2008) Goodness-of-t tests for Markovian time series models: central limit
theory and bootstrap approximations. Bernoulli 14(1):14â€“46
Robinson PM (1983) Nonparametric estimators for time series. J Time Ser Anal 4:185â€“206

Author's personal copy
342

L. Forzani et al.

Rosenblatt M (1956) A central limit theorem and a strong mixing condition. Proc Natl Acad Sci USA
42:43â€“47
Rosenblatt M (1970) Density estimates and Markov sequences. In: Nonparametric techniques in statistical
inference. Cambridge University Press, Cambridge, pp 199â€“210
Tang X, Liu Y, Zhang J, Kainz W (2008) In: Advances in spatio-temporal analysis. ISPRS, vol 5
Tran LT (1990) Kernel density estimation on random fields. J Multivar Anal 34(1):37â€“53
Tran L, Yakowitz S (1993) Nearest neighbour estimators for random fields. J Multivar Anal 44(1):23â€“46

